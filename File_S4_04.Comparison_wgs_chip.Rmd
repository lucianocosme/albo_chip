---
title: "Aedes albopictus SNP chip - Comparying the genotypes of samples via WGS and the chip."
author: "Luciano V Cosme"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: breezedark
    css:
      - "styles.css"
    toc: yes
    toc_float: no
    toc_depth: 5
editor_options:
  markdown:
    wrap: 120
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval                        = TRUE,
  echo                        = TRUE,
  cache                       = TRUE,
  # tidy = TRUE,
  fig.width                   = 9,
  fig.height                  = 8,
  class.output                = "bg-success"
)
knitr::opts_knit$set(
  root.dir = rprojroot::find_rstudio_root_file()
)
```

# Comparison of genotypes obtained by WGS and the chip

We used the DNA left over from the library prep for WGS of 18 samples. The DNA was extracted but not used entirely for the
library prep. Here we will compare the genotypes of sites that are shared between the two data sets. For the genotyping
using the chip, we used the recommended priors and the new priors we obtained using the "SSTool" from Thermo Fisher
using the crosses. The WGS data was used to design the probe sequences for the chip. However, we used 819 genomes to
design the chip, and here we will take into consideration only 18 samples. We used ANGSD to perform the genotype calls
for all 819 samples together and here we are looking at only a few samples. Therefore, although the comparison can help
us identify problematic loci, we are cautious about the accuracy of each technology. The average sequence depth for the
WGS across the 819 samples was 12X. However, it is variable from sample to sample, and across the genome. Therefore, we
cannot precisely tell if the discrepancies in the genotypes between the technologies are due to sequence depth,
sequencing errors, or with the chip. We aim to gather a general overview of loci with discrepancies in zygosity or
genomic regions with higher than expected genotype discordancies.

# Analytic approach

1.  **Data Preparation**
    1.  Set the reference allele to match the 'AalbF3' genome assembly for both WGS and SNP chip data.
    2.  Convert the genotyping data into VCF format, making sure to maintain consistency between the two datasets.
2.  **Pairwise Comparisons**
    1.  Develop Python and/or R scripts to perform pairwise comparisons of the genotypes from each sample across the two
        technologies.
    2.  Check the concordance between the two genotyping methods for each sample pair.
3.  **Results Summarization**
    1.  Compile the results of the pairwise comparisons into a comprehensible format (e.g., a table or graph).
    2.  Calculate summary statistics that capture the level of agreement or discrepancy between the two technologies
        (e.g., percent agreement, kappa coefficient).
4.  **Threshold Identification**
    1.  If discrepancies exist, investigate possible thresholds or cutoffs that might explain the difference.
    2.  Examine the relationship between these thresholds and other characteristics of the data (e.g., minor allele
        frequency, call rate).
5.  **Interpretation**
    1.  Draw conclusions about the relative performance of the two genotyping technologies based on your findings.
    2.  Consider any implications these findings might have for future research or clinical applications.
6.  **Correlation between chip and WGS variables**
    1.  Identify variables associated with increase in mismatch rate between the genotyping technologies.
    2.  Try different thresholds for the variables associated with high mismatch rate
6.  **Data filtering and PCA**
    1.  Once the variables are identified, try different thresholds for perfecting overlap of points in a PCA when comparing WGS and Chip.

## 1. Load libraries

```{r load_libraries, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(here)
library(colorout)
library(flextable)
library(ggplot2)
library(scales)
library(reticulate)
library(extrafont)
library(stringr)
library(readr)
library(dplyr)
library(data.table)
library(scales)
library(ggrepel)
library(flextable)
library(forcats)
library(officer)
library(ggvenn)
library(RColorBrewer)
library(ggstatsplot)
library(broom)
```

Note about the general approach
We have data of 18 samples from 2 populations genotyped with both technologies: 6 samples from Nepal (KAT) and 12 samples from Trinidad and Tobago (SAI) - we did not have enough DNA left after library prep for all samples


3 genotyping calls:
WGS -> 800+ samples, 30 samples (KAT 12 samples and SAI 18 samples), and 18 samples (KAT 6 samples and SAI 12 samples)

Chip -> 500 samples, 95 samples (1 plate with the 18 samples and other wild samples), and 18 samples (KAT 6 samples and SAI 12 samples)

Since the WGS calls took longer, part of the code is written comparing default and new prior generated using the crosses. The aim is illustrative and to develop the code while waiting for the WGS calls to finish. It is not a good idea to use a prior from lab crosses in genotype calls using wild animals.

## 2. Import the chip data

Check how many samples

```{bash check_n_samples_in_vcf_file}
# make sure you have all the .CEL samples in your family file - 152
bcftools query -l data/raw_data/albo/wgs_vs_chip/wgs_default_prior_recommended_june_16_2023.vcf | wc -l
```

Check sample names

```{bash check_sample_names_in_vcf_file}
# make sure you have all the .CEL samples in your family file - 152
bcftools query -l data/raw_data/albo/wgs_vs_chip/wgs_default_prior_recommended_june_16_2023.vcf | head
```

### 2.1 Use Plink2 to convert to bed format

Create output directory

```{r create_main_dir, eval=FALSE}
# Create main directory
dir.create(
  here("output", "wgs_vs_chip"),
  showWarnings = FALSE,
  recursive = FALSE
)
```

Convert 'vcf' file from Axiom suite to 'bed' format

```{bash plink2_convert_vcf_to_bed1_default_prior}
# I created a fam file with the information about each sample, but first we import the data and create a bed file setting the family id constant
plink2 \
--allow-extra-chr \
--vcf data/raw_data/albo/wgs_vs_chip/wgs_default_prior_recommended_june_16_2023.vcf \
--const-fid \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/wgs_vs_chip/chip_dp_01 `# dp - default priors` \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants" output/wgs_vs_chip/chip_dp_01.log # to get the number of variants from the log file.
```

Using the default priors we obtained 105,607 SNPs. All the reference alleles matched the reference genome (AalbF3).

```{bash plink2_convert_vcf_to_bed1_new_prior}
# I created a fam file with the information about each sample, but first we import the data and create a bed file setting the family id constant
plink2 \
--allow-extra-chr \
--vcf data/raw_data/albo/wgs_vs_chip/wgs_new_prior_recommended_june_16_2023.vcf \
--const-fid \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/wgs_vs_chip/chip_np_01 `# np - new priors` \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants" output/wgs_vs_chip/chip_np_01.log # to get the number of variants from the log file.
```

Using the new priors we obtained 118,408 SNPs. All the reference alleles matched the reference genome (AalbF3).

Check the headings of the the files we will work on.

```{bash check_headings1, cache=TRUE}
head -n 5 output/wgs_vs_chip/chip_np_01.fam
```

We need to update the family information, individual id, and sex of each individual. We can use the same file we use
with the Axiom Suite to update our .fam file.

```{bash check_headings_1, cache=TRUE}
head -n 5 data/raw_data/albo/wgs_vs_chip/sample_ped_info.txt
```

## 2.2 Use R to update the .fam file

Import the fam file we use with Axiom Suite

```{r import_fam_file_Axiom, cache=TRUE}
# the order of the rows in this file does not matter
samples <-
  read.delim(
    file   = here(
      "data",
      "raw_data",
      "albo",
      "wgs_vs_chip",
      "sample_ped_info.txt"
    ),
    header = TRUE
  )
head(samples)
```

Import .fam file we created once we created the bed file using Plink2

```{r import_fam_dp}
# The fam file is the same for both data sets with the default or new priors
fam1 <-
  read.delim(
    file   = here(
      "output", "wgs_vs_chip", "chip_dp_01.fam"
    ),
    header = FALSE,
    
  )
head(fam1)
```

We can merge the tibbles.

```{r merge_objects1}
# to keep the same order of the .fam file, we will first create an index based on the numbers of the samples, then use it too keep the order

# Extract the number part from the columns
fam1_temp <- fam1 |>
  mutate(num_id = as.numeric(str_extract(V2, "^\\d+")))

samples_temp <- samples |>
  mutate(num_id = as.numeric(str_extract(Sample.Filename, "^\\d+")))

# Perform the left join using the num_id columns and keep the order of fam1
df <- fam1_temp |>
  dplyr::left_join(samples_temp, by = "num_id") |>
  dplyr::select(-num_id) |>
  dplyr::select(8:13)

# check the data frame
head(df)
```

We can check how many samples we have in our file

```{r check_number_samples_df}
nrow(df)
```

Before you save the new fam file, you can change the original file to a different name, to compare the order later. If
you want to repeat the steps above after you saving the new file1.fam, you will need to import the vcf again.

```{r save_new_fam_file}
# Save and override the .fam file for dp
write.table(
  df,
  file      = here(
    "output", "wgs_vs_chip", "chip_dp_01.fam"
  ),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)

# Save and override the .fam file for np
# Fist we need to change the sample ids
df$Individual_ID <- gsub("a", "b", df$Individual_ID)
# Save it
write.table(
  df,
  file      = here(
    "output", "wgs_vs_chip", "chip_np_01.fam"
  ),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Check the new .fam file to see if has the order and the sample attributes we want.

```{bash check_headings2, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/wgs_vs_chip/chip_dp_01.fam
```

```{bash check_headings3, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/wgs_vs_chip/chip_np_01.fam
```

## 3. Import the WGS data

The WGS data is already in the 'bed' format, we can create a new bed file and check if the reference alleles match the
reference genome.

```{bash plink2_check_alleles_AalbF3}
# We can create a new bed file and check if the reference and alternative alleles are set correctly
# I manually added "w" to the sample names after creating the file
plink2 \
--allow-extra-chr \
--bfile data/raw_data/albo/wgs_vs_chip/wgs \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' \
--out output/wgs_vs_chip/wgs_01 \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants\|samples" output/wgs_vs_chip/wgs_01.log
```

Now we have some considerations to make about which strategy to follow to do a pairwise comparison of the 18 samples:

1.  **Single VCF for Each Technology:** We can create two multi-sample VCFs, one for each technology (sequencing and SNP
    chip). This approach could make it easier to manage and manipulate your data, especially if the number of variants
    detected by each technology is different.

2.  **Single VCF for Each Sample:** Having a separate VCF for each sample could be useful if we plan to do a lot of
    sample-specific processing. However, it could become difficult to manage if we had a large number of samples.

I will create a vcf for each sample setting the missingness to zero.

## 4. Prepare vcf files for comparisons

Create output directory

```{r, eval=FALSE}
# Create subdirectories for default and new priors. We can put the WGS vcfs in both.
subdirs <- c("vcfs")

for (subdir in subdirs) {
  dir.create(here("output", "wgs_vs_chip", subdir), showWarnings = FALSE)
}
```

We can merge the WGS and Chip data sets

```{bash}
# Create list of files to merge: wgs with chip with default prior
echo 'output/wgs_vs_chip/wgs_01
output/wgs_vs_chip/chip_dp_01
output/wgs_vs_chip/chip_np_01' > output/wgs_vs_chip/merge_list.txt
```

Merge the data (wgs and both chip data sets)

```{bash merge_cw}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list.txt \
--out output/wgs_vs_chip/wgs_chip \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_chip.log
```

Now we can subset the samples and keep the pairs that we are interested in.

**Code Explanation:**

1. **Variable Initialization:**
   - The code defines a variable "input_file" with the value "output/wgs_vs_chip/wgs_chip.fam".
   - It defines a variable "output_dir" with the value "output/wgs_vs_chip/vcfs".
   - It defines a variable "bfile" with the value "output/wgs_vs_chip/wgs_chip".

2. **Create Output Directory:**
   - The code creates the output directory if it does not exist using the following command:
     mkdir -p $output_dir

3. **Retrieve Unique Families:**
   - It retrieves the unique families from the input file specified by "input_file".
   - The "awk" command extracts the first column from the input file.
   - The "sort" command sorts the extracted column.
   - The "uniq" command filters out duplicate entries.
   - The resulting unique families are stored in the "families" variable.

4. **Loop Over Families:**
   - The code enters a loop over each family ("famid") in the "families" variable.

5. **Retrieve Base Sample IDs:**
   - Within the family loop, it retrieves the base sample IDs (without 'a', 'b', or 'w' suffixes) for the current family.
   - The "grep" command filters the input file based on the current family.
   - The "awk" command extracts the second column (base sample IDs) from the filtered lines.
   - The "sed" command removes the 'a', 'b', or 'w' suffixes from the base sample IDs.
   - The "uniq" command filters out duplicate entries.
   - The resulting base sample IDs are stored in the "base_iids" variable.

6. **Nested Loop Over Base Sample IDs and Combinations:**
   - The code enters another loop over each base sample ID ("base_iid") in the "base_iids" variable.
   - Within the base sample ID loop, it enters a nested loop over three combinations: "aw", "ab", and "bw".

7. **Check Sample Existence:**
   - For each combination, the code checks if both samples exist in the input file.
   - It uses the "grep" command with regular expressions and the "-q" option to suppress output.
   - If both samples exist, it proceeds with the following steps.

8. **Create Temporary File:**
   - It creates a temporary file using the "mktemp" command to store the relevant lines from the input file.

9. **Extract Relevant Lines:**
   - The code uses the "grep" command to extract the lines from the input file that match the family, base sample ID, and current combination.
   - The matching lines are appended to the temporary file.

10. **Execute plink2:**
    - It executes the "plink2" command with various options and arguments to perform specific operations on the data.
    - The command performs tasks such as allowing extra chromosomes, preserving allele order, using the specified binary file ("bfile"), applying filters, and specifying the output format.
    - The output is saved to a VCF file with a name based on the family, base sample ID, and combination.
    - The "--silent" option suppresses unnecessary output.

11. **Remove Temporary File:**
    - After executing "plink2", it removes the temporary file using the "rm" command.

12. **Continuation of Nested Loops:**
    - The code continues the nested loops until all combinations and base sample IDs have been processed.


```{bash create_vcfs, eval=FALSE}
input_file="output/wgs_vs_chip/wgs_chip.fam"
output_dir="output/wgs_vs_chip/vcfs"
bfile="output/wgs_vs_chip/wgs_chip"

# create the output directory if it does not exist
mkdir -p $output_dir

# get unique families
families=$(awk '{print $1}' $input_file | sort | uniq)

for famid in $families; do
  # get the base sample ids (without a, b, w)
  base_iids=$(grep "$famid" $input_file | awk '{print $2}' | sed 's/[abw]$//' | uniq)
  
  for base_iid in $base_iids; do
    for combination in "aw" "ab" "bw"; do
      # Check if both samples exist
      if grep -qE "${famid}\s${base_iid}[${combination:0:1}]\s" "$input_file" && 
         grep -qE "${famid}\s${base_iid}[${combination:1:1}]\s" "$input_file"; then
        # Create temporary file
        tmp_file=$(mktemp)
        grep -E "${famid}\s${base_iid}[${combination:0:1}]\s" "$input_file" > "$tmp_file"
        grep -E "${famid}\s${base_iid}[${combination:1:1}]\s" "$input_file" >> "$tmp_file"
  
        # Execute plink2
        plink2 \
        --allow-extra-chr \
        --keep-allele-order \
        --bfile $bfile \
        --keep "$tmp_file" \
        --recode vcf-iid \
        --geno 0 \
        --out "$output_dir/${famid}_${base_iid}${combination}" \
        --silent
  
        # Remove temporary file
        rm "$tmp_file"
      fi
    done
  done
done
```

Check how many SNPs per vcf

```{bash count_snps_per_vcf}
# Define directory with the vcfs
output_dir="output/wgs_vs_chip/vcfs"
# Count how many SNPs we have in each vcf file
for file in ${output_dir}/*.vcf; do
    echo $(basename $file): $(grep -v '^#' $file | wc -l)
done
```

Check sample names to see if our code created the vcfs with two samples

```{bash check_sample_name_vcfs}
# Define directory with the VCFs
output_dir="output/wgs_vs_chip/vcfs"

# Iterate over each VCF file
for file in "${output_dir}"/*.vcf; do
    # Extract the file name without the directory path
    file_name=$(basename "${file}")

    # Use bcftools query to retrieve the sample names
    sample_names=$(bcftools query -l "${file}")
    
    # Print the file name and the sample names
    echo "${file_name}: ${sample_names}"
done

```

Create new directories
```{r create_scripts_dir, eval=FALSE}
# Create main directory
dir.create(
  here("output", "wgs_vs_chip", "scripts"),
  showWarnings = FALSE,
  recursive = FALSE
)
```

Script to compare alleles between wgs and chip or chip priors

Code summary:
The provided code performs the following steps:

1. **Import the necessary libraries**
   The code imports the required libraries: "allel", "pandas", "os", and "numpy".

2. **Create an empty DataFrame**
   The code initializes an empty DataFrame called "output_df" to store the output results obtained from the analysis.

3. **Specify the directory**
   The code defines the directory path where the VCF files are located using the "dir_name" variable.

4. **Retrieve a list of VCF files**
   The code uses the "os.listdir()" function and list comprehension to create a list of all VCF files in the specified directory that end with '.vcf'.

5. **Iterate over each VCF file**
   The code sets up a loop to iterate over each VCF file found in the previous step.

6. **Construct the file path**
   The code constructs the full file path for the current VCF file by combining the directory path and the file name using "os.path.join()".

7. **Read the VCF file**
   The code reads the VCF file using "allel.read_vcf()" from the "allel" library, specifying to load all available fields ('*').

8. **Extract the genotype data**
   The code extracts the genotype data from the VCF file using "allel.GenotypeArray(callset['calldata/GT'])".

9. **Check sample count**
   The code verifies if the VCF file contains two samples by checking the shape of the genotype array using the "assert" statement. If the shape doesn't match the expected number of samples, an assertion error is raised.

10. **Count total SNPs**
    The code determines the total number of SNPs in the genotype data by calculating the length of the genotype array using "len(gt)".

11. **Calculate counts of homozygous and heterozygous SNPs**
    The code uses "np.count_nonzero()" and relevant methods of the "gt" object to count the number of homozygous reference, homozygous alternate, and heterozygous SNPs for each sample.

12. **Compute counts of mismatched homozygous and heterozygous SNPs**
    The code compares the genotypes between the two samples using "np.sum()" to calculate the counts of mismatched homozygous reference, homozygous alternate, and heterozygous SNPs.

13. **Extract reference and alternative alleles**
    The code retrieves the reference and alternative alleles for each SNP from the VCF file.

14. **Count mismatching reference and alternative alleles**
    The code compares the alleles between the two samples and counts the number of SNPs with mismatching reference alleles and the number of SNPs with mismatching alternative alleles.

15. **Calculate counts of A, T, C, and G alleles**
    The code computes the counts of A, T, C, and G alleles for each sample based on the genotype data and the corresponding reference and alternative alleles.

16. **Create and append result to output dataframe**
    The code creates a DataFrame called "result" to store the calculated statistics for the current VCF file and appends it to the "output_df" DataFrame using "pd.concat()".

17. **Repeat for each VCF file**
    The code repeats steps 5 to 16 for each VCF file in the directory, processing and appending the results to the "output_df" DataFrame.

18. **Write the output to a CSV file**
    The code writes the final "output_df" DataFrame to a CSV file named 'allele_comparison_stats_2.csv' using the "to_csv()" method of pandas.

```{python compare_alleles, eval=FALSE}
import allel
import pandas as pd
import os
import numpy as np

# Initialize the output dataframe
output_df = pd.DataFrame()

# Directory with vcf files
dir_name = "output/wgs_vs_chip/vcfs/"

# Get list of all vcf files in the directory
vcf_files = [f for f in os.listdir(dir_name) if f.endswith('.vcf')]

# Iterate over VCF files
for vcf_file in vcf_files:
    file_path = os.path.join(dir_name, vcf_file)
    callset = allel.read_vcf(file_path, fields=['*'])

    # Get genotype
    gt = allel.GenotypeArray(callset['calldata/GT'])
    
    # Verify the vcf contains two samples
    assert gt.shape[1] == 2, f"Expected 2 samples in {vcf_file}, found {gt.shape[1]}"

    # Count SNPs
    n_snps = len(gt)

    # Count homozygous and heterozygous SNPs for each sample
    n_homo_ref = np.count_nonzero(gt.is_hom_ref(), axis=0)
    n_homo_alt = np.count_nonzero(gt.is_hom_alt(), axis=0)
    n_hetero = np.count_nonzero(gt.is_het(), axis=0)
    
    # Count homozygous and heterozygous SNPs mismatches
    n_homo_ref_mismatch = np.sum(gt.is_hom_ref()[:, 0] != gt.is_hom_ref()[:, 1])
    n_homo_alt_mismatch = np.sum(gt.is_hom_alt()[:, 0] != gt.is_hom_alt()[:, 1])
    n_hetero_mismatch = np.sum(gt.is_het()[:, 0] != gt.is_het()[:, 1])

    # Get alleles
    ref_alleles = callset['variants/REF']
    alt_alleles = callset['variants/ALT'][:, 0]  # assuming bi-allelic

    # Count mismatching reference and alternative alleles
    n_snps_ref_mismatch = np.count_nonzero(ref_alleles[gt[:,0]] != ref_alleles[gt[:,1]])
    n_snps_alt_mismatch = np.count_nonzero(alt_alleles[gt[:,0]] != alt_alleles[gt[:,1]])

    # Count alleles for each sample
    n_a = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'A' or alt_alleles[i] == 'A')
    n_t = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'T' or alt_alleles[i] == 'T')
    n_c = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'C' or alt_alleles[i] == 'C')
    n_g = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'G' or alt_alleles[i] == 'G')

    # Append results to the output dataframe
    result = pd.DataFrame({
        'vcf_file': [file_path],
        'n_SNPs': [n_snps],
        'n_SNPs_ref_mismatch': [n_snps_ref_mismatch],
        'n_SNPs_alt_mismatch': [n_snps_alt_mismatch],
        'n_A': [n_a],
        'n_T': [n_t],
        'n_C': [n_c],
        'n_G': [n_g],
        'n_homo_ref': [n_homo_ref],
        'n_homo_alt': [n_homo_alt],
        'n_hetero': [n_hetero],
        'n_homo_ref_mismatch': [n_homo_ref_mismatch],
        'n_homo_alt_mismatch': [n_homo_alt_mismatch],
        'n_hetero_mismatch': [n_hetero_mismatch]
    })

    output_df = pd.concat([output_df, result])

# Write the result to a csv file
output_df.to_csv('output/wgs_vs_chip/allele_comparison_stats_2.csv', index=False)
```

Clean env
```{r clean_python_env3}
# python
py_run_string("import gc; gc.collect()")
```

Import the data
```{r}
data <-
  read_delim(
    "output/wgs_vs_chip/allele_comparison_stats_2.csv",
    delim = ",",
    show_col_types = FALSE
  )

data <-
  data |>
  mutate(vcf_file = str_remove(vcf_file, "output/wgs_vs_chip/vcfs/")) |>
  separate(
    vcf_file,
    into = c("Population", "Sample_Comparison"),
    sep = "_",
    extra = "drop"
  ) |>
  separate(
    Sample_Comparison,
    into = c("Sample", "Comparison"),
    sep = "(?<=\\d)(?=[a-z])",
    convert = TRUE
  ) |>
  mutate(Comparison = str_remove(Comparison, ".vcf")) |>
  arrange(Comparison)

# Split the "Comparison" column into "Sample1" and "Sample2"
data <- 
  data |>
  separate(
    Comparison,
    into = c("Sample1", "Sample2"),
    sep = 1,
    # because each comparison has two characters
    remove = FALSE
  ) |> # keep the original comparison column
  relocate(Sample1, Sample2, .after = Comparison) # move the new columns right after Comparison

cols_to_split <-
  c("n_A",
    "n_T",
    "n_C",
    "n_G",
    "n_homo_ref",
    "n_homo_alt",
    "n_hetero")

# Remove unwanted characters from the columns
for (col_name in cols_to_split) {
  data[[col_name]] <- gsub("\\[\\[|]\\n", "", data[[col_name]])
}

# Split the columns
for (col_name in cols_to_split) {
  # Create new column names based on 'Sample1' and 'Sample2'
  new_col_names <- paste0(col_name, "_sample", 1:2)
  
  data <- data |>
    separate(
      col = col_name,
      into = new_col_names,
      sep = " ",
      extra = "drop"
    )
}

# Clean the new columns
cols_to_clean <- 
  grep("^n_", names(data), value = TRUE)

for (col_name in cols_to_clean) {
  # Remove unwanted characters '[', ']', and '\n'
  data[[col_name]] <- gsub("\\[|]|\\n", "", data[[col_name]])
}

# Split the column names into "Sample" and numeric value
data <- 
  data |>
  separate(
    col = Comparison,
    into = c("Sample1", "Sample2"),
    sep = 1,
    remove = FALSE
  ) |>
  relocate(Sample1, Sample2, .after = Comparison)

# Convert columns to numeric
# Specify the column names to convert to numeric
columns_to_convert <-
  c(
    # "Population",
    "Sample",
    # "Comparison",
    # "Sample1",
    # "Sample2",
    "n_SNPs",
    "n_SNPs_ref_mismatch",
    "n_SNPs_alt_mismatch",
    "n_A_sample1",
    "n_A_sample2",
    "n_T_sample1",
    "n_T_sample2",
    "n_C_sample1",
    "n_C_sample2",
    "n_G_sample1",
    "n_G_sample2",
    "n_homo_ref_sample1",
    "n_homo_ref_sample2",
    "n_homo_alt_sample1",
    "n_homo_alt_sample2",
    "n_hetero_sample1",
    "n_hetero_sample2",
    "n_homo_ref_mismatch",
    "n_homo_alt_mismatch",
    "n_hetero_mismatch"
  )

# Convert columns to numeric
data[columns_to_convert] <-
  lapply(data[columns_to_convert], function(x)
    as.numeric(as.character(x)))

# Verify the column types
print(sapply(data[columns_to_convert], class))
```

Now we can subset the data to have more meaningful comparisons and visualizations.

## 5. Pairwise comparions

First we can compare the priors to see if it is reasonable to generate new priors using the SSTool. I am doing this first because the genotype calls for the WGS data are still running. We can test our code and later we use it to look at the comparisons of interest. I do not think that the new prior generated with the crosses data should work since the population have been in the lab for several generations and we are using the priors with wild animals.

### 5.1 Compare priors to test code for comparisons

I create new priors using the SSToll from ThermoFisher and the crosses data. We can compare the genotype calls using each priors. We need to do some data tyding first.

```{r filter_samples_for_comparison}
# Filter rows containing "ab" in column "Comparison"
priors <-
  data |>
  filter(
    Comparison == "ab"
  )

# The default priors is represented as "a" (Sample1) and the new priors are represented as "b" (Sample2)

# Change column names
colnames(priors) <- gsub("sample1", "default_prior", colnames(priors))
colnames(priors) <- gsub("sample2", "new_prior", colnames(priors))

# Verify the updated column names
print(colnames(priors))
```

Sanity check
```{r sanity_check_x1}
# Add a new column named allele_totals to sum n_A_new_prior, n_T_new_prior, n_C_new_prior, and n_G_new_prior
priors <-
  priors |>
  mutate(
    allele_total_new = n_A_new_prior + n_T_new_prior + n_C_new_prior + n_G_new_prior,
    allele_total_default = n_A_default_prior + n_T_default_prior + n_C_default_prior + n_G_default_prior
  )

# Compare the allele totals with the number of SNPs
head(priors |>
  dplyr::select(Population, Sample, n_SNPs, allele_total_new, allele_total_default))
```
The sum of A, T, C and G is twice as the number of SNPs because we have two samples in each comparison. Therefore, we need to divide by 2 when calculating the differences in allele counts.

#### 5.1.1 Allele counts

```{r summarize_allele_counts}
# we can calculate how many counts of each allele (A, T, C and G) we have for each prior. Lets do difference = New - default prior
priors_allele_count <-
  priors |>
  dplyr::select(
    Population,
    Sample,
    n_SNPs,
    n_A_default_prior,
    n_A_new_prior,
    n_T_default_prior,
    n_T_new_prior,
    n_C_default_prior,
    n_C_new_prior,
    n_G_default_prior,
    n_G_new_prior,
  ) |>
  mutate(
    n_A_diff = (n_A_new_prior / 2 - n_A_default_prior / 2),
    n_T_diff = (n_T_new_prior / 2 - n_T_default_prior / 2),
    n_C_diff = (n_C_new_prior / 2 - n_C_default_prior / 2),
    n_G_diff = (n_G_new_prior / 2 - n_G_default_prior / 2)
  ) |>
  dplyr::select(Population,
                Sample,
                n_SNPs,
                n_A_diff,
                n_T_diff,
                n_C_diff,
                n_G_diff) |>
  arrange(Population, Sample) |>
  mutate(
    n_A_diff = paste0(
      formatC(
        n_A_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_A_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_T_diff = paste0(
      formatC(
        n_T_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_T_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_C_diff = paste0(
      formatC(
        n_C_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_C_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_G_diff = paste0(
      formatC(
        n_G_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_G_diff / n_SNPs) * 100, 2),
      "%)"
    )
  ) |>
  relocate(n_C_diff, .after = n_A_diff) # move the new columns right after n_A_diff

# Convert head(results) to a tibble
table_result <-
  as_tibble(priors_allele_count)

# Set theme if you want to use something different from the previous table
set_flextable_defaults(
  font.family = "Arial",
  font.size = 9,
  big.mark = ",",
  theme_fun = "theme_zebra" # try the themes: theme_alafoli(), theme_apa(), theme_booktabs(), theme_box(), theme_tron_legacy(), theme_tron(), theme_vader(), theme_vanilla(), theme_zebra()
)

# Then create the flextable object
flex_table <-
  flextable(table_result) |>
  set_caption(caption = as_paragraph(
    as_chunk(
      "Table 1. Differences between the default and new priors from the crosses obtained using the SSTool.",
      props = fp_text_default(color = "#000000", font.size = 14)
    )
  ),
  fp_p = fp_par(text.align = "center", padding = 5))

flex_table
```

The main difference of the genotypes obtained from the different priors are the transversions of A and C. T. The problem might be from the fact we used priors from the crosses. What I can do is to run a genotype call with the entire plate that has the samples we are comparing and generate priors for them. The SSTool requires at least 1 plate to generate new priors and we have only 18 samples. I will do that and add it to the comparisons we need to do.

#### 5.1.2 Reference and alternative alleles

Lets do a sanity check and count how many homozygous and heterozygous we have

```{r}
# Add a new column named allele_totals to sum n_A_new_prior, n_T_new_prior, n_C_new_prior, and n_G_new_prior
priors <-
  priors |>
  mutate(
    n_hom_het_default = rowSums(
      cbind(
        n_homo_ref_default_prior,
        n_homo_alt_default_prior,
        n_hetero_default_prior
      ),
      na.rm = TRUE
    ),
    n_hom_het_new = rowSums(
      cbind(
        n_homo_ref_new_prior,
        n_homo_alt_new_prior,
        n_hetero_new_prior
      ),
      na.rm = TRUE
    )
  )

# Compare the allele totals with the number of SNPs
head(priors |>
  dplyr::select(Population, Sample, n_SNPs, n_hom_het_default, n_hom_het_new))
```

The total number of SNPs match the sum of homozygous and heterozygous, so we do not have to divide by 2 as we did for the sum of alleles

```{r alleles_zygosity_1}
# we can select only one of the column since it is biallelic data
priors_ref_alt <-
  priors |>
  dplyr::select(
    Population,
    Sample,
    n_SNPs,
    n_SNPs_ref_mismatch,
    n_SNPs_alt_mismatch,
    n_homo_ref_default_prior,
    n_homo_ref_new_prior,
    n_homo_ref_mismatch,
    n_homo_alt_default_prior,
    n_homo_alt_new_prior,
    n_homo_alt_mismatch,
    n_hetero_default_prior,
    n_hetero_new_prior,
    n_hetero_mismatch
  ) |>
  arrange(
    Population, Sample
  )

# We can select or rename columns to make our table easier to understand. We can create new columns since the alt and ref allele counts are the same because the alleles are swapped when we use the new priors.

# Get the number of SNPs with the alleles swapped. Remember, for 2 mosquitoes with 10 SNPs we have 40 alleles. When we want to calculate the percentages based on the number of SNPs, we need to divided the values by 2 (two samples)
priors_ref_alt <-
  priors_ref_alt |>
  mutate(
    alleles_swapped = n_SNPs_ref_mismatch,
    hom_ref_diff = n_homo_ref_mismatch,
    hom_ref_alt = n_homo_alt_mismatch,
    het_diff = n_hetero_mismatch
  ) |>
  dplyr::select(Population,
                Sample,
                n_SNPs,
                alleles_swapped,
                hom_ref_diff,
                hom_ref_alt,
                het_diff) |>
  mutate(
    alleles_swapped = paste0(
      formatC(alleles_swapped, big.mark = ",", format = "d"),
      " (",
      round((alleles_swapped / n_SNPs) * 100, 2),
      "%)"
    ),
    hom_ref_diff = paste0(
      formatC(hom_ref_diff, big.mark = ",", format = "d"),
      " (",
      round((hom_ref_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    hom_ref_alt = paste0(
      formatC(hom_ref_alt, big.mark = ",", format = "d"),
      " (",
      round((hom_ref_alt / n_SNPs) * 100, 2),
      "%)"
    ),
    het_diff = paste0(
      formatC(het_diff, big.mark = ",", format = "d"),
      " (",
      round((het_diff / n_SNPs) * 100, 2),
      "%)"
    )
  )

# Convert head(results) to a tibble
table_result <-
  as_tibble(priors_ref_alt)

# Set theme if you want to use something different from the previous table
set_flextable_defaults(
  font.family = "Arial",
  font.size = 9,
  big.mark = ",",
  theme_fun = "theme_zebra" # try the themes: theme_alafoli(), theme_apa(), theme_booktabs(), theme_box(), theme_tron_legacy(), theme_tron(), theme_vader(), theme_vanilla(), theme_zebra()
)

# Then create the flextable object
flex_table <-
  flextable(table_result) |>
  set_caption(caption = as_paragraph(
    as_chunk(
      "Table 2. Number of alleles with alleles swapped and differences in zygosity when default and new priors of the crosses.",
      props = fp_text_default(color = "#000000", font.size = 14)
    )
  ),
  fp_p = fp_par(text.align = "center", padding = 5))

# Print the flextable
flex_table
```

### 5.2 Compare default prior and WGS

I create new priors using the SSToll from ThermoFisher and the crosses data. We can compare the genotype calls using each priors. We need to do some data tidying first.
```{r filter_samples_for_comparison_2}
# Filter rows containing "ab" in column "Comparison"
default_wgs <-
  data |>
  filter(
    Comparison == "aw"
  )

# The default priors is represented as "a" (Sample1) and the new priors are represented as "b" (Sample2)

# Change column names
colnames(default_wgs) <- gsub("sample1", "default_prior", colnames(default_wgs))
colnames(default_wgs) <- gsub("sample2", "wgs", colnames(default_wgs))

# Verify the updated column names
print(colnames(default_wgs))
```

#### 5.2.1 Allele counts

```{r summarize_allele_counts_2}
# we can calculate how many counts of each allele (A, T, C and G)
priors_allele_count_dw <-
  default_wgs |>
  dplyr::select(
    Population,
    Sample,
    n_SNPs,
    n_A_default_prior,
    n_A_wgs,
    n_T_default_prior,
    n_T_wgs,
    n_C_default_prior,
    n_C_wgs,
    n_G_default_prior,
    n_G_wgs,
  ) |>
  mutate(
    n_A_diff = (n_A_wgs / 2 - n_A_default_prior / 2),
    n_T_diff = (n_T_wgs / 2 - n_T_default_prior / 2),
    n_C_diff = (n_C_wgs / 2 - n_C_default_prior / 2),
    n_G_diff = (n_G_wgs / 2 - n_G_default_prior / 2)
  ) |>
  dplyr::select(Population,
                Sample,
                n_SNPs,
                n_A_diff,
                n_T_diff,
                n_C_diff,
                n_G_diff) |>
  arrange(Population, Sample) |>
  mutate(
    n_A_diff = paste0(
      formatC(
        n_A_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_A_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_T_diff = paste0(
      formatC(
        n_T_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_T_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_C_diff = paste0(
      formatC(
        n_C_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_C_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_G_diff = paste0(
      formatC(
        n_G_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_G_diff / n_SNPs) * 100, 2),
      "%)"
    )
  ) |>
  relocate(n_C_diff, .after = n_A_diff) # move the new columns right after n_A_diff

# Convert head(results) to a tibble
table_result <-
  as_tibble(priors_allele_count_dw)

# Set theme if you want to use something different from the previous table
set_flextable_defaults(
  font.family = "Arial",
  font.size = 9,
  big.mark = ",",
  theme_fun = "theme_zebra" # try the themes: theme_alafoli(), theme_apa(), theme_booktabs(), theme_box(), theme_tron_legacy(), theme_tron(), theme_vader(), theme_vanilla(), theme_zebra()
)

# Then create the flextable object
flex_table <-
  flextable(table_result) |>
  set_caption(caption = as_paragraph(
    as_chunk(
      "Table 3. Differences between the default prior from the WGS data.",
      props = fp_text_default(color = "#000000", font.size = 14)
    )
  ),
  fp_p = fp_par(text.align = "center", padding = 5))

# Print the flextable
flex_table
```

The main difference of the genotypes obtained from the different priors are the transversions of A and C. T. The problem might be from the fact we used priors from the crosses. What I can do is to run a genotype call with the entire plate that has the samples we are comparing and generate priors for them. The SSTool requires at least 1 plate to generate new priors and we have only 18 samples. I will do that and add it to the comparisons we need to do.

#### 5.2.2 Reference and alternative alleles

```{r alleles_zygosity_2}
# we can select only one of the column since it is biallelic data
priors_ref_alt_dw <-
  default_wgs |>
  dplyr::select(
    Population,
    Sample,
    n_SNPs,
    n_SNPs_ref_mismatch,
    n_SNPs_alt_mismatch,
    n_homo_ref_default_prior,
    n_homo_ref_wgs,
    n_homo_ref_mismatch,
    n_homo_alt_default_prior,
    n_homo_alt_wgs,
    n_homo_alt_mismatch,
    n_hetero_default_prior,
    n_hetero_wgs,
    n_hetero_mismatch
  ) |>
  arrange(
    Population, Sample
  )

# We can select or rename columns to make our table easier to understand. We can create new columns since the alt and ref allele counts are the same because the alleles are swapped when we use the new priors.
# Set the display format to avoid scientific notation
options(scipen = 999)

# Get the number of SNPs with the alleles swapped
priors_ref_alt_dw <-
  priors_ref_alt_dw |>
  mutate(
    alleles_swapped = n_SNPs_ref_mismatch,
    hom_ref_diff = n_homo_ref_mismatch,
    hom_ref_alt = n_homo_alt_mismatch,
    het_diff = n_hetero_mismatch
  ) |>
  dplyr::select(Population,
                Sample,
                n_SNPs,
                alleles_swapped,
                hom_ref_diff,
                hom_ref_alt,
                het_diff) |>
  mutate(
    alleles_swapped = paste0(
      formatC(alleles_swapped, big.mark = ",", format = "d"),
      " (",
      round((alleles_swapped / n_SNPs) * 100, 2),
      "%)"
    ),
    hom_ref_diff = paste0(
      formatC(hom_ref_diff, big.mark = ",", format = "d"),
      " (",
      round((hom_ref_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    hom_ref_alt = paste0(
      formatC(hom_ref_alt, big.mark = ",", format = "d"),
      " (",
      round((hom_ref_alt / n_SNPs) * 100, 2),
      "%)"
    ),
    het_diff = paste0(
      formatC(het_diff, big.mark = ",", format = "d"),
      " (",
      round((het_diff / n_SNPs) * 100, 2),
      "%)"
    )
  )

# Convert head(results) to a tibble
table_result <-
  as_tibble(priors_ref_alt_dw)

# Set theme if you want to use something different from the previous table
set_flextable_defaults(
  font.family = "Arial",
  font.size = 9,
  big.mark = ",",
  theme_fun = "theme_zebra" # try the themes: theme_alafoli(), theme_apa(), theme_booktabs(), theme_box(), theme_tron_legacy(), theme_tron(), theme_vader(), theme_vanilla(), theme_zebra()
)

# Then create the flextable object
flex_table <-
  flextable(table_result) |>
  set_caption(caption = as_paragraph(
    as_chunk(
      "Table 4. SNPs with alleles swapped and differences in zygosity comparing the default prior and WGS data.",
      props = fp_text_default(color = "#000000", font.size = 14)
    )
  ),
  fp_p = fp_par(text.align = "center", padding = 5))

# Print the flextable
flex_table

# Reset the display format to the default
options(scipen = 0)
```

### 5.3 Compare default prior and WGS

I create new priors using the SSToll from ThermoFisher and the crosses data. We can compare the genotype calls using each priors. We need to do some data tidying first.
```{r filter_samples_for_comparison_3}
# Filter rows containing "ab" in column "Comparison"
cross_prior_wgs <-
  data |>
  filter(
    Comparison == "bw"
  )

# The default priors is represented as "a" (Sample1) and the new priors are represented as "b" (Sample2)

# Change column names
colnames(cross_prior_wgs) <- gsub("sample1", "cross_prior", colnames(cross_prior_wgs))
colnames(cross_prior_wgs) <- gsub("sample2", "wgs", colnames(cross_prior_wgs))

# Verify the updated column names
print(colnames(cross_prior_wgs))
```

#### 5.3.1 Allele counts

```{r summarize_allele_counts_3}
# we can calculate how many counts of each allele (A, T, C and G)
priors_allele_count_nw <-
  cross_prior_wgs |>
  dplyr::select(
    Population,
    Sample,
    n_SNPs,
    n_A_cross_prior,
    n_A_wgs,
    n_T_cross_prior,
    n_T_wgs,
    n_C_cross_prior,
    n_C_wgs,
    n_G_cross_prior,
    n_G_wgs,
  ) |>
  mutate(
    n_A_diff = (n_A_wgs / 2 - n_A_cross_prior / 2),
    n_T_diff = (n_T_wgs / 2 - n_T_cross_prior / 2),
    n_C_diff = (n_C_wgs / 2 - n_C_cross_prior / 2),
    n_G_diff = (n_G_wgs / 2 - n_G_cross_prior / 2)
  ) |>
  dplyr::select(Population,
                Sample,
                n_SNPs,
                n_A_diff,
                n_T_diff,
                n_C_diff,
                n_G_diff) |>
  arrange(Population, Sample) |>
  mutate(
    n_A_diff = paste0(
      formatC(
        n_A_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_A_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_T_diff = paste0(
      formatC(
        n_T_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_T_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_C_diff = paste0(
      formatC(
        n_C_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_C_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    n_G_diff = paste0(
      formatC(
        n_G_diff,
        big.mark = ",",
        format = "f",
        digits = 0
      ),
      " (",
      round((n_G_diff / n_SNPs) * 100, 2),
      "%)"
    )
  ) |>
  relocate(n_C_diff, .after = n_A_diff) # move the new columns right after n_A_diff

# Convert head(results) to a tibble
table_result <-
  as_tibble(priors_allele_count_nw)

# Set theme if you want to use something different from the previous table
set_flextable_defaults(
  font.family = "Arial",
  font.size = 9,
  big.mark = ",",
  theme_fun = "theme_zebra" # try the themes: theme_alafoli(), theme_apa(), theme_booktabs(), theme_box(), theme_tron_legacy(), theme_tron(), theme_vader(), theme_vanilla(), theme_zebra()
)

# Then create the flextable object
flex_table <-
  flextable(table_result) |>
  set_caption(caption = as_paragraph(
    as_chunk(
      "Table 5. Allele count differences between the crosses' prior from the WGS data.",
      props = fp_text_default(color = "#000000", font.size = 14)
    )
  ),
  fp_p = fp_par(text.align = "center", padding = 5))

# Print the flextable
flex_table
```

The main difference of the genotypes obtained from the different priors are the transversions of A and C. T. The problem might be from the fact we used priors from the crosses. What I can do is to run a genotype call with the entire plate that has the samples we are comparing and generate priors for them. The SSTool requires at least 1 plate to generate new priors and we have only 18 samples. I will do that and add it to the comparisons we need to do.

#### 5.3.2 Reference and alternative alleles

```{r alleles_zygosity_3}
# we can select only one of the column since it is biallelic data
priors_ref_alt_nw <-
  cross_prior_wgs |>
  dplyr::select(
    Population,
    Sample,
    n_SNPs,
    n_SNPs_ref_mismatch,
    n_SNPs_alt_mismatch,
    n_homo_ref_cross_prior,
    n_homo_ref_wgs,
    n_homo_ref_mismatch,
    n_homo_alt_cross_prior,
    n_homo_alt_wgs,
    n_homo_alt_mismatch,
    n_hetero_cross_prior,
    n_hetero_wgs,
    n_hetero_mismatch
  ) |>
  arrange(
    Population, Sample
  )

# We can select or rename columns to make our table easier to understand. We can create new columns since the alt and ref allele counts are the same because the alleles are swapped when we use the new priors.
# Set the display format to avoid scientific notation
options(scipen = 999)

# Get the number of SNPs with the alleles swapped
priors_ref_alt_nw <-
  priors_ref_alt_nw |>
  mutate(
    alleles_swapped = n_SNPs_ref_mismatch / 2,
    hom_ref_diff = n_homo_ref_mismatch / 2,
    hom_ref_alt = n_homo_alt_mismatch / 2,
    het_diff = n_hetero_mismatch / 2
  ) |>
  dplyr::select(Population,
                Sample,
                n_SNPs,
                alleles_swapped,
                hom_ref_diff,
                hom_ref_alt,
                het_diff) |>
  mutate(
    alleles_swapped = paste0(
      formatC(alleles_swapped, big.mark = ",", format = "d"),
      " (",
      round((alleles_swapped / n_SNPs) * 100, 2),
      "%)"
    ),
    hom_ref_diff = paste0(
      formatC(hom_ref_diff, big.mark = ",", format = "d"),
      " (",
      round((hom_ref_diff / n_SNPs) * 100, 2),
      "%)"
    ),
    hom_ref_alt = paste0(
      formatC(hom_ref_alt, big.mark = ",", format = "d"),
      " (",
      round((hom_ref_alt / n_SNPs) * 100, 2),
      "%)"
    ),
    het_diff = paste0(
      formatC(het_diff, big.mark = ",", format = "d"),
      " (",
      round((het_diff / n_SNPs) * 100, 2),
      "%)"
    )
  )

# Convert head(results) to a tibble
table_result <-
  as_tibble(priors_ref_alt_nw)

# Set theme if you want to use something different from the previous table
set_flextable_defaults(
  font.family = "Arial",
  font.size = 9,
  big.mark = ",",
  theme_fun = "theme_zebra" # try the themes: theme_alafoli(), theme_apa(), theme_booktabs(), theme_box(), theme_tron_legacy(), theme_tron(), theme_vader(), theme_vanilla(), theme_zebra()
)

# Then create the flextable object
flex_table <-
  flextable(table_result) |>
  set_caption(caption = as_paragraph(
    as_chunk(
      "Table 6. SNPs with alleles swapped and differences in zygosity comparing crosses prior and WGS data.",
      props = fp_text_default(color = "#000000", font.size = 14)
    )
  ),
  fp_p = fp_par(text.align = "center", padding = 5))

# Print the flextable
flex_table

# Reset the display format to the default
options(scipen = 0)
```

Now, I have to do the genotype call using the entire plate, generate new priors, and then compare the data to the wgs data set. However, I did not do any filtering. I could do some QC in the data before any comparisons, but the total number of SNPs that I can compare will be decreased.


## 6. Across samples comparisons

Comparing the two priors
```{python stats_for_1_comparison_ab1, eval=FALSE}
import allel
import pandas as pd
import os
import numpy as np
import warnings

# Ignore DtypeWarnings from pandas
warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)

# Directory with vcf files
dir_name = "output/wgs_vs_chip/vcfs/"

# Get list of all vcf files in the directory
# vcf_files = [f for f in os.listdir(dir_name) if f.endswith('.vcf')]
# Get list of all vcf files in the directory with *_ab.vcf, *_aw.vcf or *_bw.vcf
vcf_files = [f for f in os.listdir(dir_name) if f.endswith('ab.vcf')]

csv_output_files = []

# Function to convert genotype indices to alleles
def genotype_to_alleles(gt_indices, ref_allele, alt_alleles):
    alleles = np.concatenate(([ref_allele], alt_alleles))
    return " ".join(alleles[idx] for idx in gt_indices if idx!=-1)  # idx -1 means missing data

# Iterate over VCF files
for vcf_file in vcf_files:
    file_path = os.path.join(dir_name, vcf_file)
    callset = allel.read_vcf(file_path, fields=['*'])

    # Get genotype
    gt = allel.GenotypeArray(callset['calldata/GT'])

    # Get sample names and add prefix from file name
    sample_1, sample_2 = callset['samples']
    prefix = vcf_file.split("_")[0] + "_"  # Added "_" after prefix
    sample_1 = prefix + sample_1
    sample_2 = prefix + sample_2

    # Verify the vcf contains two samples
    assert gt.shape[1] == 2, f"Expected 2 samples in {vcf_file}, found {gt.shape[1]}"

    # Create DataFrame
    df = pd.DataFrame({
        'SNP_id': callset['variants/ID'],
        f'{sample_1}_geno': [genotype_to_alleles(gt, callset['variants/REF'][i], callset['variants/ALT'][i]) for i, gt in enumerate(gt[:, 0])],
        f'{sample_2}_geno': [genotype_to_alleles(gt, callset['variants/REF'][i], callset['variants/ALT'][i]) for i, gt in enumerate(gt[:, 1])],
        f'{sample_1}_{sample_2}_gcomp': np.where(gt[:, 0] == gt[:, 1], 'match', 'mismatch').tolist(),
        f'{sample_1}_zygo': np.where(gt.is_hom_ref()[:, 0], 'hom_ref', np.where(gt.is_hom_alt()[:, 0], 'hom_alt', 'het')).tolist(),
        f'{sample_2}_zygo': np.where(gt.is_hom_ref()[:, 1], 'hom_ref', np.where(gt.is_hom_alt()[:, 1], 'hom_alt', 'het')).tolist(),
        f'{sample_1}_{sample_2}_zcomp': np.where(gt.is_hom()[:, 0] == gt.is_hom()[:, 1], 'match', 'mismatch').tolist()
    })

    output_file = f'output/wgs_vs_chip/{os.path.basename(vcf_file).replace(".vcf", "")}_comparison_ab.csv' # change the name here when you change the vcfs you are analyzing
    df.to_csv(output_file, index=False)
    csv_output_files.append(output_file)
    
# # Combine only the newly created CSVs into one

# Get the directory path where your files are located
dir_path = "output/wgs_vs_chip/"

# Get list of all CSV files in the directory that end with '_ab.csv'
csv_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('_ab.csv')]

# Ensure that we have at least one such file
if not csv_files:
    raise ValueError("No CSV files found matching '_ab.csv'")

# Load the first CSV file
combined_csv = pd.read_csv(csv_files[0])

# Merge the rest of the CSV files one by one
for f in csv_files[1:]:
    df = pd.read_csv(f)
    combined_csv = pd.merge(combined_csv, df, on='SNP_id', how='outer')

combined_csv.to_csv(os.path.join(dir_path, 'combined_comparison_ab.csv'), index=False)
```


Compare the reference and alternative allele between the two priors

Import the data and use "Tidyverse" to change column names. I left the two codes to compare the output and make sure it is creating the same object.
```{r import_csv_R_tydiverse}
data_ab <-
  read_delim(
    "output/wgs_vs_chip/combined_comparison_ab.csv",
    delim = ",",
    show_col_types = FALSE
  )
# Get all column names that end with '_gcomp'
gcomp_cols <- grep("_gcomp$", names(data_ab), value = TRUE)

# Iterate over those column names and for each, create new _ref and _alt columns
for (col in gcomp_cols) {
  data_ab <- data_ab |>
    separate(col, into = c(paste0(col, "_ref"), paste0(col, "_alt")), sep = ",") |>
    mutate(across(
      starts_with(paste0(col, "_")),
      ~ str_replace_all(., "\\[|\\]|'|[:space:]", "")
    ))
}

# Renaming columns to match the reference and alternative alleles
data_ab <-
  data_ab |>
  dplyr::rename_with(~ str_replace_all(., "_gcomp_alt$", "_ALT"),
                     ends_with("_gcomp_alt")) |>
  dplyr::rename_with(~ str_replace_all(., "_gcomp_ref$", "_REF"),
                     ends_with("_gcomp_ref"))

# Now we can count how many times each SNP had errors within the 18 samples
# Check output
head(data_ab[, c("SNP_id", names(data_ab)[grepl("_REF$|_ALT$", names(data_ab))]), with = FALSE])
```

Import the data and use "library(data.table) to change column names

```{r import_csv_R_data_table}
# Read the file with fread() function which is faster than read_delim()
data_ab_dt <-
  fread(
    here(
      "output",
      "wgs_vs_chip", 
      "combined_comparison_ab.csv"
      )
    )

# Get all column names that end with '_gcomp'
gcomp_cols <- grep("_gcomp$", names(data_ab_dt), value = TRUE)

# Convert data.frame to data.table
setDT(data_ab_dt)

# Iterate over those column names and for each, create new _REF and _ALT columns
for (col in gcomp_cols) {
  
  # Split each '_gcomp' column into '_REF' and '_ALT'
  data_ab_dt[, c(paste0(col, "_REF"), paste0(col, "_ALT")) := tstrsplit(get(col), ", ", fixed=TRUE)]
  
  # Remove unwanted characters from each new column
  data_ab_dt[, (paste0(col, "_REF")) := gsub("\\[|\\]|'", "", get(paste0(col, "_REF")))]
  data_ab_dt[, (paste0(col, "_ALT")) := gsub("\\[|\\]|'", "", get(paste0(col, "_ALT")))]
}

# Renaming columns to remove _gcomp
new_names <- names(data_ab_dt)
new_names <- gsub("_gcomp_ALT$", "_ALT", new_names)
new_names <- gsub("_gcomp_REF$", "_REF", new_names)
setnames(data_ab_dt, new_names)


# Select and display only columns that match the criteria
head(data_ab_dt[, c("SNP_id", names(data_ab_dt)[grepl("_REF$|_ALT$", names(data_ab_dt))]), with = FALSE])
```

Compare one sample to see if the counts match and mismatch are correct using "Tidyverse" or "data.table"
```{r compare_one_sample}
table(data_ab$KAT_11a_KAT_11b_REF)
table(data_ab_dt$KAT_11a_KAT_11b_REF)
```

We can also count NAs
```{r compare_one_sample_nas}
table(data_ab$KAT_11a_KAT_11b_REF, useNA = "ifany")

table(data_ab_dt$KAT_11a_KAT_11b_REF, useNA = "ifany")
```

The main difference between the objects is that we kept the original columns in data_ab_dt but not in the data_ab. It is not important but we can inspect the data for inconsistencies in our code.

Now we can change our code to get all the metrics we want. We have too many column names in our data. 
Check column names
```{r colnames_data}
colnames(data_ab_dt)
```

Check the data
```{r glimpse_data}
glimpse(data_ab_dt)
```

Although we have 129 columns, we have the comparison of each sample using different priors or genotyping technology. Then, we have genotypes of each sample, for example our first samples are  "KAT_11a_geno" and "KAT_11b_geno". In this column we have the real genotype of the sample. Here sample "a" and sample "b" are references to the two priors we are comparing (a - default and b - new prior from the crosses). Later, I will compare the default prior with the plate prior (I will create a prior using the plate that had the 18 samples we are comparing).

The next columns are the comparison of the reference and alternative alleles. The values in these columns are "match" and "mismatch". Later we can summarize the data by counting the strings "match" and "mismatch" across the 18 samples. Or if we are curious, even compare the two populations.

The next column are about the zygosity of each sample. As our first samples we have the columns: "KAT_11a_zygo" "KAT_11b_zygo" and "KAT_11a_KAT_11b_zcomp". The values in the two first columns are "hom_ref", "hom_alt", or "het". The values for the column _zcomp are "match" or "mismatch" as result of comparing the zygosity of the two columns before it. 

We can create two new columns comparing all the samples.

```{r get_summary_18_samples}
# Convert your data to a data.table (it is already)
setDT(data_ab_dt)

# Create columns for match and mismatch count for columns ending with _REF
cols_REF <- 
  grep("_REF$", names(data_ab_dt), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_ab_dt[, c("REF_match_count", "REF_mismatch_count") :=
          .(rowSums(.SD == "match", na.rm = TRUE),
            rowSums(.SD == "mismatch", na.rm = TRUE)),
        .SDcols = cols_REF]

# Create columns for match and mismatch count for columns ending with _ALT
cols_ALT <- 
  grep("_ALT$", names(data_ab_dt), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_ab_dt[, c("ALT_match_count", "ALT_mismatch_count") :=
          .(rowSums(.SD == "match", na.rm = TRUE),
            rowSums(.SD == "mismatch", na.rm = TRUE)),
        .SDcols = cols_ALT]

# Create columns for match and mismatch count for columns ending with _zcomp
cols_Zigo <-
  grep("_zcomp$", names(data_ab_dt), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_ab_dt[, c("Zigo_match_count", "Zigo_mismatch_count") :=
          .(rowSums(.SD == "match", na.rm = TRUE),
            rowSums(.SD == "mismatch", na.rm = TRUE)),
        .SDcols = cols_Zigo]

# Now, you can summarize this for each SNP_id
summary_18_samples <-
  data_ab_dt[, .(
    REF_match = sum(REF_match_count, na.rm = TRUE),
    REF_mismatch = sum(REF_mismatch_count, na.rm = TRUE),
    ALT_match = sum(ALT_match_count, na.rm = TRUE),
    ALT_mismatch = sum(ALT_mismatch_count, na.rm = TRUE),
    Zigo_match = sum(Zigo_match_count, na.rm = TRUE),
    Zigo_mismatch = sum(Zigo_mismatch_count, na.rm = TRUE)
  ),
  by = SNP_id]

# Sort data by SNP_id
setorder(summary_18_samples, SNP_id)

# Check the result
head(summary_18_samples)
```

### 6.1 Total discrepancies across all samples

How many SNPs have discrepancies in the genotypes in 1 or more samples (out of the 18 samples)

```{r subset_mismatches_01}
# Discrepancies in 1 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_ab_dt$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ab_dt[data_ab_dt$REF_mismatch_count >= 1,]$SNP_id
           )
         )
cat("REF mismatch at in 1 sample:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ab_dt[data_ab_dt$ALT_mismatch_count >= 1,,]$SNP_id
           )
         )
cat("ALT mismatch at least in 1 sample:", failed_snpsA, "\n")


# How many SNPs failed zygosity
failed_snps <-
  length(
    unique(data_ab_dt[data_ab_dt$Zigo_mismatch_count >= 1,,]$SNP_id
           )
         )
cat("Zygosity mismatch in at least 1 sample:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in 1 or more samples:", percentage_failed, "%\n")
```

We see 12,031 SNPs with discrepancies but most of them are only in 1 sample. Lets check how many have errors in two samples

```{r subset_mismatches_02}
# Discrepancies in 2 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_ab_dt$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ab_dt[data_ab_dt$REF_mismatch_count >= 2,]$SNP_id
           )
         )
cat("REF mismatch in 2 or more samples:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ab_dt[data_ab_dt$ALT_mismatch_count >= 2,]$SNP_id
           )
         )
cat("ALT mismatch in 2 or more samples:", failed_snpsA, "\n")


# How many SNPs failed
failed_snps <-
  length(
    unique(data_ab_dt[data_ab_dt$Zigo_mismatch_count >= 2,]$SNP_id
           )
         )
cat("Zygosity mismatch in 2 or more samples:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in 2 or more samples:", percentage_failed, "%\n")
```

We see that half of the SNPs have mismatching genotypes in 1 sample only and 6,061 SNPs show genotyping mismatches in 2 or more samples.

```{r subset_mismatches_002}
# Check how many SNPs with errors in only 1 sample
# How many SNPs we tested
tested_snps <- length(unique(data_ab_dt$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ab_dt[data_ab_dt$REF_mismatch_count == 1,]$SNP_id
           )
         )
cat("REF mismatch in only 1 sample:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ab_dt[data_ab_dt$ALT_mismatch_count == 1,]$SNP_id
           )
         )
cat("ALT mismatch in only 1 sample:", failed_snpsA, "\n")


# How many SNPs failed
failed_snps <-
  length(
    unique(data_ab_dt[data_ab_dt$Zigo_mismatch_count == 1,]$SNP_id
           )
         )
cat("Zygosity mismatch in only 1 sample:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in only 1 sample:", percentage_failed, "%\n")
```

We observe that 6,004 SNPs have genotype mismatches in only 1 sample out of the 18 samples. Is it random or does it follow a pattern?


Nearly half of the SNPs that have discrepancies are from a single sample genotype mismatch.

We can create a histogram of the number of errors or mismatches per sample

```{r create_facet_histogram, fig.width=9, fig.height=8}
# summary_18_samples is your data.table
setDT(summary_18_samples)

# Select only the relevant columns
dt <- 
  summary_18_samples[, .(SNP_id, REF_mismatch, ALT_mismatch, Zigo_mismatch)]

# Reshape data to long format
dt_long <- 
  melt(dt, id.vars = "SNP_id", variable.name = "type", value.name = "count")

# Convert to data.table if it's not already
setDT(dt_long)

# Convert to numeric if it's not already
dt_long[, count := as.numeric(count)]

# Count occurrences per count value
dt_long <- 
  dt_long[, .(n = .N), by = .(type, count)]

# Calculate total count of unique SNPs
total_SNP <- 
  length(unique(dt$SNP_id))

# Add a new column for the percentage
dt_long[, perc := n / total_SNP * 100]

# Define new labels
new_labels <-
  c(
  "Reference Allele" = "REF_mismatch",
  "Alternative Allele" = "ALT_mismatch",
  "Zygosity Mismatch" = "Zigo_mismatch"
)

# Apply new labels
dt_long$type <-
  fct_recode(dt_long$type, !!!new_labels)

# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R" # choose my_theme.R (Roboto Condensed) or my_theme2.R (default font)
  )
)

# Create facet histogram
ggplot(dt_long, aes(x = count, y = n)) +
  geom_bar(
    stat = "identity",
    fill = "#ffcae4",
    color = ifelse(
      dt_long$count == 0,
      "#CCFF00",
      ifelse(dt_long$count == 1, "#4169E1", "#FF7F50")
    ),
    width = 0.6,
    linewidth = 1
  ) +
  geom_text_repel(aes(label = paste0(
    scales::comma(n), " (", round(perc, 2), "%)"
  )), size = 2.7, color = "gray10") +
  facet_wrap(~ type, scales = "free_y") +
  labs(
    title = "Histogram of SNP Mismatch Counts across the 18 samples",
    x = "Count",
    y = "Frequency",
    caption = "Comparison of the genotypes of 90,834 SNPs using default and crosses priors.\n 12,030 SNPs (13.24%) have discrepancies in at least 1 sample.\n Bar border colors: Electric Lime = no errors; Royal Blue =  1 error; Coral = more than 1 error"
  ) +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(breaks = 0:18) +
  my_theme() +
  coord_flip() +
  theme(plot.caption = element_text(
    face = "italic",
    size = 10,
    color = "grey20"
  ))

# save the plot
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "default_cross_priors_mismatches.pdf"
  ),
  width  = 8,
  height = 6,
  units  = "in"
)
```

### 6.2 Within KAT and SAI

Now we can create columns to get the same statistics for each population "SAI" and "KAT".

Lets check the first SNP
```{r filter_snps_check}
# the original data
data_ab_dt |>
  dplyr::filter(SNP_id == "AX-579436089")

# or the data table
dt |>
  dplyr::filter(SNP_id == "AX-579436089")
```

We have SAI_ and KAT_; we can subset the data and compare the two populations.

Check SAI
```{r get_sai_stats_ab}
# Convert your data to a data.table
# setDT(data_ab_dt)

# Extract SAI and KAT columns
SAI_cols <- grep("^SAI_", names(data_ab_dt), value = TRUE)
KAT_cols <- grep("^KAT_", names(data_ab_dt), value = TRUE)

# Subset the data into two data tables for SAI and KAT
data_SAI <- data_ab_dt[, c('SNP_id', SAI_cols), with = FALSE]
data_KAT <- data_ab_dt[, c('SNP_id', KAT_cols), with = FALSE]

# SAI
# Create columns for match and mismatch count for columns ending with _REF
cols_REF <-
  grep("_REF$", names(data_SAI), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_SAI[, c("REF_match_count", "REF_mismatch_count") :=
           .(rowSums(.SD == "match", na.rm = TRUE),
             rowSums(.SD == "mismatch", na.rm = TRUE)),
         .SDcols = cols_REF]

# Create columns for match and mismatch count for columns ending with _ALT
cols_ALT <-
  grep("_ALT$", names(data_SAI), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_SAI[, c("ALT_match_count", "ALT_mismatch_count") :=
           .(rowSums(.SD == "match", na.rm = TRUE),
             rowSums(.SD == "mismatch", na.rm = TRUE)),
         .SDcols = cols_ALT]

# Create columns for match and mismatch count for columns ending with _zcomp
cols_Zigo <-
  grep("_zcomp$", names(data_SAI), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_SAI[, c("Zigo_match_count", "Zigo_mismatch_count") :=
           .(rowSums(.SD == "match", na.rm = TRUE),
             rowSums(.SD == "mismatch", na.rm = TRUE)),
         .SDcols = cols_Zigo]

# Now, you can summarize this for each SNP_id
summary_sai <-
  data_SAI[, .(
    REF_match = sum(REF_match_count, na.rm = TRUE),
    REF_mismatch = sum(REF_mismatch_count, na.rm = TRUE),
    ALT_match = sum(ALT_match_count, na.rm = TRUE),
    ALT_mismatch = sum(ALT_mismatch_count, na.rm = TRUE),
    Zigo_match = sum(Zigo_match_count, na.rm = TRUE),
    Zigo_mismatch = sum(Zigo_mismatch_count, na.rm = TRUE)
  ),
  by = SNP_id]

# Sort data by SNP_id
setorder(summary_sai, SNP_id)

# Check the result
head(summary_sai)
```

Now KAT

```{r get_kat_stats_ab}
# KAT
# Create columns for match and mismatch count for columns ending with _REF
cols_REF <-
  grep("_REF$", names(data_KAT), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_KAT[, c("REF_match_count", "REF_mismatch_count") :=
           .(rowSums(.SD == "match", na.rm = TRUE),
             rowSums(.SD == "mismatch", na.rm = TRUE)),
         .SDcols = cols_REF]

# Create columns for match and mismatch count for columns ending with _ALT
cols_ALT <-
  grep("_ALT$", names(data_KAT), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_KAT[, c("ALT_match_count", "ALT_mismatch_count") :=
           .(rowSums(.SD == "match", na.rm = TRUE),
             rowSums(.SD == "mismatch", na.rm = TRUE)),
         .SDcols = cols_ALT]

# Create columns for match and mismatch count for columns ending with _zcomp
cols_Zigo <-
  grep("_zcomp$", names(data_KAT), value = TRUE)

# Calculate the count of "match" or "mismatch" for each row
data_KAT[, c("Zigo_match_count", "Zigo_mismatch_count") :=
           .(rowSums(.SD == "match", na.rm = TRUE),
             rowSums(.SD == "mismatch", na.rm = TRUE)),
         .SDcols = cols_Zigo]

# Now, you can summarize this for each SNP_id
summary_kat <-
  data_KAT[, .(
    REF_match = sum(REF_match_count, na.rm = TRUE),
    REF_mismatch = sum(REF_mismatch_count, na.rm = TRUE),
    ALT_match = sum(ALT_match_count, na.rm = TRUE),
    ALT_mismatch = sum(ALT_mismatch_count, na.rm = TRUE),
    Zigo_match = sum(Zigo_match_count, na.rm = TRUE),
    Zigo_mismatch = sum(Zigo_mismatch_count, na.rm = TRUE)
  ),
  by = SNP_id]

# Sort data by SNP_id
setorder(summary_kat, SNP_id)

# Check output
head(summary_kat)
```

Make plot to visualize the output

First lets get statistics to add to the plot caption. I tried two codes to make sure we get the right output:

How many SNPs have discrepancies in the genotypes in 1 or more samples for KAT?

Code 1
```{r get_stats_kat_ab1}
# Discrepancies in 2 or more samples, we use or operator |
failed_kat_ab <-
  data_KAT |>
  dplyr::filter(REF_mismatch_count > 0 |
                  ALT_mismatch_count > 0 | Zigo_mismatch_count > 0)
# How many SNPs we tested
tested_snps <-
  length(unique(data_KAT$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snps_kat_ab <-
  length(unique(failed_kat_ab$SNP_id))
cat("Number of SNPs failed:", failed_snps_kat_ab, "\n")

# Calculate percentage
percentage_failed_kat_ab <-
  round(failed_snps_kat_ab / tested_snps * 100, 2)
cat("Percentage of failed SNPs:", percentage_failed_kat_ab, "%\n")
```

Code 2

```{r get_stats_kat_ab2}
# Discrepancies in 1 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_KAT$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_kat_ab <-
  length(unique(data_KAT[data_KAT$REF_mismatch_count > 0 |
                           data_KAT$ALT_mismatch_count > 0 |
                           data_KAT$Zigo_mismatch_count > 0, ]$SNP_id))
cat("Number of SNPs failed:", failed_kat_ab, "\n")

# Calculate percentage
percentage_failed <- round(failed_kat_ab / tested_snps * 100, 2)
cat("Percentage of failed SNPs:", percentage_failed, "%\n")
```

How many SNPs have discrepancies in the genotypes in 1 or more samples for SAI

Code 1

```{r get_stats_sai_ab1}
# Discrepancies in 2 or more samples, we use or operator |
failed_sai_ab <-
  data_SAI |>
  dplyr::filter(REF_mismatch_count > 0 |
                  ALT_mismatch_count > 0 | Zigo_mismatch_count > 0)

# How many SNPs we tested
tested_snps <-
  length(unique(data_SAI$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snps_sai_ab <-
  length(unique(failed_sai_ab$SNP_id))
cat("Number of SNPs failed:", failed_snps_sai_ab, "\n")

# Calculate percentage
percentage_failed_sai_ab <-
  round(failed_snps_sai_ab / tested_snps * 100, 2)
cat("Percentage of failed SNPs:", percentage_failed_sai_ab, "%\n")
```

Code 2

```{r get_stats_sai_ab2}
# Discrepancies in 1 or more samples
# How many SNPs we tested
tested_snps <-
  length(unique(data_SAI$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_sai_ab <-
  length(unique(data_SAI[data_SAI$REF_mismatch_count > 0 |
                             data_SAI$ALT_mismatch_count > 0 |
                             data_SAI$Zigo_mismatch_count > 0,]$SNP_id))
cat("Number of SNPs failed:", failed_sai_ab, "\n")

# Calculate percentage
percentage_failed <- 
  round(failed_sai_ab / tested_snps * 100, 2)
cat("Percentage of failed SNPs:", percentage_failed, "%\n")
```

Both codes created the same output.

Data tidying and plotting

```{r plot_sai_kat_ab, fig.width=9, fig.height=8}
# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R" # choose my_theme.R (Roboto Condensed) or my_theme2.R (default font)
  )
)

# Merge summary_sai and summary_kat
merged_sai_kat <-
  merge(summary_sai,
        summary_kat,
        by = "SNP_id",
        suffixes = c("_sai", "_kat"))

# Select only the relevant columns
dt <- 
  merged_sai_kat[, .(
  SNP_id,
  REF_mismatch_sai,
  ALT_mismatch_sai,
  Zigo_mismatch_sai,
  REF_mismatch_kat,
  ALT_mismatch_kat,
  Zigo_mismatch_kat
)]


# Reshape data to long format
dt_long <-
  melt(dt,
       id.vars = "SNP_id",
       variable.name = "type",
       value.name = "count")

# Convert to data.table if it's not already
setDT(dt_long)

# Extract the last part after "_" in the 'type' column to form 'group' column
dt_long[, group := str_extract(type, "(?<=_)[^_]+$")]

# Extract the part before the first "_" in the 'type' column to form 'allele' column
dt_long[, allele := str_extract(type, "^[^_]+")]

# Convert to numeric if it's not already
dt_long[, count := as.numeric(count)]

# Count occurrences per count value
dt_long <-
  dt_long[, .(n = .N), by = .(allele, group, count)]
# dt_long[, n := .N, by = .(allele, group, count)]

# Calculate total count of unique SNPs
total_SNP <-
  length(unique(dt$SNP_id))

# Add a new column for the percentage
dt_long[, perc := n / total_SNP * 100, by = group]

# Set levels for 'group' variable
dt_long$group <-
  factor(dt_long$group, levels = c("sai", "kat"))

# Set levels for 'allele' variable
dt_long$allele <-
  factor(dt_long$allele, levels = c("REF", "ALT", "Zigo"))

# Modify levels for 'allele' variable
levels(dt_long$allele) <-
  c("Reference Allele", "Alternative Allele", "Zygosity")

# Modify levels for 'group' variable
levels(dt_long$group) <-
  c("SAI", "KAT")

dt_long$count <-
  as.numeric(dt_long$count)

# Create plot
ggplot(dt_long, aes(x = count, y = n)) +
  geom_bar(
    stat = "identity",
    fill = "#ffcae4",
    color = ifelse(
      dt_long$count == 0,
      "#CCFF00",
      ifelse(dt_long$count == 1, "#4169E1", "#FF7F50")
    ),
    width = 0.6,
    linewidth = 1
  ) +
  geom_text_repel(aes(label = paste0(
    scales::comma(n), " (", round(perc, 2), "%)"
  )), size = 2.7, color = "gray10") +
  facet_wrap(~ group + allele, scales = "free_y", ncol = 3) +
  labs(
    title = "Histogram of SNP Mismatch Counts across all samples for each population",
    x = "Count",
    y = "Frequency",
    caption = "Comparison of the genotypes of 90,834 SNPs using default and crosses priors.\n Number of genotype discordance in at least 1 sample for each sampling locality:\n KAT 6 samples from native range         SAI 12 samples from invasive range\n Bar border colors: Electric Lime = no errors; Royal Blue =  1 error; Coral = more than 1 error \nSAI: Saint Augustine, Trinidad and Tobago -> 9,619 SNPs (10.59%)\n KAT: Kathmandu, Nepal -> 4,165 SNPs (4.59%)"
  ) +
  coord_flip() +
  my_theme() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(breaks = 0:18) +
  theme(plot.caption = element_text(
    face = "italic",
    size = 10,
    color = "grey20"
  ))

# save the plot
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "default_cross_priors_mismatches_SAI_KAT.pdf"
  ),
  width  = 8,
  height = 8,
  units  = "in"
)
```
It seems that SAI has more mismatches but it has twice as many samples than KAT. We can check the mismatches per sample.

### 6.3 Discrepancies per sample

```{r sanity_check_code_comparison}
# Initialize an empty list to hold the counts
count_list <- list()

# Select columns
matching_columns <- colnames(data_ab_dt)[grepl(pattern = "(_REF$|_ALT$|_zcomp$)", colnames(data_ab_dt))]

# Loop through each column
for (column in matching_columns) {
  match_count <-
    sum(str_detect(data_ab_dt[[column]], "match"), na.rm = TRUE)
  mismatch_count <-
    sum(str_detect(data_ab_dt[[column]], "mismatch"), na.rm = TRUE)
  
  # Create a data.table with counts for the current column
  count_dt <-
    data.table(Column = column,
               Match = match_count,
               Mismatch = mismatch_count)
  
  # Add the count data.table to the list
  count_list[[column]] <- count_dt
}

# Combine all count data.tables into a single data.table
counts_all_columns <-
  rbindlist(count_list)

# Calculate total
counts_all_columns <-
  counts_all_columns |>
  mutate(Total = Match + Mismatch)

# Create new columns: Population, Sample, and Comparison
counts_all_columns <-
  counts_all_columns |>
  mutate(
    Population = sub("^([^_]+).*", "\\1", Column),
    Sample = sub("^.*_(\\d+).*", "\\1", Column),
    Comparison = sub(".*_([^_]+)$", "\\1", Column)
  )

# Reorder the columns and create sample_id
counts_all_columns <-
  counts_all_columns |>
  dplyr::select(Population, Sample, Comparison, Match, Mismatch, Total)

# Calculate percentage columns
counts_all_columns <-
  counts_all_columns |>
  mutate(Percent_Match = round((Match / Total) * 100, 2),
         Percent_Mismatch = round((Mismatch / Total) * 100, 2))

# Replace zcomp with Zygosity
counts_all_columns$Comparison <-
  gsub("zcomp", "Zygosity", counts_all_columns$Comparison)

head(counts_all_columns)
```

Make a plot
```{r plot_per_sample_comparison, fig.width=9, fig.height=8}
# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R" # choose my_theme.R (Roboto Condensed) or my_theme2.R (default font)
  )
)

# Define color palette
color_palette <- c("#92C6FF", "#f5cb8b", "#bff28c")

# Convert Sample to numeric and sort samples numerically within each Population group
counts_all_columns$Sample <-
  as.numeric(counts_all_columns$Sample)
counts_all_columns <- 
  counts_all_columns |>
  arrange(Population, Sample)

# Convert Sample column back to factor with sorted levels within each group
counts_all_columns$Sample <-
  factor(counts_all_columns$Sample,
         levels = unique(counts_all_columns$Sample))


# Rename and reorder Comparison column
counts_all_columns <-
  counts_all_columns |>
  mutate(
    Comparison_new = recode(
      Comparison,
      "REF" = "Reference Allele",
      "ALT" = "Alternative Allele",
      "Zygosity" = "Zygosity"
    )
  ) |>
  mutate(Comparison_new = factor(
    Comparison_new,
    levels = c("Reference Allele", "Alternative Allele", "Zygosity")
  ))

# Create plot
ggplot(counts_all_columns,
       aes(x = Sample, y = Mismatch, fill = Comparison)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(Population ~ Comparison_new,
             scales = "free_y",
             space = "free") +
  coord_flip() +
  labs(
    title = "SNP Mismatch Counts per Sample",
    x = "Sample",
    y = "Mismatches",
    caption = "Genotyping errors per sample within each population using the default and the crosses priors."
  ) +
  # labs(x = "Sample", y = "Mismatch") +
  theme(panel.spacing = unit(0.5, "lines")) +
  geom_text(aes(label = paste0(
    scales::comma(Mismatch), " (", Percent_Mismatch, "%)"
  )),
  # position = position_dodge(width = 0.9),
  hjust = 1,
  size = 2.5) +
  scale_fill_manual(values = color_palette) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill = "none") +
  my_theme() +
  # theme(plot.margin = margin(10, 20, 10, 10)) +   # Increase right margin to prevent labels getting cut off
  scale_y_continuous(labels = scales::comma) +  # Add thousands separator to y-axis labels
  theme(plot.caption = element_text(
    face = "italic",
    size = 10,
    color = "grey20"
  ))

# save the plot
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "default_cross_priors_mismatches_SAI_KAT_per_sample_stats.pdf"
  ),
  width  = 8,
  height = 7,
  units  = "in"
)
```

We see that the number of mismatches are quite consistent across all 18 samples and there does not seem to be a bias towards native or invasive ranges. What we have to decide now is what is random and we can accept and what we need to filter out to avoid problems in our downstream analyses.

### 6.4 Save the data to load later

```{r savd_data_01}
# Save the data 18 samples
saveRDS(
  summary_18_samples,
  file = here(
    "output",
    "wgs_vs_chip",
    "summary_18_samples.rds"
  )
)

# Save the data KAT
saveRDS(
  summary_kat,
  file = here(
    "output",
    "wgs_vs_chip",
    "summary_kat.rds"
  )
)


# Save the data SAI
saveRDS(
  summary_sai,
  file = here(
    "output",
    "wgs_vs_chip",
    "summary_sai.rds"
  )
)


# Save the data
saveRDS(
  counts_all_columns,
  file = here(
    "output",
    "wgs_vs_chip",
    "counts_all_columns.rds"
  )
)

# Save the data
saveRDS(
  data_ab_dt,
  file = here(
    "output",
    "wgs_vs_chip",
    "data_ab_dt.rds"
  )
)
```

### 6.5 SNPs with errors in 2 or more samples

We can compare the SNP with 2 or more samples with discrepancies with the SNPs that did not pass our segregation test.

```{r}
# Load the data
data_ab_dt <-
  readRDS(
    file = here(
      "output",
      "wgs_vs_chip",
      "data_ab_dt.rds"
    )
  )
```

Get the SNPs that have errors in 2 or more samples
```{r subset_mismatches_03}
# Discrepancies in 2 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_ab_dt$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ab_dt[data_ab_dt$REF_mismatch_count >= 2,]$SNP_id
           )
         )
cat("REF mismatch at in 2 samples:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ab_dt[data_ab_dt$ALT_mismatch_count >= 2,]$SNP_id
           )
         )
cat("ALT mismatch at least in 2 samples:", failed_snpsA, "\n")


# How many SNPs failed zygosity
failed_snps <-
  length(
    unique(data_ab_dt[data_ab_dt$Zigo_mismatch_count >= 2,]$SNP_id
           )
         )
cat("Zygosity mismatch in at least 2 samples:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in 2 or more samples:", percentage_failed, "%\n")
```

Get the SNP ids

```{r get_mismatch_SNP_ids_2_samples}
failed_snps_ids <-
  unique(
    data_ab_dt[data_ab_dt$Zigo_mismatch_count >= 2, ]$SNP_id
    )

# Define the file path
file_path <- here("output",
                  "wgs_vs_chip",
                  "SNPs_failed_2_samples.txt")

# Write unique SNPs to the file
writeLines(failed_snps_ids, con = file_path)
```

### 6.6 Venn diagram fail Mendel and mismatches

Create a Venn diagram between the SNPs with genotyping mismatches and those that failed our segregation test

```{r Venn_diagram_segregation_genotype_mismatches, fig.width=9, fig.height=8}
# Read in the two files as vectors
fail_mendel <-
  read_table(
    here(
     "output", 
     "segregation",
     "albopictus",
     "albopictus_SNPs_fail_segregation.txt"
    ),
    col_names = FALSE,
    show_col_types = FALSE
    )[[1]]

fail_geno <-
  read_table(
    here(
     "output", 
     "wgs_vs_chip",
     "SNPs_failed_2_samples.txt"
    ),
    col_names = FALSE,
    show_col_types = FALSE
    )[[1]]

# Calculate shared values
errors_SNPs <-
  intersect(
    fail_mendel,
    fail_geno
  )


# Create Venn diagram
venn_data <-
  list(
    "Fail Mendel" = fail_mendel,
    "Genotype Mismatches" = fail_geno
  )
venn_plot <-
  ggvenn(
    venn_data,
    fill_color = c("steelblue", "darkorange"),
    show_percentage = TRUE
  )

# Add a title
venn_plot <-
  venn_plot +
  ggtitle("Comparison of SNPs with errors") +
  theme(plot.title = element_text(hjust = .5))

# Display the Venn diagram
print(venn_plot)

# Save Venn diagram to PDF
output_path <-
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "Mendel_geno_priors.pdf"
  )
ggsave(
  output_path,
  venn_plot,
  height = 6,
  width = 6,
  dpi = 300
)
```

### 6.8 PCA before and after removing SNPs

We can prepare a PCA before and after removing the SNPs with errors. First let's combine the two vectors with the SNP ids with errors

```{r combine_save_snps_with_erros}
# Combine vectors
combined_errors <-
  unique(c(fail_mendel,
           fail_geno))

# Write to file
write.table(
  combined_errors,
  file = here(
    "output",
    "wgs_vs_chip",
    "SNPs_with_errors.txt"
  ),
  row.names = FALSE,
  col.names = FALSE,
  quote = FALSE
)
```

Now use Plink to create PCA excluding only the SNPs that failed our segregation test

Lets import our .fam file to filter the IDs we want to compare.

```{r select_samples_pca}
# Read the data
fam_data <-
  here("output", "wgs_vs_chip", "wgs_chip.fam") |>
  read_delim(
    delim = "\t",
    col_names = FALSE,
    show_col_types = FALSE
   ) |>
  setNames(
    c(
      "FID", "IID", "PID", "MID", "Sex", "Phenotype"
      )
    )

# Filter the data
filtered_data <-
  fam_data |>
  dplyr::filter(stringr::str_detect(IID, "a$|b$")) |>
  dplyr::select("FID", "IID")

# Save to file
write.table(
  filtered_data,
  file = here("output", "wgs_vs_chip", "samples_priors.txt"),
  quote = FALSE,
  sep = " ",
  row.names = FALSE,
  col.names = FALSE
)
```

Use Plink with only the samples we are comparing (priors) and remove SNPs that failed Mendel test

```{bash pca_1}
# Before
plink \
--allow-extra-chr \
--keep-allele-order \
--bfile output/wgs_vs_chip/wgs_chip \
--exclude output/segregation/albopictus/albopictus_SNPs_fail_segregation.txt \
--keep output/wgs_vs_chip/samples_priors.txt \
--pca \
--geno 0.1 \
--maf 0.05 \
--out output/wgs_vs_chip/priors_pca_1 \
--silent
```

Now do it again but remove both SNPs that failed Mendel test and that have genotype mismatches in at least 2 samples (plus those with segregation errors).

```{bash pca_2}
# After
plink \
--allow-extra-chr \
--keep-allele-order \
--bfile output/wgs_vs_chip/wgs_chip \
--exclude output/wgs_vs_chip/SNPs_with_errors.txt \
--keep output/wgs_vs_chip/samples_priors.txt \
--pca \
--geno 0.1 \
--maf 0.05 \
--out output/wgs_vs_chip/priors_pca_2 \
--silent
```

Create PCA plot
```{r pca_3, fig.width=9, fig.height=8}
# Load the PCA results
pca_1 <-
  read.table(here("output", "wgs_vs_chip", "priors_pca_1.eigenvec"),
             header = FALSE)
colnames(pca_1) <- c("FID", "IID", paste0("PC", 1:(ncol(pca_1) - 2)))
pca_1$analysis <- "Before"
pca_1$group <- ifelse(
  stringr::str_detect(pca_1$IID, "a$"),
  "a",
  ifelse(stringr::str_detect(pca_1$IID, "b$"), "b", "Other")
)

pca_2 <-
  read.table(here("output", "wgs_vs_chip", "priors_pca_2.eigenvec"),
             header = FALSE)
colnames(pca_2) <- c("FID", "IID", paste0("PC", 1:(ncol(pca_2) - 2)))
pca_2$analysis <- "After"
pca_2$group <- ifelse(
  stringr::str_detect(pca_2$IID, "a$"),
  "a",
  ifelse(stringr::str_detect(pca_2$IID, "b$"), "b", "Other")
)

# Combine the data
combined_pca <- rbind(pca_1, pca_2)

# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R"
  )
)

# Convert the 'analysis' column to a factor and specify the level order
combined_pca$analysis <- 
  factor(combined_pca$analysis, levels = c("Before", "After"))

# Create a facet plot
ggplot(combined_pca, aes(x = PC1, y = PC2, color = group, shape = group)) +
  geom_point(size = 2) +
  facet_grid(FID ~ analysis, scales = "free") +
  # geom_text_repel(aes(label = IID), size = 3, max.overlaps = Inf) + 
  labs(
    x = "PC1",
    y = "PC2",
    title = "The effect of SNPs with genotyping mismatches in 2 or more samples",
    colour = "Prior",
    shape = "Prior",
    caption = "Removing SNPs with genotypes errors in at least 2 samples. \n'Before' with 71,144 SNPs 'After' with 66,485 SNPs (--maf 0.05 and --geno 0.1)."
  ) +
  my_theme() +
  scale_color_manual(
    values = c(
      "a" = "lightblue",
      "b" = "orange",
      "Other" = "black"
    ),
    labels = c("a" = "Default", "b" = "Crosses", "Other" = "Other")
  ) +
  theme(plot.caption = element_text(
    face = "italic",
    size = 10,
    color = "grey20"
  ),
  legend.position = "top") +
  scale_shape_manual(
    values = c(
      "a" = 19,  # Filled circle
      "b" = 1,  # Open circle
      "Other" = 3  # Plus
    ),
    labels = c("a" = "Default", "b" = "Crosses", "Other" = "Other")
  )

# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "PCA_before_after_remove_SNPs_errors_2_or_more_samples.pdf"
  ),
  height = 6,
  width = 6,
  dpi = 300
)
```

We can remove all SNPs with errors and then we would have a perfect overlap of the points. The frequencies and genotypes would be all the same. It is interesting to know that we can see the effect of few thousand SNPs (~ 6k) that have 1 genotype wrong in 1 sample out of the 18 samples.

## 7. New genotype calls for WGS data on the cluster

Because we extracted the genotypes of the WGS samples from the output of the genotype call using 819 genomes, with KAT and SAI having more samples than we are analyzing here, I will re-do the genotype call using only the samples we have here. Then we can compare the results. One would think that it is okay to subset a dataset and compare it to another one. However, since we used ANGSD doing the genotype calls using all samples, we have the opportunity to compare the outcomes.

We can use the "filtered_data" object to get the sample IDs we need.
```{r get_wgs_samples_new_genotype_call}
# Removing 'a' and 'b' from IID column
samples_wgs <- 
  filtered_data |>
  mutate(IID = str_remove_all(IID, "[ab]")) |>
  dplyr::select(FID, IID) |>
  distinct()

# Get the number of samples
length(samples_wgs$IID)
```

Check the wgs samples
```{r}
samples_wgs
```


We have a total of 30 samples for KAT + SAI
```{bash on_the_cluster, eval=FALSE}
ls -1 *.cram | wc -l
# 30
```

The name of the wgs samples on the cluster
```{r sample_name_wgs_cluster, eval=FALSE}
# all 30 samples for genotype call
# Kathmandu_Nepal_F_10.cram
# Kathmandu_Nepal_F_11.cram
# Kathmandu_Nepal_F_12.cram
# Kathmandu_Nepal_F_7.cram
# Kathmandu_Nepal_F_8.cram
# Kathmandu_Nepal_F_9.cram
# Kathmandu_Nepal_M_1.cram
# Kathmandu_Nepal_M_2.cram
# Kathmandu_Nepal_M_3.cram
# Kathmandu_Nepal_M_4.cram
# Kathmandu_Nepal_M_5.cram
# Kathmandu_Nepal_M_6.cram
# StAugustine_Trinidad_F_12.cram
# StAugustine_Trinidad_F_13.cram
# StAugustine_Trinidad_F_14.cram
# StAugustine_Trinidad_F_15.cram
# StAugustine_Trinidad_F_16.cram
# StAugustine_Trinidad_F_17.cram
# StAugustine_Trinidad_F_18.cram
# StAugustine_Trinidad_F_1.cram
# StAugustine_Trinidad_F_2.cram
# StAugustine_Trinidad_F_3.cram
# StAugustine_Trinidad_F_4.cram
# StAugustine_Trinidad_F_5.cram
# StAugustine_Trinidad_F_6.cram
# StAugustine_Trinidad_M_10.cram
# StAugustine_Trinidad_M_11.cram
# StAugustine_Trinidad_M_7.cram
# StAugustine_Trinidad_M_8.cram
# StAugustine_Trinidad_M_9.cram


# we will do a genotype call with the 18 samples
# Kathmandu_Nepal_F_10.cram
# Kathmandu_Nepal_F_11.cram
# Kathmandu_Nepal_F_12.cram
# Kathmandu_Nepal_F_7.cram
# Kathmandu_Nepal_F_8.cram
# Kathmandu_Nepal_F_9.cram
# StAugustine_Trinidad_F_1.cram
# StAugustine_Trinidad_F_2.cram
# StAugustine_Trinidad_F_3.cram
# StAugustine_Trinidad_F_4.cram
# StAugustine_Trinidad_F_5.cram
# StAugustine_Trinidad_F_12.cram
# StAugustine_Trinidad_F_13.cram
# StAugustine_Trinidad_F_14.cram
# StAugustine_Trinidad_F_15.cram
# StAugustine_Trinidad_F_16.cram
# StAugustine_Trinidad_F_17.cram
# StAugustine_Trinidad_F_18.cram
```

On the cluster the data is at /ycga-gpfs/project/caccone/lvc26/september_2020/crams

We can do two genotype calls. One with all samples and one with the samples (30) we genotyped with the chip (18). Then, we can compare the results with the extracted genotypes of the 18 samples. We extracted it from a file that we created using angsd and 819 samples.

We can use the same script that we used for the genotype calls, but change the samples and the sites file (use only the one we have in the chip).

To create a sites file we can use the .bim file of the wgs data with all the sites we have in the chip (175k)

### 7.1 Batch scripts

Here is a batch script I used for the genotype calls
```{bash batch_file_819, eval=FALSE}
#!/bin/sh
#SBATCH --mail-type=BEGIN,END,FAIL          
#SBATCH --mail-user=luciano.cosme@yale.edu 
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=20          
#SBATCH --mem-per-cpu=6gb                  
#SBATCH --time=120:00:00 
#SBATCH --array=1-819
#SBATCH --job-name=angsd_chr
#SBATCH -o angsd_chr.%A_%a.o.txt
#SBATCH -e angsd_chr.%A_%a.ERROR.txt

cd /gpfs/ycga/project/caccone/lvc26/september_2020/snp_calls/chunk_calls

samplesheet="scaffolds.txt"

threads=$SLURM_JOB_CPUS_PER_NODE

name=`sed -n "$SLURM_ARRAY_TASK_ID"p $samplesheet |  awk '{print $1}'`

/home/lvc26/project/angsd/angsd \
-ref /gpfs/ycga/project/caccone/lvc26/september_2020/genome/aedes_albopictus_LA2_20200826.fasta \
-bam /gpfs/ycga/project/caccone/lvc26/september_2020/snp_calls/bams.txt \
-nThreads 40 \
-r $name \
-gl 1 \
-dopost 1 \
-doMaf 2 \
-doMajorMinor 4 \
-minMapQ 20 \
-minQ 10 \
-remove_bads 1 \
-uniqueOnly 1 \
-sites /gpfs/ycga/project/caccone/lvc26/september_2020/sites/cat/intersects/shared/shared_sites.txt \
-doCounts 1 \
-setMinDepthInd 10 \
-minInd 2 \
-SNP_pval 1e-6 \
-doPlink 2 \
-doGeno 4 \
-capDepth 45 \
-minMaf 0.01 \
-out $name
```

We need to create two lists of cram files and a new sites file.

Create new sites file. First, check the .bim file
```{bash wgs_bim_file_1}
head output/wgs_vs_chip/wgs_01.bim
```

We can get the first (chromosome) and forth column (position) to create a sites file. Check how many SNPs we have in the .bim file

```{bash wgs_bim_file_2}
wc -l output/wgs_vs_chip/wgs_01.bim
```

We can use "awk" to do what we need

```{zsh create_new_sites_file}
awk '{print "chr"$1, $4}' output/wgs_vs_chip/wgs_01.bim > output/wgs_vs_chip/new_calls/wgs_sites.txt;
head output/wgs_vs_chip/new_calls/wgs_sites.txt
```

The reference genome that I used had "chr" before the scaffold names. We need to use it to match the genome. It is easy to remove or add it.

We can create a file with the SNP id that ANGSD creates (chromosome_position)
```
chr1.1 chr1.1_97856
chr1.1 chr1.1_161729
chr1.1 chr1.1_229640
chr1.1 chr1.1_305518
chr1.1 chr1.1_308124
chr1.1 chr1.1_311920
chr1.1 chr1.1_315059
chr1.1 chr1.1_315386
```

```{bash create_new_sites_file2}
awk -v OFS='\t' '{$6="chr"$1 "_" $4; $7="chr" $1; print $1, $7, $4, $6, $2}' output/wgs_vs_chip/wgs_01.bim > output/wgs_vs_chip/new_calls/wgs_snps_ids.txt;
head output/wgs_vs_chip/new_calls/wgs_snps_ids.txt
```

We can use this file to replace the SNP ids that we will get with ANGSD.

We can add the SNP ids (AX-) to our file to convert between the two SNP names. We can use the position as reference when replacing the SNP id that ANGSD creates and the ones we have in the chip.

Since we are using only 175k sites instead of over 300 million when we did a genotype call, we do not need to split the genome into chunks or scaffolds. We can do a genotype call for the entire genome.

Index the sites file with ANGSD on the cluster
```{bash angsd_index_cluster, eval=FALSE}
/home/lvc26/project/angsd/angsd sites index wgs_sites.txt
```

Now we create the list of cram files. 

```{r on_the_cluster_cram_lists, eval=FALSE}
# Define path and file names
path <- "/ycga-gpfs/project/caccone/lvc26/september_2020/crams/"
samples_30 <-
  c(
    "Kathmandu_Nepal_F_10.cram",
    "Kathmandu_Nepal_F_11.cram",
    "Kathmandu_Nepal_F_12.cram",
    "Kathmandu_Nepal_F_7.cram",
    "Kathmandu_Nepal_F_8.cram",
    "Kathmandu_Nepal_F_9.cram",
    "Kathmandu_Nepal_M_1.cram",
    "Kathmandu_Nepal_M_2.cram",
    "Kathmandu_Nepal_M_3.cram",
    "Kathmandu_Nepal_M_4.cram",
    "Kathmandu_Nepal_M_5.cram",
    "Kathmandu_Nepal_M_6.cram",
    "StAugustine_Trinidad_F_12.cram",
    "StAugustine_Trinidad_F_13.cram",
    "StAugustine_Trinidad_F_14.cram",
    "StAugustine_Trinidad_F_15.cram",
    "StAugustine_Trinidad_F_16.cram",
    "StAugustine_Trinidad_F_17.cram",
    "StAugustine_Trinidad_F_18.cram",
    "StAugustine_Trinidad_F_1.cram",
    "StAugustine_Trinidad_F_2.cram",
    "StAugustine_Trinidad_F_3.cram",
    "StAugustine_Trinidad_F_4.cram",
    "StAugustine_Trinidad_F_5.cram",
    "StAugustine_Trinidad_F_6.cram",
    "StAugustine_Trinidad_M_10.cram",
    "StAugustine_Trinidad_M_11.cram",
    "StAugustine_Trinidad_M_7.cram",
    "StAugustine_Trinidad_M_8.cram",
    "StAugustine_Trinidad_M_9.cram"
  )


# Combine path and file names
full_paths_30 <- file.path(path, samples_30)

# Write to a text file
writeLines(full_paths_30, here("output","wgs_vs_chip", "new_calls", "crams_30.txt"))

# 18 samples
samples_18 <-
  c(
    "Kathmandu_Nepal_F_10.cram",
    "Kathmandu_Nepal_F_11.cram",
    "Kathmandu_Nepal_F_12.cram",
    "Kathmandu_Nepal_F_7.cram",
    "Kathmandu_Nepal_F_8.cram",
    "Kathmandu_Nepal_F_9.cram",
    "StAugustine_Trinidad_F_1.cram",
    "StAugustine_Trinidad_F_2.cram",
    "StAugustine_Trinidad_F_3.cram",
    "StAugustine_Trinidad_F_4.cram",
    "StAugustine_Trinidad_F_5.cram",
    "StAugustine_Trinidad_F_12.cram",
    "StAugustine_Trinidad_F_13.cram",
    "StAugustine_Trinidad_F_14.cram",
    "StAugustine_Trinidad_F_15.cram",
    "StAugustine_Trinidad_F_16.cram",
    "StAugustine_Trinidad_F_17.cram",
    "StAugustine_Trinidad_F_18.cram"
  )


# Combine path and file names
full_paths_18 <- file.path(path, samples_18)

# Write to a text file
writeLines(full_paths_18, here("output","wgs_vs_chip", "new_calls", "crams_18.txt"))
```

Now we have to create the batch scripts to submit in the cluster

30 samples
```{bash batch_script_30_samples, eval=FALSE}
#!/bin/sh
#SBATCH --mail-type=BEGIN,END,FAIL          
#SBATCH --mail-user=luciano.cosme@yale.edu 
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=20          
#SBATCH --mem-per-cpu=5gb                  
#SBATCH --time=120:00:00 
#SBATCH --job-name=angsd_wgs_chip_30
#SBATCH -o angsd_wgs_chip_30%A_%a.o.txt
#SBATCH -e angsd_wgs_chip_30%A_%a.ERROR.txt

cd /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls

/home/lvc26/project/angsd/angsd \
-ref /gpfs/ycga/project/caccone/lvc26/september_2020/genome/aedes_albopictus_LA2_20200826.fasta \
-bam /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/crams_30.txt \
-nThreads 40 \
-gl 1 \
-dopost 1 \
-doMaf 2 \
-doMajorMinor 4 \
-minMapQ 20 \
-minQ 10 \
-remove_bads 1 \
-uniqueOnly 1 \
-sites /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/wgs_sites.txt \
-doCounts 1 \
-setMinDepthInd 10 \
-minInd 2 \
-SNP_pval 1e-6 \
-doPlink 2 \
-doGeno 4 \
-capDepth 45 \
-minMaf 0.01 \
-out /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/wgs_chip_30
```

18 samples
```{bash batch_script_18_samples, eval=FALSE}
#!/bin/sh
#SBATCH --mail-type=BEGIN,END,FAIL          
#SBATCH --mail-user=luciano.cosme@yale.edu 
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=20          
#SBATCH --mem-per-cpu=5gb                  
#SBATCH --time=120:00:00 
#SBATCH --job-name=angsd_wgs_chip_18
#SBATCH -o angsd_wgs_chip_18%A_%a.o.txt
#SBATCH -e angsd_wgs_chip_18%A_%a.ERROR.txt

cd /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls

/home/lvc26/project/angsd/angsd \
-ref /gpfs/ycga/project/caccone/lvc26/september_2020/genome/aedes_albopictus_LA2_20200826.fasta \
-bam /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/crams_18.txt \
-nThreads 40 \
-gl 1 \
-dopost 1 \
-doMaf 2 \
-doMajorMinor 4 \
-minMapQ 20 \
-minQ 10 \
-remove_bads 1 \
-uniqueOnly 1 \
-sites /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/wgs_sites.txt \
-doCounts 1 \
-setMinDepthInd 10 \
-minInd 2 \
-SNP_pval 1e-6 \
-doPlink 2 \
-doGeno 4 \
-capDepth 45 \
-minMaf 0.01 \
-out /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/wgs_chip_18
```

### 7.2 Convert tped to bed on the cluster

Once the genotype calls are done we can convert the tped to bed file. Our file has only the SNPs from the list we supplied. We can double check and extract the SNP ids to see if everything works.

```{bash cluster_plink_make_bed_1, eval=FALSE}
awk -v OFS='\t' '{$6="chr"$1 "_" $4; $7="chr" "_"$1; print $7, $6}' output/wgs_vs_chip/wgs_01.bim > output/wgs_vs_chip/new_calls/SNPs_175k.txt;
head output/wgs_vs_chip/new_calls/SNPs_175k.txt
```

Now extract the SNPs and create new bed file

```{bash cluster_plink_make_bed_2, eval=FALSE}
# Load Plink
module load PLINK/1.90-beta4.4

cd /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls

# 30 samples
# Run Plink and extract the 175k SNPs
plink \
--allow-extra-chr \
--keep-allele-order \
--tfile wgs_chip_30 \
--make-bed \
--extract SNPs_175k.txt \
--out wgs_chip_30

# 128238 MB RAM detected; reserving 64119 MB for main workspace.
# Processing .tped file... done.
# wgs_chip_30-temporary.bed + wgs_chip_30-temporary.bim +
# wgs_chip_30-temporary.fam written.
# 169798 variants loaded from .bim file.
# 30 people (0 males, 0 females, 30 ambiguous) loaded from .fam.
# Ambiguous sex IDs written to wgs_chip_30.nosex .
# --extract: 169798 variants remaining.
# Using 1 thread (no multithreaded calculations invoked).
# Before main variant filters, 30 founders and 0 nonfounders present.
# Calculating allele frequencies... done.
# 169798 variants and 30 people pass filters and QC.
# Note: No phenotypes present.
# --make-bed to wgs_chip_30.bed + wgs_chip_30.bim + wgs_chip_30.fam ... done.

# 18 samples
# Run Plink and extract the 175k SNPs
plink \
--allow-extra-chr \
--keep-allele-order \
--tfile wgs_chip_18 \
--make-bed \
--extract SNPs_175k.txt \
--out wgs_chip_18

# 128238 MB RAM detected; reserving 64119 MB for main workspace.
# Processing .tped file... done.
# wgs_chip_18-temporary.bed + wgs_chip_18-temporary.bim +
# wgs_chip_18-temporary.fam written.
# 165104 variants loaded from .bim file.
# 18 people (0 males, 0 females, 18 ambiguous) loaded from .fam.
# Ambiguous sex IDs written to wgs_chip_18.nosex .
# --extract: 165104 variants remaining.
# Using 1 thread (no multithreaded calculations invoked).
# Before main variant filters, 18 founders and 0 nonfounders present.
# Calculating allele frequencies... done.
# 165104 variants and 18 people pass filters and QC.
# Note: No phenotypes present.
# --make-bed to wgs_chip_18.bed + wgs_chip_18.bim + wgs_chip_18.fam ... done.
```

### 7.3 Check and update SNP ids for wgs

The only last thing we need to adjust is to make sure our files have the same IDs for chromosome and SNPs. The reference genome used for mapping had "chr" before each scaffold name. When we do a genotype call with the chip data, we do not have the extra string "chr" in each scaffold name. Therefore, we need to adjust that to compare the samples. I did remove the string "chr" from the reference genome. We can remove it from our bed file using any tool.

In the past I did a genotype call for each population. We have 18 samples for SAI and 12 samples for KAT. We had DNA left over for 6 samples for KAT and 12 samples for SAI. That makes everything more complicated to compare. We have to make sure that there is no differences in the genotype calls based on the number of samples with which we do the calls.

For now. I will compare the results of the wgs calls using the 819 samples (all populations), 30 samples (both populations, KAT and SAI), and 18 samples (only the samples we have chip data).

For the chip calls, I did a call using only the 18 samples. Since I did not have more samples, I did a genotype call using the entire plate of samples where the 18 samples were (95 samples total). Finally, I did a genotype call using all wild samples (native and invasive ranges) we have in the manuscript with the 18 samples.

Therefore, we have 3 wgs calls and 3 chip calls. I decided not to compare the priors. We can compare the priors separately. 

Lets get the data in the same format. I download the data from the cluster and put it in the dir "new_calls"

Check the .bim file after downloading it from the cluster

```{bash check_1_bim_file}
head output/wgs_vs_chip/new_calls/wgs_chip_18.bim
```

Now check how the chip data is different

```{bash check_1_bim_file2}
head output/wgs_vs_chip/chip_dp_01.bim
```

We can see first they are not in the same order and that the SNP ids are different. We can use the file we created earlier to update the ids.

Check the file
```{bash}
head output/wgs_vs_chip/new_calls/wgs_snps_ids.txt
```


We can import the files, but make sure we keep the same order of the "wgs_chip_18.bim", we can create an index once we import.

```{r update_SNP_ids_18_samples}
# Define file paths using here
bim_file <-
  here("output", "wgs_vs_chip", "new_calls", "wgs_chip_18.bim")
snp_ids_file <-
  here("output", "wgs_vs_chip", "new_calls", "wgs_snps_ids.txt")
output_file <-
  here("output",
       "wgs_vs_chip",
       "new_calls",
       "wgs_chip_18_updated.bim")

# Import the .bim file
bim_data <- read_delim(
  bim_file,
  delim = "\t",
  show_col_types = FALSE,
  col_names = c("chr", "id_match", "cm", "bp", "allele1", "allele2"),
  col_types = cols(.default = col_character())
)

# Create an index column
bim_data <- 
  bim_data |>
  mutate(index = row_number()) |>
  # Remove the string "chr" from the chr column
  mutate(chr = str_remove(chr, "chr"))

# Import the .txt file
snp_ids <- read_delim(
  snp_ids_file,
  delim = "\t",
  show_col_types = FALSE,
  col_names = c("chr_ref", "id_ref", "bp_ref", "id_match", "snp_id"),
  col_types = cols(.default = col_character())
)

# Merge the two data frames by matching chr and bp in bim_data with chr_ref and bp_ref in snp_ids
merged_data <-
  left_join(bim_data, snp_ids, by = "id_match") |>
  dplyr::select(
    chr, snp_id, cm, bp, allele1, allele2
  )

# Check output
head(merged_data)

# Write the updated data frame to a new .bim file without headers or quotes
write.table(
  merged_data,
  file = output_file,
  sep = "\t",
  quote = FALSE,
  row.names = FALSE,
  col.names = FALSE
)
```

Now, add word "backup" to the current .bim file and then delete "updated" from the new file we save. Then it replaces the current .bim file

Compare both .bim files to see if they look okay

Before
```{bash}
head output/wgs_vs_chip/new_calls/wgs_chip_18.bim
```

After
```{bash}
head output/wgs_vs_chip/new_calls/wgs_chip_18_updated.bim
```

It looks okay. We can replace the original file with the new file

```{bash move_bim_files_18_samples}
mv output/wgs_vs_chip/new_calls/wgs_chip_18.bim output/wgs_vs_chip/new_calls/wgs_chip_18_backup.bim;
mv output/wgs_vs_chip/new_calls/wgs_chip_18_updated.bim output/wgs_vs_chip/new_calls/wgs_chip_18.bim;
```


### 7.4 Set reference allele

We can check if everything is working by checking the reference allele using the genome without the string 'chr'

```{bash set_alleles_18_samples2}
plink2 \
--allow-extra-chr \
--bfile output/wgs_vs_chip/new_calls/wgs_chip_18 \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/wgs_vs_chip/new_calls/wgs_chip_18_samples \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants" output/wgs_vs_chip/new_calls/wgs_chip_18_samples.log # to get the number of variants from the log file.
```

We updated the alleles and now we can do the same operation for the other file with the 30 samples.

```{r update_SNP_ids_30_samples}
# Define file paths using here
bim_file <-
  here("output", "wgs_vs_chip", "new_calls", "wgs_chip_30.bim")
snp_ids_file <-
  here("output", "wgs_vs_chip", "new_calls", "wgs_snps_ids.txt")
output_file <-
  here("output",
       "wgs_vs_chip",
       "new_calls",
       "wgs_chip_30_updated.bim")

# Import the .bim file
bim_data <- read_delim(
  bim_file,
  delim = "\t",
  show_col_types = FALSE,
  col_names = c("chr", "id_match", "cm", "bp", "allele1", "allele2"),
  col_types = cols(.default = col_character())
)

# Create an index column
bim_data <- 
  bim_data |>
  mutate(index = row_number()) |>
  # Remove the string "chr" from the chr column
  mutate(chr = str_remove(chr, "chr"))

# Import the .txt file
snp_ids <- read_delim(
  snp_ids_file,
  delim = "\t",
  show_col_types = FALSE,
  col_names = c("chr_ref", "id_ref", "bp_ref", "id_match", "snp_id"),
  col_types = cols(.default = col_character())
)

# Merge the two data frames by matching chr and bp in bim_data with chr_ref and bp_ref in snp_ids
merged_data <-
  left_join(bim_data, snp_ids, by = "id_match") |>
  dplyr::select(
    chr, snp_id, cm, bp, allele1, allele2
  )

# Check output
head(merged_data)

# Write the updated data frame to a new .bim file without headers or quotes
write.table(
  merged_data,
  file = output_file,
  sep = "\t",
  quote = FALSE,
  row.names = FALSE,
  col.names = FALSE
)
```

Compare both .bim files to see if they look okay

Before
```{bash}
head output/wgs_vs_chip/new_calls/wgs_chip_30.bim
```

After
```{bash}
head output/wgs_vs_chip/new_calls/wgs_chip_30_updated.bim
```

It looks okay. We can replace the original file with the new file

```{bash move_bim_files_30_samples}
mv output/wgs_vs_chip/new_calls/wgs_chip_30.bim output/wgs_vs_chip/new_calls/wgs_chip_30_backup.bim;
mv output/wgs_vs_chip/new_calls/wgs_chip_30_updated.bim output/wgs_vs_chip/new_calls/wgs_chip_30.bim
```

We can check if everything is working by checking the reference allele using the genome without the string 'chr'

```{bash set_alleles_18_samples}
plink2 \
--allow-extra-chr \
--bfile output/wgs_vs_chip/new_calls/wgs_chip_30 \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/wgs_vs_chip/new_calls/wgs_chip_30_samples \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants" output/wgs_vs_chip/new_calls/wgs_chip_30_samples.log # to get the number of variants from the log file.
```

The bed file with the genotypes for the 18 samples extracted after a genotype call with all 819 samples is in our directory. We already set the reference alleles. Check the log of the file

```{bash check_18_819_call}
head -n 100 output/wgs_vs_chip/wgs_01.log
```

## 8. Data sets for comparisons

For the chip calls we will use only the default prior. We will have the 3 data sets: call using 18 samples, call using a plate (95 samples), and call using all wild samples (515 samples).

### 8.1 Setting labels for each data set

We need to make sure the sex is correct in all files. We can add letters to separate each data set"

a - chip call with 18 samples
b - chip call with plate (95 samples)
c - chip call with 500+ samples
w - wgs call with 800+ samples
x - wgs call with 30 samples
y - wgs call with 18 samples

Check the log of Plink when we set alleles for the data set with the 18 samples only.

```{bash check_18_819_call2}
head -n 100 output/wgs_vs_chip/chip_dp_01.log
```

Import the new results (95 and 515 samples). I used the default prior for both. We have a different document where we compare the priors and decided if it is worth using it.

```{bash plink2_convert_vcf_to_bed1_default_prior_plate}
# I created a fam file with the information about each sample, but first we import the data and create a bed file setting the family id constant
plink2 \
--allow-extra-chr \
--vcf data/raw_data/albo/wgs_vs_chip/chip_wgs_plate_june_28_dp.vcf \
--const-fid \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/wgs_vs_chip/chip_plate_dp_01 `# dp - default priors` \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants" output/wgs_vs_chip/chip_plate_dp_01.log # to get the number of variants from the log file.
```

Now the chip calls using 500+ samples

Import the fam file we use with Axiom Suite

```{r import_fam_file_Axiom_plate, cache=TRUE}
# the order of the rows in this file does not matter
samples <-
  read.delim(
    file   = here(
      "data",
      "raw_data",
      "albo",
      "wgs_vs_chip",
      "sample_ped_info_2.txt"
    ),
    header = TRUE
  )
head(samples)
```

Import .fam file we created once we created the bed file using Plink2

```{r import_fam_dp_plate}
# The fam file is the same for both data sets with the default or new priors
fam1 <-
  read.delim(
    file   = here(
      "output", "wgs_vs_chip", "chip_plate_dp_01.fam"
    ),
    header = FALSE,
    
  )
head(fam1)
```

We can merge the tibbles

```{r merge_objects1_plate}
# to keep the same order of the .fam file, we will first create an index based on the numbers of the samples, then use it too keep the order

# Extract the number part from the columns
fam1_temp <- fam1 |>
  mutate(num_id = as.numeric(str_extract(V2, "^\\d+")))

samples_temp <- samples |>
  mutate(num_id = as.numeric(str_extract(Sample.Filename, "^\\d+")))

# Perform the left join using the num_id columns and keep the order of fam1
df <- fam1_temp |>
  dplyr::left_join(samples_temp, by = "num_id") |>
  dplyr::select(-num_id) |>
  dplyr::select(8:13)

# check the data frame
head(df)
```

We can check how many samples we have in our file

```{r check_number_samples_df_plate}
nrow(df)
```

Before you save the new fam file, you can change the original file to a different name, to compare
the order later. If you want to repeat the steps above after you save the new file1.fam, you will
need to import the vcf again.

```{r save_new_fam_file_plate}
# Save and override the .fam file for dp
write.table(
  df,
  file      = here(
    "output", "wgs_vs_chip", "chip_plate_dp_01.fam"
  ),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Now we have to subset the data set to keep only the samples form KAT and SAI. We can create a file with the samples we have to keep using the .fam file of our previous call.

Check the .fam file
```{bash}
head output/wgs_vs_chip/chip_dp_01.fam
```

We need to remove the "a"

```{bash get_samples_to_keep}
awk '{gsub("a", "", $2); print $1,$2}' output/wgs_vs_chip/chip_dp_01.fam > output/wgs_vs_chip/chip_samples_subset.txt;
head output/wgs_vs_chip/chip_samples_subset.txt
```

Now subset the samples

```{bash subset_plate_18_samples}
plink2 \
--allow-extra-chr \
--bfile output/wgs_vs_chip/chip_plate_dp_01 \
--make-bed \
--keep output/wgs_vs_chip/chip_samples_subset.txt \
--out output/wgs_vs_chip/chip_plate_dp_02 \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants\|samples" output/wgs_vs_chip/chip_plate_dp_02.log # to get the number of variants from the log file.
```


Check the new .fam file to see if has the order and the sample attributes we want.

Check the fam file of the call with 18 samples
```{bash check_headings_plate, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/wgs_vs_chip/chip_dp_01.fam
```

Check the plate data
```{bash check_headings_3_plate, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/wgs_vs_chip/chip_plate_dp_02.fam
```

We see inconsistency in the sex and that we could add a letter to the fam file of "chip_plate_dp_02.fam". Lets use awk to add the letter "b"

```{bash append_b_plate_data}
# Run this only once
awk '{$2 = $2 "b"; print $0}' output/wgs_vs_chip/chip_plate_dp_02.fam > output/wgs_vs_chip/chip_plate_dp_02_new.fam && mv output/wgs_vs_chip/chip_plate_dp_02_new.fam output/wgs_vs_chip/chip_plate_dp_02.fam;

# Check the output
head output/wgs_vs_chip/chip_plate_dp_02.fam
```

I fixed the sex manually and created new file

```{bash}
# Check the output
head output/wgs_vs_chip/chip_plate_dp_03.fam
```

We can use 'c' for the data set from the call with 500+ samples.

We can also update the .fam file of the wgs data, adding letters to the samples. We will then merge the bed files and use code to create vcf files with pairs of samples setting missingness to zero.

Check the wgs data
```{bash check_headings_3_wgs, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/wgs_vs_chip/new_calls/wgs_chip_30_samples.fam
```

ANGSD create the file with the samples following the order of the samples in our list of crams files

```{bash check_headings_4_wgs}
head -n 5 output/wgs_vs_chip/new_calls/crams_30.txt
```

I created a file with 3 columns: Family id, sex, individual id

```{bash check_headings_4_wgs2}
head -n 5 output/wgs_vs_chip/new_calls/crams_30_names_sex.txt
```

Now we can use the file with the name of the samples to replace columns in the .fam file
```{bash}
# Create new fam
paste output/wgs_vs_chip/new_calls/wgs_chip_30_samples.fam output/wgs_vs_chip/new_calls/crams_30_names_sex.txt| awk '{print $7, $9, $3, $4, $8, $6}' > output/wgs_vs_chip/new_calls/merged_30.fam;
# Check it
head output/wgs_vs_chip/new_calls/merged_30.fam;
# Backup and replace
mv output/wgs_vs_chip/new_calls/wgs_chip_30_samples.fam output/wgs_vs_chip/new_calls/wgs_chip_30_samples_backup.fam;
mv output/wgs_vs_chip/new_calls/merged_30.fam output/wgs_vs_chip/new_calls/wgs_chip_30_samples.fam
```

We have to repeat it for the other wgs data sets

18 samples
```{bash}
# Create new fam
paste output/wgs_vs_chip/new_calls/wgs_chip_18_samples.fam output/wgs_vs_chip/new_calls/crams_18_names_sex.txt| awk '{print $7, $9, $3, $4, $8, $6}' > output/wgs_vs_chip/new_calls/merged_18.fam;
# Check it
head output/wgs_vs_chip/new_calls/merged_18.fam;
# Backup and replace
mv output/wgs_vs_chip/new_calls/wgs_chip_18_samples.fam output/wgs_vs_chip/new_calls/wgs_chip_18_samples_backup.fam;
mv output/wgs_vs_chip/new_calls/merged_18.fam output/wgs_vs_chip/new_calls/wgs_chip_18_samples.fam
```

Check the file extracted from the 819 samples genotype call
```{bash}
head output/wgs_vs_chip/wgs_01.fam
```

I created a new file and added the "w"

```{bash}
head output/wgs_vs_chip/wgs_02.fam
```

Lets make sure the sex is set the same in all files

Check "a" chip call with 18 samples
```{bash}
head output/wgs_vs_chip/chip_dp_01.fam
```

Check "b" chip call with plate
```{bash}
head output/wgs_vs_chip/chip_plate_dp_03.fam
```

Check "c" chip call with 500+ samples

We need to prepare the bed file first.
```{bash plink2_convert_vcf_to_bed1_default_prior__500}
# I created a fam file with the information about each sample, but first we import the data and create a bed file setting the family id constant
plink2 \
--allow-extra-chr \
--vcf data/raw_data/albo/wgs_vs_chip/manuscript_dp_june_28.vcf \
--const-fid \
--make-bed \
--fa data/genome/albo.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/wgs_vs_chip/chip_500_dp_01 `# dp - default priors` \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants" output/wgs_vs_chip/chip_500_dp_01.log # to get the number of variants from the log file.
```

Import the fam file we use with Axiom Suite

```{r import_fam_file_Axiom_plate_500, cache=TRUE}
# the order of the rows in this file does not matter
samples <-
  read.delim(
    file   = here(
      "data",
      "raw_data",
      "albo",
      "wgs_vs_chip",
      "sample_ped_info_ALLPOPS_for_comparisons.txt"
    ),
    header = TRUE
  )
head(samples)
```

Import .fam file we created once we created the bed file using Plink2

```{r import_fam_dp_plate_500}
# The fam file is the same for both data sets with the default or new priors
fam1 <-
  read.delim(
    file   = here(
      "output", "wgs_vs_chip", "chip_500_dp_01.fam"
    ),
    header = FALSE,
    
  )
head(fam1)
```

We can merge the tibbles.

```{r merge_objects1_plate_500}
# to keep the same order of the .fam file, we will first create an index based on the numbers of the samples, then use it too keep the order

# Extract the number part from the columns
fam1_temp <- fam1 |>
  mutate(num_id = as.numeric(str_extract(V2, "^\\d+")))

samples_temp <- samples |>
  mutate(num_id = as.numeric(str_extract(Sample.Filename, "^\\d+")))

# Perform the left join using the num_id columns and keep the order of fam1
df <- fam1_temp |>
  dplyr::left_join(samples_temp, by = "num_id") |>
  dplyr::select(-num_id) |>
  dplyr::select(8:13)

# check the data frame
head(df)
```

We can check how many samples we have in our file

```{r check_number_samples_df_plate2}
nrow(df)
```

Before you save the new fam file, you can change the original file to a different name, to compare
the order later. If you want to repeat the steps above after you saving the new file1.fam, you will
need to import the vcf again.

```{r save_new_fam_file_plate2}
# Save and override the .fam file for dp
write.table(
  df,
  file      = here("output", "wgs_vs_chip", "chip_500_dp_01.fam"),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Now we have to subset the data set to keep only the samples form KAT and SAI. We can create a file with the samples we have to keep using the .fam file of our previous call.

Check the .fam file
```{bash}
head output/wgs_vs_chip/chip_500_dp_01.fam
```

Now we have to select only the 18 samples for our comparisons.

```{bash subset_plate_18_samples_out_500_chip}
plink2 \
--allow-extra-chr \
--bfile output/wgs_vs_chip/chip_500_dp_01 \
--make-bed \
--keep output/wgs_vs_chip/chip_samples_subset.txt \
--out output/wgs_vs_chip/chip_500_dp_02 \
--silent;
# --keep-allele-order \ if you use Plink 1.9
grep "variants\|samples" output/wgs_vs_chip/chip_500_dp_02.log # to get the number of variants from the log file.
```

Check the .fam file
```{bash}
head output/wgs_vs_chip/chip_500_dp_02.fam
```

After fixing the sex and add letter
```{bash}
head output/wgs_vs_chip/chip_500_dp_03.fam
```

Check "w" wgs call with 800+ samples
```{bash}
head output/wgs_vs_chip/wgs_02.fam
```

Check "x" wgs call with 30 samples (I added the x manually after dupplicating the files and adding x)
```{bash}
head output/wgs_vs_chip/new_calls/wgs_chip_30x.fam
```

Check "y" wgs call with 18 samples
```{bash}
head output/wgs_vs_chip/new_calls/wgs_chip_18y.fam
```

### 8.2 Merge data sets

Now we can merge the files into a single bed file. We set all the reference alleles to match the reference genome in every data set. This is crucial for our comparisons. We also need to use --keep-allele-order if we use Plink 1.9

We can create a list of the files to merge
```{bash}
# chip
echo 'output/wgs_vs_chip/chip_dp_01
output/wgs_vs_chip/chip_plate_dp_03
output/wgs_vs_chip/chip_500_dp_03
' > output/wgs_vs_chip/merge_list_2.txt

# wgs
echo 'output/wgs_vs_chip/wgs_02
output/wgs_vs_chip/new_calls/wgs_chip_30x
output/wgs_vs_chip/new_calls/wgs_chip_18y
' > output/wgs_vs_chip/merge_list_3.txt
```

Merge the chip bed files
```{bash merge_all_bed_chip, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_2.txt \
--out output/wgs_vs_chip/chip_3_datasets \
--silent

grep "variants\|samples" output/wgs_vs_chip/chip_3_datasets.log
```

Merge the wgs bed files
```{bash merge_all_bed_wgs, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_3.txt \
--out output/wgs_vs_chip/wgs_3_datasets \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_3_datasets.log
```

### 8.3 WGS calls with different alternative alleles

When we run Plink to merge the files, we get an error about sites having three alleles. It happens because we did genotype calls using only 18, 30 or 819 samples, we end up with different alleles. We used angsd which is a population based algorithm for genotype calls. Plink creates a list of SNPs that have more than 2 alleles. We can check it later. Lets count how many SNPs:

```{bash}
wc -l output/wgs_vs_chip/wgs_3_datasets.missnp
```

Let's see how many SNPs have this problem once we decrease the sample size
```{bash}
# wgs 2 - 819 samples vs 30 samples
echo 'output/wgs_vs_chip/wgs_02
output/wgs_vs_chip/new_calls/wgs_chip_30x
' > output/wgs_vs_chip/merge_list_4.txt;

# wgs 3 -  891 samples vs 18 samples
echo 'output/wgs_vs_chip/wgs_02
output/wgs_vs_chip/new_calls/wgs_chip_18y
' > output/wgs_vs_chip/merge_list_5.txt;

# wgs 4 - 30 samples vc 18 samples
echo 'output/wgs_vs_chip/new_calls/wgs_chip_30x
output/wgs_vs_chip/new_calls/wgs_chip_18y
' > output/wgs_vs_chip/merge_list_6.txt;
```

Now we can try to merge them to see how many SNPs have different alleles

30 versus 819 samples
```{bash merge_all_bed_wgs2, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_4.txt \
--out output/wgs_vs_chip/wgs_800_vs_30_samples \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_800_vs_30_samples.log
```

We have 2,245 SNPs with 3+ alleles. It happens because the alternative alleles are different in each data set
```{bash}
wc -l output/wgs_vs_chip/wgs_800_vs_30_samples.missnp
```

18 versus 819 samples
```{bash merge_all_bed_wgs3, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_5.txt \
--out output/wgs_vs_chip/wgs_800_vs_18_samples \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_800_vs_18_samples.log
```

We have 2,257 SNPs with 3+ alleles
```{bash}
wc -l output/wgs_vs_chip/wgs_800_vs_18_samples.missnp
```

18 versus 30 samples
```{bash merge_all_bed_wgs4, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_6.txt \
--out output/wgs_vs_chip/wgs_18_vs_30_samples \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_18_vs_30_samples.log
```

We have 882 SNPs with 3+ alleles
```{bash}
wc -l output/wgs_vs_chip/wgs_18_vs_30_samples.missnp
```

We can get the list of SNPs
```{bash}
cat output/wgs_vs_chip/wgs_800_vs_30_samples.missnp output/wgs_vs_chip/wgs_800_vs_18_samples.missnp output/wgs_vs_chip/wgs_18_vs_30_samples.missnp | awk '!seen[$0]++' > output/wgs_vs_chip/SNPs_wgs_3_alleles.txt;
wc -l output/wgs_vs_chip/SNPs_wgs_3_alleles.txt
```

We need to remove the 2,755 SNPs.

We can remove these SNPs and only compare the other ones. Lets double check to make sure we have only bi-allelic data as well. Perhaps that is why we see inconsistencies between the genotype calls from the chip and wgs

Exclude from 18 samples

```{bash merge_all_bed5, eval=FALSE}
plink \
--bfile output/wgs_vs_chip/new_calls/wgs_chip_18y \
--allow-extra-chr \
--keep-allele-order \
--biallelic-only \
--exclude output/wgs_vs_chip/SNPs_wgs_3_alleles.txt \
--out output/wgs_vs_chip/new_calls/wgs_chip_18y_b \
--make-bed \
--silent

grep "variants\|samples" output/wgs_vs_chip/new_calls/wgs_chip_18y_b.log
```

Exclude from 30 samples

```{bash merge_all_bed6, eval=FALSE}
plink \
--bfile output/wgs_vs_chip/new_calls/wgs_chip_30x \
--allow-extra-chr \
--keep-allele-order \
--biallelic-only \
--exclude output/wgs_vs_chip/SNPs_wgs_3_alleles.txt \
--out output/wgs_vs_chip/new_calls/wgs_chip_30x_b \
--make-bed \
--silent

grep "variants\|samples" output/wgs_vs_chip/new_calls/wgs_chip_30x_b.log
```

Exclude from data subset with 819 samples

```{bash merge_all_bed61, eval=FALSE}
plink \
--bfile output/wgs_vs_chip/wgs_02 \
--allow-extra-chr \
--keep-allele-order \
--biallelic-only \
--exclude output/wgs_vs_chip/SNPs_wgs_3_alleles.txt \
--out output/wgs_vs_chip/wgs_03 \
--make-bed \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_03.log
```

Exclude from data subset from 30 samples

```{bash merge_all_bed7, eval=FALSE}
plink \
--bfile output/wgs_vs_chip/new_calls/wgs_chip_30x \
--allow-extra-chr \
--keep-allele-order \
--biallelic-only \
--exclude output/wgs_vs_chip/SNPs_wgs_3_alleles_b.txt \
--out output/wgs_vs_chip/new_calls/wgs_chip_30x_c \
--make-bed \
--silent

grep "variants\|samples" output/wgs_vs_chip/new_calls/wgs_chip_30x_c.log
```

Exclude from data subset from 18 samples

```{bash merge_all_bed8, eval=FALSE}
plink \
--bfile output/wgs_vs_chip/new_calls/wgs_chip_18y \
--allow-extra-chr \
--keep-allele-order \
--biallelic-only \
--exclude output/wgs_vs_chip/SNPs_wgs_3_alleles_b.txt \
--out output/wgs_vs_chip/new_calls/wgs_chip_18y_c \
--make-bed \
--silent

grep "variants\|samples" output/wgs_vs_chip/new_calls/wgs_chip_18y_c.log
```

Now create new list to merge. We can merge all files (chip and wgs) into one single file, but first lets create one file with the wgs samples only

```{bash}
# wgs 
echo 'output/wgs_vs_chip/wgs_03
output/wgs_vs_chip/new_calls/wgs_chip_30x_b
output/wgs_vs_chip/new_calls/wgs_chip_18y_b
' > output/wgs_vs_chip/merge_list_7.txt

# all
echo 'output/wgs_vs_chip/wgs_03
output/wgs_vs_chip/new_calls/wgs_chip_30x_b
output/wgs_vs_chip/new_calls/wgs_chip_18y_b
output/wgs_vs_chip/chip_dp_01
output/wgs_vs_chip/chip_plate_dp_03
output/wgs_vs_chip/chip_500_dp_03
' > output/wgs_vs_chip/merge_list_8.txt
```

WGS

```{bash merge_all_bed_wgs41, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_7.txt \
--out output/wgs_vs_chip/wgs_3_datasets_b \
--silent;

grep "variants\|samples" output/wgs_vs_chip/wgs_3_datasets_b.log
```


Merge all data sets

```{bash merge_all_bed_wgs42, eval=FALSE}
plink \
--allow-extra-chr \
--keep-allele-order \
--merge-list output/wgs_vs_chip/merge_list_8.txt \
--out output/wgs_vs_chip/wgs_chip_merged \
--silent

grep "variants\|samples" output/wgs_vs_chip/wgs_chip_merged.log
```

Now, we have to set the reference allele to match the reference genome: we remove SNPs with more than 1 alternative allele due to genotype calls with low sample size, and create a single file.

```{bash}
head -n 25 output/wgs_vs_chip/wgs_chip_merged.fam
```

## 9. Create vcf files for all comparison

The 18 samples were extracted when more samples were used for the genotype call

**a**: chip - call with 18 samples
**b**: chip - call with 95 samples (full plate)
**c**: chip - call with 500+ samples
**w**: wgs - call with 800+ samples
**x**: wgs - call with 30 samples (all wgs samples for both populations)
**y**: wgs - call with 18 samples (only samples with wgs and chip)

We do not need to do all pairwise comparisons.

**Chip**: ab, ac, bc
**WGS**: wx, wy, xy
**WGS versus chip**: aw, ax, ay, bw, bx, by, cw, cx, cy

All comparisons (* those I will focus on)

**Chip**
ab - chip_18 vs chip_95
ac - chip_18 vs chip_500 *
bc - chip_95 vs chip_500

**WGS**
wx - wgs_800 vs wgs_30
wy - wgs_800 vs wgs_18 *
xy - wgs_18 vs wgs_30

**Chip vs WGS**
aw - chip_18 vs wgs_800 *
ax - chip_18 vs wgs_30
ay - chip_18 vs wgs_18 *
bw - chip_95 vs wgs_800
bx - chip_95 vs wgs_30
by - chip_95 vs wgs_18
cw - chip_500 vs wgs_800 *
cx - chip_500 vs wgs_30
cy - chip_500 vs wgs_18 *


### 9.1 Create all the vcfs

```{bash create_vcfs_2, eval=FALSE}
input_file="output/wgs_vs_chip/wgs_chip_merged.fam"
output_dir="output/wgs_vs_chip/vcfs2"
bfile="output/wgs_vs_chip/wgs_chip_merged"

# create the output directory if it does not exist
mkdir -p $output_dir

# get unique families
families=$(awk '{print $1}' $input_file | sort | uniq)

for famid in $families; do
  # get the base sample ids (without a, b, w)
  base_iids=$(grep "$famid" $input_file | awk '{print $2}' | sed 's/[abcwxy]$//' | uniq)
  
  for base_iid in $base_iids; do
    for combination in "ab" "ac" "bc" "wx" "wy" "xy" "aw" "ax" "ay" "bw" "bx" "by" "cw" "cx" "cy"; do
      # Check if both samples exist
      if grep -qE "${famid}\s${base_iid}[${combination:0:1}]\s" "$input_file" && 
         grep -qE "${famid}\s${base_iid}[${combination:1:1}]\s" "$input_file"; then
        # Create temporary file
        tmp_file=$(mktemp)
        grep -E "${famid}\s${base_iid}[${combination:0:1}]\s" "$input_file" > "$tmp_file"
        grep -E "${famid}\s${base_iid}[${combination:1:1}]\s" "$input_file" >> "$tmp_file"
  
        # Execute plink2
        plink2 \
        --allow-extra-chr \
        --keep-allele-order \
        --bfile $bfile \
        --keep "$tmp_file" \
        --recode vcf-iid \
        --geno 0 \
        --out "$output_dir/${famid}_${base_iid}${combination}" \
        --silent
  
        # Remove temporary file
        rm "$tmp_file"
      fi
    done
  done
done
```

Check how many SNPs per vcf

```{bash count_snps_per_vcf_2}
# Define directory with the vcfs
output_dir="output/wgs_vs_chip/vcfs2"
# Count how many SNPs we have in each vcf file
for file in ${output_dir}/*.vcf; do
    echo $(basename $file): $(grep -v '^#' $file | wc -l)
done
```
Since we set genotyping missingness to zero within each pair of samples, we see different number of SNPs in each vcf.

Check sample names to see if our code created the vcfs with two samples

```{bash check_sample_name_vcfs_2}
# Define directory with the VCFs
output_dir="output/wgs_vs_chip/vcfs2"

# Iterate over each VCF file
for file in "${output_dir}"/*.vcf; do
    # Extract the file name without the directory path
    file_name=$(basename "${file}")

    # Use bcftools query to retrieve the sample names
    sample_names=$(bcftools query -l "${file}")
    
    # Print the file name and the sample names
    echo "${file_name}: ${sample_names}"
done
```

### 9.2 Pairwise comparisons summary

Compare the two samples in each vcf file and create csv output across all samples
```{python compare_alleles2, eval=FALSE}
import allel
import pandas as pd
import os
import numpy as np

# Initialize the output dataframe
output_df = pd.DataFrame()

# Directory with vcf files
dir_name = "output/wgs_vs_chip/vcfs2/"

# Get list of all vcf files in the directory
vcf_files = [f for f in os.listdir(dir_name) if f.endswith('.vcf')]

# Iterate over VCF files
for vcf_file in vcf_files:
    file_path = os.path.join(dir_name, vcf_file)
    callset = allel.read_vcf(file_path, fields=['*'])

    # Get genotype
    gt = allel.GenotypeArray(callset['calldata/GT'])
    
    # Verify the vcf contains two samples
    assert gt.shape[1] == 2, f"Expected 2 samples in {vcf_file}, found {gt.shape[1]}"

    # Count SNPs
    n_snps = len(gt)

    # Count homozygous and heterozygous SNPs for each sample
    n_homo_ref = np.count_nonzero(gt.is_hom_ref(), axis=0)
    n_homo_alt = np.count_nonzero(gt.is_hom_alt(), axis=0)
    n_hetero = np.count_nonzero(gt.is_het(), axis=0)
    
    # Count homozygous and heterozygous SNPs mismatches
    n_homo_ref_mismatch = np.sum(gt.is_hom_ref()[:, 0] != gt.is_hom_ref()[:, 1])
    n_homo_alt_mismatch = np.sum(gt.is_hom_alt()[:, 0] != gt.is_hom_alt()[:, 1])
    n_hetero_mismatch = np.sum(gt.is_het()[:, 0] != gt.is_het()[:, 1])

    # Get alleles
    ref_alleles = callset['variants/REF']
    alt_alleles = callset['variants/ALT'][:, 0]  # assuming bi-allelic

    # Count mismatching reference and alternative alleles
    n_snps_ref_mismatch = np.count_nonzero(ref_alleles[gt[:,0]] != ref_alleles[gt[:,1]])
    n_snps_alt_mismatch = np.count_nonzero(alt_alleles[gt[:,0]] != alt_alleles[gt[:,1]])

    # Count alleles for each sample
    n_a = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'A' or alt_alleles[i] == 'A')
    n_t = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'T' or alt_alleles[i] == 'T')
    n_c = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'C' or alt_alleles[i] == 'C')
    n_g = sum(np.count_nonzero(gt == i, axis=0) for i in range(4) if ref_alleles[i] == 'G' or alt_alleles[i] == 'G')

    # Append results to the output dataframe
    result = pd.DataFrame({
        'vcf_file': [file_path],
        'n_SNPs': [n_snps],
        'n_SNPs_ref_mismatch': [n_snps_ref_mismatch],
        'n_SNPs_alt_mismatch': [n_snps_alt_mismatch],
        'n_A': [n_a],
        'n_T': [n_t],
        'n_C': [n_c],
        'n_G': [n_g],
        'n_homo_ref': [n_homo_ref],
        'n_homo_alt': [n_homo_alt],
        'n_hetero': [n_hetero],
        'n_homo_ref_mismatch': [n_homo_ref_mismatch],
        'n_homo_alt_mismatch': [n_homo_alt_mismatch],
        'n_hetero_mismatch': [n_hetero_mismatch]
    })

    output_df = pd.concat([output_df, result])

# Write the result to a csv file
output_df.to_csv('output/wgs_vs_chip/vcfs2/allele_comparison_stats.csv', index=False)
```

Clean env
```{r clean_python_env4}
# python
py_run_string("import gc; gc.collect()")
```

Import the data
```{r warning=FALSE}
data <-
  read_delim(
    "output/wgs_vs_chip/vcfs2/allele_comparison_stats.csv",
    delim = ",",
    show_col_types = FALSE
  ) 

data <-
  data |>
  mutate(vcf_file = str_remove(vcf_file, "output/wgs_vs_chip/vcfs2/")) |>
  separate(
    vcf_file,
    into = c("Population", "Sample_Comparison"),
    sep = "_",
    extra = "drop"
  ) |>
  separate(
    Sample_Comparison,
    into = c("Sample", "Comparison"),
    sep = "(?<=\\d)(?=[a-z])",
    convert = TRUE
  ) |>
  mutate(Comparison = str_remove(Comparison, ".vcf")) |>
  arrange(Comparison)

# Split the "Comparison" column into "Sample1" and "Sample2"
data <- 
  data |>
  separate(
    Comparison,
    into = c("Sample1", "Sample2"),
    sep = 1,
    # because each comparison has two characters
    remove = FALSE
  ) |> # keep the original comparison column
  relocate(Sample1, Sample2, .after = Comparison) # move the new columns right after Comparison

cols_to_split <-
  c("n_A",
    "n_T",
    "n_C",
    "n_G",
    "n_homo_ref",
    "n_homo_alt",
    "n_hetero")

# Remove unwanted characters from the columns
for (col_name in cols_to_split) {
  data[[col_name]] <- gsub("\\[\\[|]\\n", "", data[[col_name]])
}

# Split the columns
for (col_name in cols_to_split) {
  # Create new column names based on 'Sample1' and 'Sample2'
  new_col_names <- paste0(col_name, "_sample", 1:2)
  
  data <- data |>
    separate(
      col = col_name,
      into = new_col_names,
      sep = " ",
      extra = "drop"
    )
}

# Clean the new columns
cols_to_clean <- 
  grep("^n_", names(data), value = TRUE)

for (col_name in cols_to_clean) {
  # Remove unwanted characters '[', ']', and '\n'
  data[[col_name]] <- gsub("\\[|]|\\n", "", data[[col_name]])
}

# Specify the column names to convert to numeric
columns_to_convert <-
  c(
    # "Population",
    "Sample",
    # "Comparison",
    # "Sample1",
    # "Sample2",
    "n_SNPs",
    "n_SNPs_ref_mismatch",
    "n_SNPs_alt_mismatch",
    "n_A_sample1",
    "n_A_sample2",
    "n_T_sample1",
    "n_T_sample2",
    "n_C_sample1",
    "n_C_sample2",
    "n_G_sample1",
    "n_G_sample2",
    "n_homo_ref_sample1",
    "n_homo_ref_sample2",
    "n_homo_alt_sample1",
    "n_homo_alt_sample2",
    "n_hetero_sample1",
    "n_hetero_sample2",
    "n_homo_ref_mismatch",
    "n_homo_alt_mismatch",
    "n_hetero_mismatch"
  )

# Convert columns to numeric
data[columns_to_convert] <-
  lapply(data[columns_to_convert], function(x)
    as.numeric(as.character(x)))

# Verify the column types
print(sapply(data[columns_to_convert], class))
```

We can look over all the comparisons to see if we can see any pattern

```{r plot_all_pairwise_comparisons, fig.width=9, fig.height=8}
# Calculate the percentages for each category
data <-
  data |>
  mutate(
    Perc_n_homo_ref_mismatch = round((n_homo_ref_mismatch / n_SNPs) * 100, 2),
    Perc_n_homo_alt_mismatch = round((n_homo_alt_mismatch / n_SNPs) * 100, 2),
    Perc_n_hetero_mismatch = round((n_hetero_mismatch / n_SNPs) * 100, 2)
  )

# Continue with the reshaping
data_long <- data |>
  pivot_longer(cols = starts_with("n_"),
               names_to = "Category",
               values_to = "Value") |>
  pivot_longer(cols = starts_with("Perc_"),
               names_to = "Category_Perc",
               values_to = "Percentage") |>
  mutate(Category_Perc = str_remove(Category_Perc, "Perc_")) |>
  filter(Category == Category_Perc |
           Category == "n_SNPs")  # Remove Total_mismatch

# Define a color palette
color_palette <- c("#FF8C94", "#FFE180", "#9CE09C", "#A391FF")

# Rename categories
data_long <- data_long |>
  mutate(
    Category = recode(
      Category,
      "n_SNPs" = "SNPs",
      "n_homo_ref_mismatch" = "Homozygous REF",
      "n_homo_alt_mismatch" = "Homozygous ALT",
      "n_hetero_mismatch" = "Heterozygous"
    )
  )

# Change the order of the Comparison variable (Chip, WGS, and Chip vs WGS)
data_long$Comparison <-
  factor(
    data_long$Comparison,
    levels = c(
      "ab",
      "ac",
      "bc",
      "wx",
      "wy",
      "xy",
      "aw",
      "ax",
      "ay",
      "bw",
      "bx",
      "by",
      "cw",
      "cx",
      "cy"
    )
  )

# Recode the levels of the "Comparison" variable
data_long$Comparison <- recode(
  data_long$Comparison,
  "ab" = "chip_18 : chip_95",
  "ac" = "chip_18 : chip_500",
  "bc" = "chip_95 : chip_500",
  "wx" = "wgs_800 : wgs_30",
  "wy" = "wgs_800 : wgs_18",
  "xy" = "wgs_18 : wgs_30",
  "aw" = "chip_18 : wgs_800",
  "ax" = "chip_18 : wgs_30",
  "ay" = "chip_18 : wgs_18",
  "bw" = "chip_95 : wgs_800",
  "bx" = "chip_95 : wgs_30",
  "by" = "chip_95 : wgs_18",
  "cw" = "chip_500 : wgs_800",
  "cx" = "chip_500 : wgs_30",
  "cy" = "chip_500 : wgs_18"
)

# Create the plot
ggplot(data_long, aes(x = Category, y = Value, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(Comparison ~ Population, scales = "free_y", space = "free") +
  coord_flip() +
  labs(
    title = "Mismatches of zygosity in pairwise comparisons",
    x = "Category",
    y = "Count",
    caption = "The comparison are between SNPs genotyped in both samples.\n Each sample as genotyped with a different number of samples. Each pair of sample was subseted to\n a vcf file allowing no genotyping missingness. Next, with custom python script,\n the total number of genotypes matches and mismatches was stored in a csv file. \nThe data was tidy and visualized in R.\nREF = Reference allele; ALT = Alternative allele\n At right are the number of samples used in the genotype calls\n for each data set comparison."
  ) +
  theme(panel.spacing = unit(1.5, "lines")) +
  geom_text(
    aes(label = ifelse(
      Category == "SNPs",
      scales::comma(Value),
      paste0(scales::comma(Value), " (", sprintf("%.2f", Percentage), "%)")
    )),
    position = position_dodge(width = 0.7),
    hjust = 0.9,
    vjust = 0.5,
    size = 2.5,
    check_overlap = TRUE,
    color = "black"
  ) +
  scale_fill_manual(values = color_palette) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill = "none") +
  my_theme() +
  scale_y_continuous(
    labels = scales::comma,
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000),
    limits = c(1, NA),
    expand = expansion(mult = c(0, 0.1))
  ) +
  theme(
    plot.caption = element_text(
      face = "italic",
      size = 8,
      color = "grey20"
    ),
    plot.margin = unit(c(1, 2, 1, 1), "cm"),
    axis.text.x = element_text(angle = 0, hjust = 1),
    axis.text = element_text(size = 7),
    strip.text.y = element_text(angle = 360)
  )

# save the plot
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "01.Pairwise_comparions_wgs_chip.pdf"
  ),
  width  = 8,
  height = 14,
  units  = "in"
)
```

We see the lowest mismatch rate for the comparisons within each technology was used independently to how many samples the sample was genotyped with. The chip seems slightly better for smaller sample sizes. The sample size seems not to affect the overall result of the technologies comparisons. The mismatch rate in SAI (island invasive range) is higher than KAT (continent native range), indicating the presence of low frequency alleles that might might be difficult to detect.

Next, we can look at the performance of the technologies across all 18 samples. Our next questions are a bit different. For example, how many SNPs have mismatches in 1, 2 or more samples? Are there any SNPs that have errors in more than 2 samples? Can we find a way to identify them and remove them?


Since the sample size with which the genotype call was performed does not affect the overall results, we can select a few comparisons to look into in detail. We then can compare the read count for each allele from the WGS with the mismatch rate. Do the SNPs with mismatches between the technologies have a lower read count? If so, what is the error rate if we remove SNPs that had 1 or a few reads? Does the output of the comparison improve the concordance between the technologies? We can do that selecting 1 or two data sets. For example, we can select chip_18: wgs_18 (ay) and chip_500: wgs_800 (cw). Then we will compare the genotypes of samples that were genotyped using only 18 samples or the entire data set we had (around 500 samples for the chip and 800 for the wgs data set). We extracted the 18 samples from our large data set for comparisons.

We can also compare if the SNPs with mismatches are the same when the sample size for the genotype call is large or small. What percentage of SNPs have mismatches when varying the sample size? We can do the same comparison for each technology.

Save the data first
```{r}
# Save the data
saveRDS(
  data_long,
  file = here(
    "output",
    "wgs_vs_chip",
    "pairwise_comparison_long.rds"
  )
)

# Clean environment and memory
rm(list = ls())
gc()

# # Load the data
# data_long <-
#   readRDS(
#     file = here(
#       "output",
#       "wgs_vs_chip",
#       "pairwise_comparison_long.rds"
#     )
#   )
```

### 9.3 Script to process vcf files and function to import csv files

Python script to get the match and mismatches from a vcf file
```{python create_csv_from_vcfs.py, eval=FALSE}
import argparse
import allel
import pandas as pd
import os
import numpy as np
import warnings

# Ignore DtypeWarnings from pandas
warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)

# Function to convert genotype indices to alleles
def genotype_to_alleles(gt_indices, ref_allele, alt_alleles):
    alleles = np.concatenate(([ref_allele], alt_alleles))
    return " ".join(alleles[idx] for idx in gt_indices if idx!=-1)  # idx -1 means missing data

def process_vcf_files(vcf_file_ending):
    dir_name = "output/wgs_vs_chip/vcfs2/"
    vcf_files = [f for f in os.listdir(dir_name) if f.endswith(f'{vcf_file_ending}.vcf')]
    
    if not vcf_files:
        raise ValueError(f"No VCF files found matching '{vcf_file_ending}'")

    for vcf_file in vcf_files:
        file_path = os.path.join(dir_name, vcf_file)
        callset = allel.read_vcf(file_path, fields=['*'])

        # Get genotype
        gt = allel.GenotypeArray(callset['calldata/GT'])

        # Get sample names and add prefix from file name
        sample_1, sample_2 = callset['samples']
        prefix = vcf_file.split("_")[0] + "_"  # Added "_" after prefix
        sample_1 = prefix + sample_1
        sample_2 = prefix + sample_2

        # Verify the vcf contains two samples
        assert gt.shape[1] == 2, f"Expected 2 samples in {vcf_file}, found {gt.shape[1]}"

        # Create DataFrame
        df = pd.DataFrame({
            'SNP_id': callset['variants/ID'],
            f'{sample_1}_geno': [genotype_to_alleles(gt, callset['variants/REF'][i], callset['variants/ALT'][i]) for i, gt in enumerate(gt[:, 0])],
            f'{sample_2}_geno': [genotype_to_alleles(gt, callset['variants/REF'][i], callset['variants/ALT'][i]) for i, gt in enumerate(gt[:, 1])],
            f'{sample_1}_{sample_2}_gcomp': np.where(gt[:, 0] == gt[:, 1], 'match', 'mismatch').tolist(),
            f'{sample_1}_zygo': np.where(gt.is_hom_ref()[:, 0], 'hom_ref', np.where(gt.is_hom_alt()[:, 0], 'hom_alt', 'het')).tolist(),
            f'{sample_2}_zygo': np.where(gt.is_hom_ref()[:, 1], 'hom_ref', np.where(gt.is_hom_alt()[:, 1], 'hom_alt', 'het')).tolist(),
            f'{sample_1}_{sample_2}_zcomp': np.where(gt.is_hom()[:, 0] == gt.is_hom()[:, 1], 'match', 'mismatch').tolist()
        })

        # When you write your output file, use the input filename to create the corresponding output filename
        output_file = f'output/wgs_vs_chip/{os.path.basename(vcf_file).replace(".vcf", "")}_comparison.csv'
        df.to_csv(output_file, index=False)

def combine_csv_files(vcf_file_ending):
    # Combine only the newly created CSVs into one
    dir_path = "output/wgs_vs_chip/"
    csv_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith(f'{vcf_file_ending}_comparison.csv')]

    # Ensure that we have at least one such file
    if not csv_files:
        raise ValueError(f"No CSV files found matching '{vcf_file_ending}_comparison.csv'")

    # Load the first CSV file
    combined_csv = pd.read_csv(csv_files[0])

    # Merge the rest of the CSV files one by one
    for f in csv_files[1:]:
        df = pd.read_csv(f)
        combined_csv = pd.merge(combined_csv, df, on='SNP_id', how='outer')

    combined_csv.to_csv(os.path.join(dir_path, f'combined_comparison_{vcf_file_ending}.csv'), index=False)

def main():
    # Initialize parser
    parser = argparse.ArgumentParser(description="Process VCF files and output CSV comparison files")

    # Add argument
    parser.add_argument('vcf_file_ending', type=str, help="The ending for VCF files to be processed (e.g., 'ay')")

    # Parse arguments
    args = parser.parse_args()

    # Remove '.vcf' from the ending, if present
    vcf_file_ending = args.vcf_file_ending.replace('.vcf', '')

    # Process VCF files and combine CSV files
    process_vcf_files(vcf_file_ending)
    combine_csv_files(vcf_file_ending)

if __name__ == "__main__":
    main()
```

How to run the Python script
```{bash, how_to_run_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py ay
```

We can write a function to import and process the csv files our python script generates
```{r function_to_import_csv_process_csv_files}
process_csv_files <- function(csv_file_ending) {
  # Read the CSV file using fread() function
  csv_file <- paste0("output/wgs_vs_chip/combined_comparison_", csv_file_ending, ".csv")
  data_dt <- data.table::fread(csv_file)
  
  # Get all column names that end with '_gcomp'
  gcomp_cols <- grep("_gcomp$", names(data_dt), value = TRUE)
  
  # Convert data.frame to data.table
  setDT(data_dt)
  
  # Iterate over the '_gcomp' columns and create new '_REF' and '_ALT' columns
  for (col in gcomp_cols) {
    # Split each '_gcomp' column into '_REF' and '_ALT'
    ref_col <- paste0(col, "_REF")
    alt_col <- paste0(col, "_ALT")
    data_dt[, c(ref_col, alt_col) := tstrsplit(get(col), ", ", fixed = TRUE)]
    
    # Remove unwanted characters from each new column
    data_dt[, (ref_col) := gsub("\\[|\\]|'", "", get(ref_col))]
    data_dt[, (alt_col) := gsub("\\[|\\]|'", "", get(alt_col))]
  }
  
  # Rename columns to remove '_gcomp'
  new_names <- names(data_dt)
  new_names <- gsub("_gcomp_ALT$", "_ALT", new_names)
  new_names <- gsub("_gcomp_REF$", "_REF", new_names)
  setnames(data_dt, new_names)
  setnames(data_dt, new_names)
  
  # Return the processed data.table
  return(data_dt)
}

# we can save the function to source it later
dump(
  "process_csv_files",
  here(
    "scripts", "analysis", "process_csv_files.R")
)
```

How to run the function to import the csv files
```{r how_to_run_process_csv_files, eval=FALSE}
data_ay_dt <- process_csv_files("ay")

# Check and display only columns that match the criteria
head(data_ay_dt[, c("SNP_id", names(data_ay_dt)[grepl("_REF$|_ALT$", names(data_ay_dt))]), with = FALSE])
```


Function to get summary of the mismatches
```{r function_get_summary}
process_data_object <- function(object_name) {
  # Get the data.table object based on the input name
  data_dt <- get(object_name)
  
  # Create columns for match and mismatch count for columns ending with _REF
  cols_REF <- grep("_REF$", names(data_dt), value = TRUE)
  data_dt[, c("REF_match_count", "REF_mismatch_count") := .(
    rowSums(.SD == "match", na.rm = TRUE),
    rowSums(.SD == "mismatch", na.rm = TRUE)
  ), .SDcols = cols_REF]
  
  # Create columns for match and mismatch count for columns ending with _ALT
  cols_ALT <- grep("_ALT$", names(data_dt), value = TRUE)
  data_dt[, c("ALT_match_count", "ALT_mismatch_count") := .(
    rowSums(.SD == "match", na.rm = TRUE),
    rowSums(.SD == "mismatch", na.rm = TRUE)
  ), .SDcols = cols_ALT]
  
  # Create columns for match and mismatch count for columns ending with _zcomp
  cols_Zigo <- grep("_zcomp$", names(data_dt), value = TRUE)
  data_dt[, c("Zigo_match_count", "Zigo_mismatch_count") := .(
    rowSums(.SD == "match", na.rm = TRUE),
    rowSums(.SD == "mismatch", na.rm = TRUE)
  ), .SDcols = cols_Zigo]
  
  # Summarize the data for each SNP_id
  summary_dt <- data_dt[, .(
    REF_match = sum(REF_match_count, na.rm = TRUE),
    REF_mismatch = sum(REF_mismatch_count, na.rm = TRUE),
    ALT_match = sum(ALT_match_count, na.rm = TRUE),
    ALT_mismatch = sum(ALT_mismatch_count, na.rm = TRUE),
    Zigo_match = sum(Zigo_match_count, na.rm = TRUE),
    Zigo_mismatch = sum(Zigo_mismatch_count, na.rm = TRUE)
  ), by = SNP_id]
  
  # Sort the summarized data by SNP_id
  setorder(summary_dt, SNP_id)
  
  # Return the processed summary data.table
  return(summary_dt)
}

# we can save the function to source it later
dump(
  "process_data_object",
  here(
    "scripts", "analysis", "process_data_object.R")
)
```

How to run the function
```{r, eval=FALSE}
summary_ab <- process_data_object("data_ab_dt")
```

Function to process the summaries for plotting
```{r process_summary_object_function}
process_summary_object <- function(summary_object_name) {
  # Select only the relevant columns
  dt <- get(summary_object_name)[, .(SNP_id, REF_mismatch, ALT_mismatch, Zigo_mismatch)]
  
  # Reshape data to long format
  dt_long <- reshape2::melt(dt, id.vars = "SNP_id", variable.name = "type", value.name = "count")
  
  # Convert to data.table if it's not already
  setDT(dt_long)
  
  # Convert count to numeric if it's not already
  dt_long[, count := as.numeric(count)]
  
  # Count occurrences per count value
  dt_long <- dt_long[, .(n = .N), by = .(type, count)]
  
  # Calculate total count of unique SNPs
  total_SNP <- length(unique(dt$SNP_id))
  
  # Add a new column for the percentage
  dt_long[, perc := n / total_SNP * 100]
  
  # Define new labels
  new_labels <- c(
    "Reference Allele" = "REF_mismatch",
    "Alternative Allele" = "ALT_mismatch",
    "Zygosity Mismatch" = "Zigo_mismatch"
  )
  
  # Apply new labels
  dt_long$type <- forcats::fct_recode(dt_long$type, !!!new_labels)
  
  # Return the processed data.table
  return(dt_long)
}
# we can save the function to source it later
dump(
  "process_summary_object",
  here(
    "scripts", "analysis", "process_summary_object.R")
)
```

How to run process_summary_objects function
```{r how_to_run_process_summary_object_function, eval=FALSE}
dt_long_ab <- process_summary_object("summary_ab")
```


Theme for plotting
```{r}
# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R" # choose my_theme.R (Roboto Condensed) or my_theme2.R (default font)
  )
)
```

Function to errors per SNP per sample

```{r plot_dt_long_function}
plot_dt_long <- function(object_suffix) {
  # Get the object name based on the suffix
  object_name <- paste0("dt_long_", object_suffix)
  
  # Get the corresponding data.table object
  dt_long <- get(object_name)
  
  # Create facet histogram
  p <- ggplot(dt_long, aes(x = count, y = n)) +
    geom_bar(
      stat = "identity",
      fill = "#ffcae4",
      color = ifelse(
        dt_long$count == 0,
        "#CCFF00",
        ifelse(dt_long$count == 1, "#4169E1", "#FF7F50")
      ),
      width = 0.6,
      linewidth = 1
    ) +
    geom_text(
      aes(label = paste0(
        scales::comma(n), " (", round(perc, 2), "%)"
      )),
      hjust = ifelse(dt_long$count == 0, .7, 0.01),
      size = 2.3,
      color = "gray10"
    ) +
    facet_wrap(~ type, scales = "free_y") +
    labs(
      title = paste("Histogram of SNP Mismatch Counts", object_suffix),
      x = "Sample Count",
      y = "SNP Count",
      caption = paste(object_suffix, "\n Bar border colors: Electric Lime = no errors; Royal Blue =  1 error; Coral = more than 1 error")
    ) +
    scale_y_continuous(
      breaks = c(0, 25000, 50000, 75000, 100000, 125000, 150000, 175000),
      labels = function(x) paste0(x / 1000, "k"),
      expand = expansion(mult = c(0, 0.2))
    ) +
    scale_x_continuous(breaks = 0:18, expand = expansion(add = c(0.5, 0))) +
    my_theme() +
    coord_flip() +
    theme(
      plot.caption = element_text(
        face = "italic",
        size = 10,
        color = "grey20"
      ),
      panel.spacing = unit(2, "lines"),
      plot.margin = unit(c(1, 3, 1, 1), "cm"),
      axis.text.x = element_text(size = 7, angle = 0) 
    )
  
  # Save the plot
  output_file <- here("output", "wgs_vs_chip", "figures", paste0(object_suffix, "_mismatches.pdf"))
  ggsave(output_file, p, width = 8, height = 6, units = "in")
  
  # Return the plot object
  return(p)
}
```

How to run the plotting function
```{r how_to_run_plot_dt_long_function, eval=FALSE}
plot_dt_long("ab")
```

Function to get summary for each population
```{r}
generate_summary <- function(data_dt, population, object_suffix) {
  # Extract population columns
  pop_cols <- grep(paste0("^", population, "_"), names(data_dt), value = TRUE)
  
  # Subset the data into population-specific data table
  data_pop <- data_dt[, c('SNP_id', pop_cols), with = FALSE]
  
  # Create columns for match and mismatch count for columns ending with _REF
  cols_REF <- grep("_REF$", names(data_pop), value = TRUE)
  
  # Calculate the count of "match" or "mismatch" for each row
  data_pop[, c("REF_match_count", "REF_mismatch_count") :=
               .(rowSums(.SD == "match", na.rm = TRUE),
                 rowSums(.SD == "mismatch", na.rm = TRUE)),
           .SDcols = cols_REF]
  
  # Create columns for match and mismatch count for columns ending with _ALT
  cols_ALT <- grep("_ALT$", names(data_pop), value = TRUE)
  
  # Calculate the count of "match" or "mismatch" for each row
  data_pop[, c("ALT_match_count", "ALT_mismatch_count") :=
               .(rowSums(.SD == "match", na.rm = TRUE),
                 rowSums(.SD == "mismatch", na.rm = TRUE)),
           .SDcols = cols_ALT]
  
  # Create columns for match and mismatch count for columns ending with _zcomp
  cols_Zigo <- grep("_zcomp$", names(data_pop), value = TRUE)
  
  # Calculate the count of "match" or "mismatch" for each row
  data_pop[, c("Zigo_match_count", "Zigo_mismatch_count") :=
               .(rowSums(.SD == "match", na.rm = TRUE),
                 rowSums(.SD == "mismatch", na.rm = TRUE)),
           .SDcols = cols_Zigo]
  
  # Now, you can summarize this for each SNP_id
  summary_pop <- data_pop[, .(
    REF_match = sum(REF_match_count, na.rm = TRUE),
    REF_mismatch = sum(REF_mismatch_count, na.rm = TRUE),
    ALT_match = sum(ALT_match_count, na.rm = TRUE),
    ALT_mismatch = sum(ALT_mismatch_count, na.rm = TRUE),
    Zigo_match = sum(Zigo_match_count, na.rm = TRUE),
    Zigo_mismatch = sum(Zigo_mismatch_count, na.rm = TRUE)
  ),
  by = SNP_id]
  
  # Sort data by SNP_id
  setorder(summary_pop, SNP_id)
  
  # Assign the summary_pop object to a new variable based on the object_suffix
  summary_pop_object_name <- paste0("summary_", population, "_", object_suffix)
  assign(summary_pop_object_name, summary_pop, envir = .GlobalEnv)
  
  # Return the summary_pop object
  return(summary_pop)
}
```


How to run the functions
```{r how_to_run_summary_sai_function, eval=FALSE}
summary_sai_ay <- generate_summary(data_ay_dt, "SAI", "suffix")
summary_kat_ay <- generate_summary(data_ay_dt, "KAT", "suffix")
dt_long_2_ay <- merge_and_transform("ay")
```

Function to merge the SAI and KAT summaries
```{r merge_and_transform_function}
merge_and_transform <- function(object_suffix) {
  # Merge summary_sai and summary_kat
  merged_sai_kat <- merge(
    get(paste0("summary_sai_", object_suffix)),
    get(paste0("summary_kat_", object_suffix)),
    by = "SNP_id",
    suffixes = c("_sai", "_kat")
  )
  
  # Select only the relevant columns
  dt <- merged_sai_kat[, .(
    SNP_id,
    REF_mismatch_sai,
    ALT_mismatch_sai,
    Zigo_mismatch_sai,
    REF_mismatch_kat,
    ALT_mismatch_kat,
    Zigo_mismatch_kat
  )]
  
  # Reshape data to long format
  dt_long <- melt(
    dt,
    id.vars = "SNP_id",
    variable.name = "type",
    value.name = "count"
  )
  
  # Convert to data.table if it's not already
  setDT(dt_long)
  
  # Extract the last part after "_" in the 'type' column to form 'group' column
  dt_long[, group := str_extract(type, "(?<=_)[^_]+$")]
  
  # Extract the part before the first "_" in the 'type' column to form 'allele' column
  dt_long[, allele := str_extract(type, "^[^_]+")]
  
  # Convert to numeric if it's not already
  dt_long[, count := as.numeric(count)]
  
  # Count occurrences per count value
  dt_long <- dt_long[, .(n = .N), by = .(allele, group, count)]
  
  # Calculate total count of unique SNPs
  total_SNP <- length(unique(dt$SNP_id))
  
  # Add a new column for the percentage
  dt_long[, perc := n / total_SNP * 100, by = group]
  
  # Set levels for 'group' variable
  dt_long$group <- factor(dt_long$group, levels = c("sai", "kat"))
  
  # Set levels for 'allele' variable
  dt_long$allele <- factor(dt_long$allele, levels = c("REF", "ALT", "Zigo"))
  
  # Modify levels for 'allele' variable
  levels(dt_long$allele) <- c("Reference Allele", "Alternative Allele", "Zygosity")
  
  # Modify levels for 'group' variable
  levels(dt_long$group) <- c("SAI", "KAT")
  
  dt_long$count <- as.numeric(dt_long$count)
  
  # Assign the dt_long object to a new variable
  dt_long_object_name <- paste0("dt_long_", object_suffix)
  assign(dt_long_object_name, dt_long, envir = .GlobalEnv)
  
  # Return the dt_long object
  return(dt_long)
}
```

Function to create plot comparing the two populations
```{r create_plot2_function}
create_plot2 <- function(object_suffix, output_path, dt_long) {
  # Create plot
  plot <- ggplot(dt_long, aes(x = count, y = n)) +
    geom_bar(
      stat = "identity",
      fill = "#ffcae4",
      color = ifelse(
        dt_long$count == 0,
        "#CCFF00",
        ifelse(dt_long$count == 1, "#4169E1", "#FF7F50")
      ),
      width = 0.6,
      linewidth = 1
    ) +
    geom_text(
      aes(label = paste0(
        scales::comma(n), " (", round(perc, 2), "%)"
      )),
      hjust = ifelse(dt_long$count == 0, .7, 0.01),
      size = 2.3,
      color = "gray10"
    ) +
    facet_wrap(~ group + allele, scales = "free_y", ncol = 3) +
    labs(
      title = paste("Histogram of SNP Mismatch Counts", object_suffix),
      x = "Count",
      y = "Frequency",
      caption = paste(
        object_suffix,
        "\n KAT 6 samples from native range         SAI 12 samples from invasive range\n Bar border colors: Electric Lime = no errors; Royal Blue =  1 error; Coral = more than 1 error"
      )
    ) +
    coord_flip() +
    my_theme() +
    # scale_y_continuous(labels = scales::comma) +
    scale_y_continuous(
      breaks = c(0, 25000, 50000, 75000, 100000, 125000, 150000, 175000),
      labels = function(x) paste0(x / 1000, "k"),
      expand = expansion(mult = c(0, 0.2))
    ) +
    scale_x_continuous(breaks = 0:18) +
    theme(
      plot.caption = element_text(
        face = "italic",
        size = 10,
        color = "grey20"
      ),
      panel.spacing = unit(3, "lines"),
      plot.margin = unit(c(1, 3, 1, 1), "cm"),
      axis.text.x = element_text(size = 7, angle = 0) 
    )
  
  # Print the plot in RStudio
  print(plot)
  
  # Save the plot
  ggsave(
    output_path,
    plot = plot,
    width = 8,
    height = 8,
    units = "in"
  )
}
```

How to run the functions
```{r how_to_run_summary_sai_function2, eval=FALSE}
summary_sai_ay <- generate_summary_sai(data_ay_dt, "ay")
summary_kat_ay <- generate_summary_kat(data_KAT, "ay")
dt_long_2_ay <- merge_and_transform("ay")
create_plot2("ay", here("output", "wgs_vs_chip", "figures", "ay_mismatches_SAI_KAT.pdf"), dt_long_2_ay)
```

Function to get counts for pairwise comparison plot

```{r calculate_counts_function}
calculate_counts <- function(data_dt) {
  # Initialize an empty list to hold the counts
  count_list <- list()

  # Select columns
  matching_columns <- colnames(data_dt)[grepl(pattern = "(_REF$|_ALT$|_zcomp$)", colnames(data_dt))]

  # Loop through each column
  for (column in matching_columns) {
    match_count <- sum(str_detect(data_dt[[column]], "match"), na.rm = TRUE)
    mismatch_count <- sum(str_detect(data_dt[[column]], "mismatch"), na.rm = TRUE)

    # Create a data.table with counts for the current column
    count_dt <- data.table(Column = column, Match = match_count, Mismatch = mismatch_count)

    # Add the count data.table to the list
    count_list[[column]] <- count_dt
  }

  # Combine all count data.tables into a single data.table
  counts <- rbindlist(count_list)

  # Calculate total
  counts <- counts |>
    mutate(Total = Match + Mismatch)

  # Create new columns: Population, Sample, and Comparison
  counts <- counts |>
    mutate(
      Population = sub("^([^_]+).*", "\\1", Column),
      Sample = sub("^.*_(\\d+).*", "\\1", Column),
      Comparison = sub(".*_([^_]+)$", "\\1", Column)
    )

  # Reorder the columns and create sample_id
  counts <- counts |>
    dplyr::select(Population, Sample, Comparison, Match, Mismatch, Total)

  # Calculate percentage columns
  counts <- counts |>
    mutate(
      Percent_Match = round((Match / Total) * 100, 2),
      Percent_Mismatch = round((Mismatch / Total) * 100, 2)
    )

  # Replace zcomp with Zygosity
  counts$Comparison <- gsub("zcomp", "Zygosity", counts$Comparison)

  # Define color palette
  color_palette <- c("#92C6FF", "#f5cb8b", "#bff28c")

  # Convert Sample to numeric and sort samples numerically within each Population group
  counts$Sample <- as.numeric(counts$Sample)
  counts <- counts |> arrange(Population, Sample)

  # Convert Sample column back to factor with sorted levels within each group
  counts$Sample <- factor(counts$Sample, levels = unique(counts$Sample))

  # Rename and reorder Comparison column
  counts <- counts |> mutate(
    Comparison_new = recode(
      Comparison,
      "REF" = "Reference Allele",
      "ALT" = "Alternative Allele",
      "Zygosity" = "Zygosity"
    )
  ) |> mutate(
    Comparison_new = factor(
      Comparison_new,
      levels = c("Reference Allele", "Alternative Allele", "Zygosity")
    )
  )

  return(counts)
}
```

Pairwise plotting function

```{r plot_counts_function}
plot_counts <- function(counts, output_file = NULL) {
  library(ggplot2)

  # Define color palette
  color_palette <- c("#92C6FF", "#f5cb8b", "#bff28c")

  # Create plot
  plot <- ggplot(counts,
                 aes(x = Sample, y = Mismatch, fill = Comparison)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(Population ~ Comparison_new,
               scales = "free_y",
               space = "free") +
    coord_flip() +
    labs(
      title = "SNP Mismatch Counts per Sample",
      x = "Sample",
      y = "Mismatches",
      caption = "Genotyping errors per sample within each population."
    ) +
    my_theme() +
    theme(panel.spacing = unit(0.5, "lines")) +
    geom_text(aes(label = paste0(
      scales::comma(Mismatch), " (", Percent_Mismatch, "%)"
    )),
    hjust = 1,
    size = 2.5) +
    scale_fill_manual(values = color_palette) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1, size = 7)) +
    guides(fill = "none") +
    theme(plot.caption = element_text(
      face = "italic",
      size = 10,
      color = "grey20"
    )) +
    scale_y_continuous(labels = scales::comma)  # Add thousands separator to y-axis labels

  # Save the plot if output_file is provided
  if (!is.null(output_file)) {
    ggsave(output_file, plot, width = 8, height = 7, units = "in")
  }

  # Return the plot
  return(plot)
}
```


How to run the calculate_counts and plot_counts functions
```{r run_calculate_counts_plot_4, eval=FALSE}
# Call the function with data_*_dt as input
counts_ay <- calculate_counts(data_ay_dt)
plot_counts(counts_ay, here("output", "wgs_vs_chip", "figures", "ay_SAI_KAT_per_sample_stats.pdf"))
```

The comparisons we will make:

Chip:
"ab" - Genotyping calls using 18 versus 95 samples
"ac" - Genotyping calls using 18 versus 500 samples
"bc" - Genotyping calls using 95 versus 500 samples

WGS:
"xy" Genotyping calls with 18 versus 30 samples
"wy" Genotyping calls with 18 versus 800 samples
"wx" Genotyping calls with 30 versus 800 samples

Chip x WGS:
"ay" - WGS and chip calls with 18 samples
"bx" - WGS call with 30 samples and chip call with 95 samples
"cw" - WGS call with 800 samples and chip call with 500 samples


## 10. Chip comparisons

### 10.1 "ab" - Genotype calls using 18 versus 95 samples

Generate csv files
```{bash ab_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py ab
```

Import csv
```{r ab_import_csv}
data_ab_dt <- process_csv_files("ab")

# Check and display only columns that match the criteria
head(data_ab_dt[, c("SNP_id", names(data_ab_dt)[grepl("_REF$|_ALT$", names(data_ab_dt))]), with = FALSE])
```

Get the summary
```{r ab_summary}
summary_ab <- process_data_object("data_ab_dt")
head(summary_ab)
```

Check NAs, match and mismatch counts
```{r}
table(data_ab_dt$KAT_11a_KAT_11b_zcomp, useNA = "ifany")
```

Make data long format for plotting
```{r ab_data_long}
dt_long_ab <- process_summary_object("summary_ab")
head(dt_long_ab)
```

Create plot of SNP error per sample
```{r ab_plot1}
plot_dt_long("ab")
```

Compare both populations
```{r ab_SAI_KAT5}
summary_sai_ab <- generate_summary(data_ab_dt, "SAI", "suffix")
summary_kat_ab <- generate_summary(data_ab_dt, "KAT", "suffix")
dt_long_2_ab <- merge_and_transform("ab")
create_plot2("ab", here("output", "wgs_vs_chip", "figures", "ab_mismatches_SAI_KAT.pdf"), dt_long_2_ab)
```


Counts plot
```{r run_calculate_counts_plot_6}
# Call the function with data_*_dt as input
counts_ab <- calculate_counts(data_ab_dt)
plot_counts(counts_ab, here("output", "wgs_vs_chip", "figures", "ab_SAI_KAT_per_sample_stats.pdf"))
```

### 10.2 "ac" - Genotype calls using 18 versus 500 samples

Generate csv files
```{bash ac_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py ac
```

Import csv
```{r ac_import_csv}
data_ac_dt <- process_csv_files("ac")

# Check and display only columns that match the criteria
head(data_ac_dt[, c("SNP_id", names(data_ac_dt)[grepl("_REF$|_ALT$", names(data_ac_dt))]), with = FALSE])
```

Get the summary
```{r ac_summary}
summary_ac <- process_data_object("data_ac_dt")
head(summary_ac)
```

Make data long format for plotting
```{r ac_data_long}
dt_long_ac <- process_summary_object("summary_ac")
head(dt_long_ac)
```

Create plot of SNP error per sample
```{r ac_plot1}
plot_dt_long("ac")
```

Compare both populations
```{r ac_SAI_KAT}
summary_sai_ac <- generate_summary(data_ac_dt, "SAI", "suffix")
summary_kat_ac <- generate_summary(data_ac_dt, "KAT", "suffix")
dt_long_2_ac <- merge_and_transform("ac")
create_plot2("ac", here("output", "wgs_vs_chip", "figures", "ac_mismatches_SAI_KAT.pdf"), dt_long_2_ac)
```

Counts plot
```{r run_calculate_counts_plot_9}
# Call the function with data_*_dt as input
counts_ac <- calculate_counts(data_ac_dt)
plot_counts(counts_ac, here("output", "wgs_vs_chip", "figures", "ac_SAI_KAT_per_sample_stats.pdf"))
```

### 10.3 "bc" - Genotype calls using 95 versus 500 samples

Generate csv files
```{bash bc_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py bc
```

Import csv
```{r bc_import_csv}
data_bc_dt <- process_csv_files("bc")

# Check and display only columns that match the criteria
head(data_bc_dt[, c("SNP_id", names(data_bc_dt)[grepl("_REF$|_ALT$", names(data_bc_dt))]), with = FALSE])
```

Get the summary
```{r bc_summary}
summary_bc <- process_data_object("data_bc_dt")
head(summary_bc)
```

Make data long format for plotting
```{r bc_data_long}
dt_long_bc <- process_summary_object("summary_bc")
head(dt_long_bc)
```

Create plot of SNP error per sample
```{r ab_plot1a}
plot_dt_long("bc")
```

Compare both populations
```{r bc_SAI_KAT}
summary_sai_bc <- generate_summary(data_bc_dt, "SAI", "suffix")
summary_kat_bc <- generate_summary(data_bc_dt, "KAT", "suffix")
dt_long_2_bc <- merge_and_transform("bc")
create_plot2("bc", here("output", "wgs_vs_chip", "figures", "bc_mismatches_SAI_KAT.pdf"), dt_long_2_bc)
```

Counts plot
```{r run_calculate_counts_plot3}
# Call the function with data_*_dt as input
counts_bc <- calculate_counts(data_bc_dt)
plot_counts(counts_bc, here("output", "wgs_vs_chip", "figures", "bc_SAI_KAT_per_sample_stats.pdf"))
```

## 11. WGS comparsions

### 11.1 "xy" Genotyping calls with 18 versus 30 samples

Generate csv files
```{bash by_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py xy
```

Import csv
```{r xy_import_csv}
data_xy_dt <- process_csv_files("xy")

# Check and display only columns that match the criteria
head(data_xy_dt[, c("SNP_id", names(data_xy_dt)[grepl("_REF$|_ALT$", names(data_xy_dt))]), with = FALSE])
```

Get the summary
```{r xy_summary}
summary_xy <- process_data_object("data_xy_dt")
head(summary_xy)
```

Make data long format for plotting
```{r xy_data_long}
dt_long_xy <- process_summary_object("summary_xy")
head(dt_long_xy)
```

Create plot of SNP error per sample
```{r xy_plot1b}
plot_dt_long("xy")
```

Compare both populations
```{r xy_SAI_KAT}
summary_sai_xy <- generate_summary(data_xy_dt, "SAI", "suffix")
summary_kat_xy <- generate_summary(data_xy_dt, "KAT", "suffix")
dt_long_2_xy <- merge_and_transform("xy")
create_plot2("xy", here("output", "wgs_vs_chip", "figures", "xy_mismatches_SAI_KAT.pdf"), dt_long_2_xy)
```

Counts plot
```{r run_calculate_counts_plot7}
# Call the function with data_*_dt as input
counts_xy <- calculate_counts(data_xy_dt)
plot_counts(counts_xy, here("output", "wgs_vs_chip", "figures", "xy_SAI_KAT_per_sample_stats.pdf"))
```

### 11.2 "ey" Genotyping calls with 18 versus 800 samples

Generate csv files
```{bash wy_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py wy
```

Import csv
```{r wy_import_csv}
data_wy_dt <- process_csv_files("wy")

# Check and display only columns that match the criteria
head(data_wy_dt[, c("SNP_id", names(data_wy_dt)[grepl("_REF$|_ALT$", names(data_wy_dt))]), with = FALSE])
```

Get the summary
```{r wy_summary}
summary_wy <- process_data_object("data_wy_dt")
head(summary_wy)
```

Make data long format for plotting
```{r wy_data_long}
dt_long_wy <- process_summary_object("summary_wy")
head(dt_long_wy)
```

Create plot of SNP error per sample
```{r aw_plot1}
plot_dt_long("wy")
```

Compare both populations
```{r aw_SAI_KAT}
summary_sai_wy <- generate_summary(data_wy_dt, "SAI", "suffix")
summary_kat_wy <- generate_summary(data_wy_dt, "KAT", "suffix")
dt_long_2_wy <- merge_and_transform("wy")
create_plot2("wy", here("output", "wgs_vs_chip", "figures", "wy_mismatches_SAI_KAT.pdf"), dt_long_2_wy)
```

Counts plot
```{r run_calculate_counts_plot8}
# Call the function with data_*_dt as input
counts_wy <- calculate_counts(data_wy_dt)
plot_counts(counts_wy, here("output", "wgs_vs_chip", "figures", "wy_SAI_KAT_per_sample_stats.pdf"))
```

### 11.3 "wx" Genotyping calls with 30 versus 800 samples

Generate csv files
```{bash wx_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py wx
```

Import csv
```{r wx_import_csv}
data_wx_dt <- process_csv_files("wx")

# Check and display only columns that match the criteria
head(data_wy_dt[, c("SNP_id", names(data_wy_dt)[grepl("_REF$|_ALT$", names(data_wy_dt))]), with = FALSE])
```

Get the summary
```{r wx_summary}
summary_wx <- process_data_object("data_wx_dt")
head(summary_wx)
```

Make data long format for plotting
```{r wx_data_long}
dt_long_wx <- process_summary_object("summary_wx")
head(dt_long_wx)
```

Create plot of SNP error per sample
```{r wx_plot1}
plot_dt_long("wx")
```

Compare both populations
```{r wx_SAI_KAT}
summary_sai_wx <- generate_summary(data_wx_dt, "SAI", "suffix")
summary_kat_wx <- generate_summary(data_wx_dt, "KAT", "suffix")
dt_long_2_wx <- merge_and_transform("wx")
create_plot2("wx", here("output", "wgs_vs_chip", "figures", "wx_mismatches_SAI_KAT.pdf"), dt_long_2_wx)
```

Counts plot
```{r wx_run_calculate_counts_plot9}
# Call the function with data_*_dt as input
counts_wx <- calculate_counts(data_wx_dt)
plot_counts(counts_wx, here("output", "wgs_vs_chip", "figures", "wx_SAI_KAT_per_sample_stats.pdf"))
```

## 12. Chip and WGS comparisons

###  12.1 "ay" - WGS and chip calls with 18 samples

Generate csv files
```{bash ya_create_csv_from_vcfs.py}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py ay
```

Import csv
```{r ay_import_csv2}
data_ay_dt <- process_csv_files("ay")

# Check and display only columns that match the criteria
head(data_ay_dt[, c("SNP_id", names(data_ay_dt)[grepl("_REF$|_ALT$", names(data_ay_dt))]), with = FALSE])
```

Get the summary
```{r ay_summary}
summary_ay <- process_data_object("data_ay_dt")
head(summary_ay)
```

Make data long format for plotting
```{r ay_data_long}
dt_long_ay <- process_summary_object("summary_ay")
head(dt_long_ay)
```

Create plot of SNP error per sample
```{r ay_plot1}
plot_dt_long("ay")
```

Compare both populations
```{r ab_SAI_KAT1}
summary_sai_ay <- generate_summary(data_ay_dt, "SAI", "suffix")
summary_kat_ay <- generate_summary(data_ay_dt, "KAT", "suffix")
dt_long_2_ay <- merge_and_transform("ay")
create_plot2("ay", here("output", "wgs_vs_chip", "figures", "ay_mismatches_SAI_KAT.pdf"), dt_long_2_ay)
```

Counts plot
```{r run_calculate_counts_plot_0}
# Call the function with data_*_dt as input
counts_ay <- calculate_counts(data_ay_dt)
plot_counts(counts_ay, here("output", "wgs_vs_chip", "figures", "ay_SAI_KAT_per_sample_stats.pdf"))
```

###  12.2 "bx" - WGS call with 30 samples and chip call with 95 samples

Generate csv files
```{bash bx_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py bx
```

Import csv
```{r ay_import_csv}
data_bx_dt <- process_csv_files("bx")

# Check and display only columns that match the criteria
head(data_bx_dt[, c("SNP_id", names(data_bx_dt)[grepl("_REF$|_ALT$", names(data_bx_dt))]), with = FALSE])
```

Get the summary
```{r bx_summary}
summary_bx <- process_data_object("data_bx_dt")
head(summary_bx)
```

Make data long format for plotting
```{r bx_data_long}
dt_long_bx <- process_summary_object("summary_bx")
head(dt_long_bx)
```

Create plot of SNP error per sample
```{r bx_plot1}
plot_dt_long("bx")
```

Compare both populations
```{r ab_SAI_KAT2}
summary_sai_bx <- generate_summary(data_bx_dt, "SAI", "suffix")
summary_kat_bx <- generate_summary(data_bx_dt, "KAT", "suffix")
dt_long_2_bx <- merge_and_transform("bx")
create_plot2("bx", here("output", "wgs_vs_chip", "figures", "bx_mismatches_SAI_KAT.pdf"), dt_long_2_bx)
```

Counts plot
```{r run_calculate_counts_plot_b}
# Call the function with data_*_dt as input
counts_bx <- calculate_counts(data_bx_dt)
plot_counts(counts_bx, here("output", "wgs_vs_chip", "figures", "bx_SAI_KAT_per_sample_stats.pdf"))
```

###  12.3 "cw" - WGS call with 800 samples and chip call with 500 samples

Generate csv files
```{bash cw_create_csv_from_vcfs.py, eval=FALSE}
python output/wgs_vs_chip/scripts/create_csv_from_vcfs.py cw
```

Import csv
```{r cw_import_csv}
data_cw_dt <- process_csv_files("cw")

# Check and display only columns that match the criteria
head(data_cw_dt[, c("SNP_id", names(data_cw_dt)[grepl("_REF$|_ALT$", names(data_cw_dt))]), with = FALSE])
```

Get the summary
```{r cw_summary}
summary_cw <- process_data_object("data_cw_dt")
head(summary_cw)
```

Make data long format for plotting
```{r cw_data_long}
dt_long_cw <- process_summary_object("summary_cw")
head(dt_long_cw)
```

Create plot of SNP error per sample
```{r xy_plot1a}
plot_dt_long("cw")
```

Compare both populations
```{r ab_SAI_KAT3}
summary_sai_cw <- generate_summary(data_cw_dt, "SAI", "suffix")
summary_kat_cw <- generate_summary(data_cw_dt, "KAT", "suffix")
dt_long_2_cw <- merge_and_transform("cw")
create_plot2("cw", here("output", "wgs_vs_chip", "figures", "cw_mismatches_SAI_KAT.pdf"), dt_long_2_cw)
```

Counts plot
```{r ab_run_calculate_counts_plot}
# Call the function with data_*_dt as input
counts_cw <- calculate_counts(data_cw_dt)
plot_counts(counts_cw, here("output", "wgs_vs_chip", "figures", "cw_SAI_KAT_per_sample_stats.pdf"))
```

## 13. Statistical comparisons

Function to get the Zygosity summary for each object
```{r}
create_Zygosity_df <- function(counts_df) {
  Zygosity_df <- counts_df |>
    filter(Comparison == "Zygosity") |>
    dplyr::select(
      Population,
      Sample,
      Total,
      Match,
      Percent_Match,
      Mismatch,
      Percent_Mismatch
    )
  
  return(Zygosity_df)
}
```

Apply the function
```{r}
# chip
Zygosity_ab <- create_Zygosity_df(counts_ab)
Zygosity_ac <- create_Zygosity_df(counts_ac)
Zygosity_bc <- create_Zygosity_df(counts_bc)

# wgs
Zygosity_xy <- create_Zygosity_df(counts_xy)
Zygosity_wy <- create_Zygosity_df(counts_wy)
Zygosity_wx <- create_Zygosity_df(counts_wx)

# wgs x chip
Zygosity_ay <- create_Zygosity_df(counts_ay)
Zygosity_bx <- create_Zygosity_df(counts_bx)
Zygosity_cw <- create_Zygosity_df(counts_cw)
```

Use library(ggstatsplot) to compare the mean error rate for Zygosity.
We classified each loci as homo_ref, homo_alt, and het. Then we checked if they matched or not.
```{r}
# Add source columns to each data frame
Zygosity_ab$Source <- 'ab'
Zygosity_ac$Source <- 'ac'
Zygosity_bc$Source <- 'bc'

Zygosity_xy$Source <- 'xy'
Zygosity_wy$Source <- 'wy'
Zygosity_wx$Source <- 'wx'

Zygosity_ay$Source <- 'ay'
Zygosity_bx$Source <- 'bx'
Zygosity_cw$Source <- 'cw'


# Combine all data frames
combined_data <-
  rbind(
    Zygosity_ab,
    Zygosity_ac,
    Zygosity_bc,
    Zygosity_xy,
    Zygosity_wy,
    Zygosity_wx,
    Zygosity_ay,
    Zygosity_bx,
    Zygosity_cw
  )
```


For KAT
```{r}
# Specify the desired order
desired_order <- c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw")

# Convert the 'Source' column to a factor and specify the order of the levels
combined_data$Source <- factor(combined_data$Source, levels = desired_order)

# For KAT
data_KAT_t <- subset(combined_data, Population == "KAT")


# first, assign the plot to a variable
plot_KAT_plot <- ggbetweenstats(
  data = data_KAT_t,
  x = Source,
  y = Percent_Mismatch,
  title = "Genotyping mismatches for KAT (native)",
  type = "nonparametric", 
  pairwise.comparisons = TRUE, 
  pairwise.display = "significant",
  palette = "RdYlBu", # change to a different palette if you prefer
  package = "RColorBrewer"
)

plot_KAT_plot

# Use here function to specify the path
output_path <- here("output", "wgs_vs_chip", "figures", "stats_KAT.pdf")

# Save the plot
ggsave(filename = output_path, plot = plot_KAT_plot, width = 10, height = 7, dpi = 300)
```

For SAI
```{r}
# For SAI
data_SAI_t <- subset(combined_data, Population == "SAI")

# first, assign the plot to a variable
plot_SAI_plot <- ggbetweenstats(
  data = data_SAI_t,
  x = Source,
  y = Percent_Mismatch,
  title = "Genotyping mismatches for SAI (invasive)",
  type = "nonparametric", 
  pairwise.comparisons = TRUE, 
  pairwise.display = "significant",
  palette = "RdYlBu", # change to a different palette if you prefer
  package = "RColorBrewer"
)

plot_SAI_plot

# Use here function to specify the path
output_path <- here("output", "wgs_vs_chip", "figures", "stats_SAI.pdf")

# Save the plot
ggsave(filename = output_path, plot = plot_SAI_plot, width = 10, height = 7, dpi = 300)
```

Comparison irrespective of population
```{r}
plot_both_plot<- ggbetweenstats(
  data = combined_data, # using the entire data here, not just KAT
  x = Source,
  y = Percent_Mismatch,
  title = "Comparison of mean percent mismatch between sources",
  type = "nonparametric",
  pairwise.comparisons = TRUE, 
  pairwise.display = "significant",
  palette = "RdYlBu",
  package = "RColorBrewer"
)

plot_both_plot

# Use here function to specify the path
output_path <- here("output", "wgs_vs_chip", "figures", "stats_both.pdf")

# Save the plot
ggsave(filename = output_path, plot = plot_both_plot, width = 10, height = 7, dpi = 300)
```

We can use library broom to get a table
```{r warning=FALSE}
set.seed(123)
# I put warning=FALSE because some of the values are close. In the next chunk we add some jitter and we will not get warnings.

# Conduct pairwise Wilcoxon test
result <- pairwise.wilcox.test(
    combined_data$Percent_Mismatch,
    combined_data$Source,
    p.adjust.method = "holm"
)

# Tidy the result to a dataframe
result_tidy <- broom::tidy(result)

# Print the result
print(result_tidy)
```

Add jitters
```{r}
set.seed(123)
# If we add jitters the p-values are slightly different.
combined_data$Percent_Mismatch_jitter <- jitter(combined_data$Percent_Mismatch, amount = 1e-9)

result <- pairwise.wilcox.test(
    combined_data$Percent_Mismatch_jitter,
    combined_data$Source,
    p.adjust.method = "holm"
)

# Tidy the result to a dataframe
result_tidy <- broom::tidy(result)

# Print the result
print(result_tidy)
```

Create table
```{r warning=FALSE}
# Calculate mean
mean_df <- combined_data |>
  group_by(Source)  |>
  summarise(Mean_Percent_Mismatch = mean(Percent_Mismatch, na.rm = TRUE))

# Calculate median
median_df <- combined_data |>
  group_by(Source) |>
  summarise(Median_Percent_Mismatch = median(Percent_Mismatch, na.rm = TRUE))

# Pairwise Wilcoxon test
result <- pairwise.wilcox.test(
    combined_data$Percent_Mismatch,
    combined_data$Source,
    p.adjust.method = "holm"
)

# # Extract p-values and tidy the result into a data frame
pvalues_df <- as.data.frame(result$p.value) |>
  rownames_to_column("Source") |>
  dplyr::rename(P_Value = 2)
# pvalues_df <-
#   tibble::rownames_to_column(as.data.frame(result$p.value), "Source") %>%
#   dplyr::rename(P_Value = 2)

# Merge mean, median and p-values into one table
summary_df <- full_join(mean_df, median_df, by = "Source") |>
  full_join(pvalues_df, by = "Source")

# Rename "Source" column
summary_df <- dplyr::rename(summary_df, Comparison = Source)

# Function to format data for mean and median
format_mean_median <- function(x) {
  round(x, 2)
}

# Function to format data for p-values
format_pvalue <- function(x) {
  formatted_x <- ifelse(abs(x) < 1e-4, formatC(x, format = "e", digits = 4), round(x, 4))
  # Append an asterisk for p-values below 0.05
  ifelse(x < 0.05, paste0(formatted_x, "*"), formatted_x)
}


# Apply the function to each column as needed
summary_df$Mean_Percent_Mismatch <- format_mean_median(summary_df$Mean_Percent_Mismatch)
summary_df$Median_Percent_Mismatch <- format_mean_median(summary_df$Median_Percent_Mismatch)

# Apply the function to each column as needed
pvalue_cols <- colnames(summary_df)[-(1:3)]
for(col in pvalue_cols){
  summary_df[[col]] <- format_pvalue(summary_df[[col]])
}


# Create the flextable
ft <- flextable::flextable(summary_df)

# Apply zebra theme
ft <- flextable::theme_zebra(ft)

# Add a caption to the table
ft <- flextable::add_header_lines(ft, "Table 1: Mean and Median Percent Mismatch by Comparison. The P-values are from a pairwise Wilcoxon test with Holm adjustment for multiple comparisons. An asterisk (*) next to a P-value indicates a statistically significant difference (P < 0.05).")

# Save it to a Word document
officer::read_docx() |>
  body_add_flextable(ft) |>
  print(target = here::here("output", "wgs_vs_chip", "figures", "summary_table.docx"))

ft
```


## 14. Check how many samples a SNP might have errors

We can look at each population or across all samples. The code below assumes you have all the data loaded.

### 14.1 For chip (ac)

The "ac" comparison is for when we genotype 18 samples alone or use around 500 samples in the genotype call.

We extracted the 18 samples out of the full data set to compare with the 18 samples genotyped alone.

How many SNPs have discrepancies in the genotypes in 1 or more samples (out of the 18 samples)

```{r subset_mismatches_01_ac}
# Discrepancies in 1 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_ac_dt$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ac_dt[data_ac_dt$REF_mismatch_count >= 1,]$SNP_id
           )
         )
cat("REF mismatch at in 1 sample:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ac_dt[data_ac_dt$ALT_mismatch_count >= 1,,]$SNP_id
           )
         )
cat("ALT mismatch at least in 1 sample:", failed_snpsA, "\n")


# How many SNPs failed zygosity
failed_snps <-
  length(
    unique(data_ac_dt[data_ac_dt$Zigo_mismatch_count >= 1,,]$SNP_id
           )
         )
cat("Zygosity mismatch in at least 1 sample:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in 1 or more samples:", percentage_failed, "%\n")
```

When we look at the Zygosity of each SNP we find that 10,545 SNPs have mismatches (11.68%). However, we see from the previous plot that we have SNPs showing discrepancies in only 1 sample out of the 18 samples. 

Check how many SNPs have erros in 2 or more samples 

```{r subset_mismatches_02_ac}
# Discrepancies in 2 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_ac_dt$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ac_dt[data_ac_dt$REF_mismatch_count >= 2,]$SNP_id
           )
         )
cat("REF mismatch in 2 or more samples:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ac_dt[data_ac_dt$ALT_mismatch_count >= 2,]$SNP_id
           )
         )
cat("ALT mismatch in 2 or more samples:", failed_snpsA, "\n")


# How many SNPs failed
failed_snps <-
  length(
    unique(data_ac_dt[data_ac_dt$Zigo_mismatch_count >= 2,]$SNP_id
           )
         )
cat("Zygosity mismatch in 2 or more samples:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in 2 or more samples:", percentage_failed, "%\n")
```

We can check how many times a SNP has mismatching Zygosity or alleles across the 18 samples.
```{r}
# Number of samples you want to iterate over
num_samples <- 18

# Create an empty data frame to store results
results2 <- data.frame()

# How many SNPs we tested
tested_snps <- length(unique(data_ac_dt$SNP_id))

for(i in 1:num_samples){
  
  # How many SNPs failed REF
  failed_snpsR <- length(unique(data_ac_dt[data_ac_dt$REF_mismatch_count >= i,]$SNP_id))
  
  # How many SNPs failed ALT
  failed_snpsA <- length(unique(data_ac_dt[data_ac_dt$ALT_mismatch_count >= i,]$SNP_id))
  
  # How many SNPs failed zygosity
  failed_snpsZ <- length(unique(data_ac_dt[data_ac_dt$Zigo_mismatch_count >= i,]$SNP_id))
  
  # Calculate percentage
  percentage_failed <- round(failed_snpsZ / tested_snps * 100, 2)
  
  # Create a data frame with results for this number of samples
  temp_results <- data.frame(
    'Samples' = i,
    'SNPs' = tested_snps,
    'Mismatch_REF' = failed_snpsR,
    'Mismatch_ALT' = failed_snpsA,
    'Mismatch_Zygosity' = failed_snpsZ,
    'Mismatch_Zygosity_perc' = percentage_failed
  )
  
  # Append the results to the main results data frame
  results2 <- rbind(results2, temp_results)
  
}

# Create the flextable
ft <- flextable(results2)

# Apply zebra theme
ft <- theme_zebra(ft)

# Add a caption to the table
ft <- add_header_lines(ft, "Table 2: Summary of the SNP mismatch rate for the 18 samples genotyped alone or with 500 samples. ")

# Save it to a Word document
officer::read_docx() |>
  body_add_flextable(ft) |>
  print(target = here::here("output", "wgs_vs_chip", "figures", "summary_ac.docx"))
```

KAT
```{r get_stats_kat_ay2}
# Discrepancies in 1 or more samples
# How many SNPs we tested
tested_snps_ac <- length(unique(summary_kat_ac$SNP_id))
cat("Number of SNPs tested:", tested_snps_ac, "\n")

# How many SNPs failed
failed_kat_ac <-
  length(unique(summary_kat_ac[summary_kat_ac$REF_mismatch > 0 |
                           summary_kat_ac$ALT_mismatch > 0 |
                           summary_kat_ac$Zigo_mismatch > 0, ]$SNP_id))
cat("Number of SNPs failed:", failed_kat_ac, "\n")

# Calculate percentage
percentage_failed_ac <- round(failed_kat_ac / tested_snps_ac * 100, 2)
cat("Percentage of failed SNPs:", percentage_failed_ac, "%\n")


# How many SNPs failed KAT
failed_kat_ac <-
  length(unique(summary_kat_ac[summary_kat_ac$REF_mismatch > 0 |
                           summary_kat_ac$ALT_mismatch > 0 |
                           summary_kat_ac$Zigo_mismatch > 0, ]$SNP_id))
cat("Number of SNPs failed:", failed_kat_ac, "\n")


# How many SNPs failed SAI
failed_sai_ac <-
  length(unique(summary_sai_ac[summary_sai_ac$REF_mismatch > 0 |
                           summary_sai_ac$ALT_mismatch > 0 |
                           summary_sai_ac$Zigo_mismatch > 0, ]$SNP_id))
cat("Number of SNPs failed:", failed_sai_ac, "\n")

# Calculate percentage
percentage_kat_ac <- round(failed_kat_ac / tested_snps_ac * 100, 2)
cat("Percentage of failed SNPs:", percentage_kat_ac, "%\n")
percentage_sai_ac <- round(failed_sai_ac / tested_snps_ac * 100, 2)
cat("Percentage of failed SNPs:", percentage_sai_ac, "%\n")
```

Summary
```{r}
# Create an empty data frame to store results
results_ac <- data.frame()

# How many SNPs we tested
tested_snps_ac <- length(unique(summary_kat_ac$SNP_id))

# Datasets and corresponding number of samples
datasets_ac <- list(KAT=list(data=summary_kat_ac, num_samples=6), SAI=list(data=summary_sai_ac, num_samples=12))

for(name in names(datasets_ac)){
  data <- datasets_ac[[name]]$data
  num_samples <- datasets_ac[[name]]$num_samples
  
  for(i in 1:num_samples){
    
    # How many SNPs failed
    failed_snps <- length(unique(data[data$REF_mismatch >= i |
                                      data$ALT_mismatch >= i |
                                      data$Zigo_mismatch >= i, ]$SNP_id))
    
    # Calculate percentage
    percentage_failed <- round(failed_snps / tested_snps_ac * 100, 2)
    
    # Create a data frame with results for this number of samples
    temp_results <- data.frame(
      'Data_Set' = name,
      'Num_Samples' = i,
      'Tested_SNPs' = tested_snps_ac,
      'Failed_SNPs' = failed_snps,
      'Perc_ac' = percentage_failed
    )
    
    # Append the results to the main results data frame
    results_ac <- rbind(results_ac, temp_results)
  }
}

# Print the results
print(results_ac)
```

We can get the percentage of failing SNPs for all the comparisons we made: 
c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw")

```{r percentage_fail_Zygosity_all_data_sets_overall}
# Your data set identifiers
datasets_identifiers <- c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw")

# Define all possible 'Num_Samples' 
all_samples <- 1:18

# Initialize an empty list to hold results data frames for each data set
results_list <- list()

# Iterate over the data set identifiers
for(ds_id in datasets_identifiers){
  
  # Generate the variable name for this data set
  summary_var_name <- paste0("summary_", ds_id)
  
  # Retrieve the data frame
  summary_data <- get(summary_var_name)
  
  # Create an empty data frame to store results with all possible 'Num_Samples'
  results <- data.frame(Num_Samples = all_samples)
  
  for(i in 1:18){
    
    # How many SNPs we tested
    tested_snps <- length(unique(summary_data$SNP_id))
    
    # How many SNPs failed
    failed_snps <- length(unique(summary_data[summary_data$REF_mismatch >= i |
                                              summary_data$ALT_mismatch >= i |
                                              summary_data$Zigo_mismatch >= i, ]$SNP_id))
    
    # Calculate percentage
    percentage_failed <- round(failed_snps / tested_snps * 100, 2)
    
    # Assign the results to the corresponding row
    results[i, paste0('Perc_', ds_id)] <- percentage_failed
  }
  
  # Add the results data frame to the list
  results_list[[ds_id]] <- results
}

# Initialize the final merged results data frame with just 'Num_Samples' and the first percentage column.
merged_results <- results_list[[datasets_identifiers[1]]]

# Merge all other results data frames into the final results data frame
for(ds_id in datasets_identifiers[-1]){
  merged_results <- merge(merged_results, results_list[[ds_id]], by = "Num_Samples", all = TRUE)
}

# Rename 'Num_Samples' to 'n_sample_fail'
names(merged_results)[names(merged_results) == "Num_Samples"] <- "n_sample_fail"

# Remove 'Perc_' from other column names
names(merged_results)[-1] <- sub("Perc_", "", names(merged_results)[-1])

# Create the flextable
ft <- flextable::flextable(merged_results)

# Apply zebra theme
ft <- flextable::theme_zebra(ft)

# Add a caption to the table
ft <- flextable::add_header_lines(ft, "Table 3: SNP mismatch percentage for Zygosity across all data set comparisons. ")

# Save it to a Word document
officer::read_docx() |>
  body_add_flextable(ft) |>
  print(target = here::here("output", "wgs_vs_chip", "figures", "summary_all_data_sets.docx"))

ft
```

Create a plot
Theme for plotting
```{r}
# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R" # choose my_theme.R (Roboto Condensed) or my_theme2.R (default font)
  )
)
```

Plot
```{r plot}
# Convert the data frame from wide to long format
long_results <- merged_results %>%
  pivot_longer(
    cols = -n_sample_fail,
    names_to = "Data_Set",
    values_to = "Percentage"
  )

# Specify the order of the fill factor
long_results$Data_Set <-
  factor(long_results$Data_Set,
         levels = c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw"))

# Define color blind friendly palette
color_blind_friendly <- c(
  "Chip (ab)" = "#E69F00",
  "Chip (ac)" = "#56B4E9",
  "Chip (bc)" = "#009E73",
  "WGS (xy)" = "#F0E442",
  "WGS (wy)" = "#0072B2",
  "WGS (wx)" = "#D55E00",
  "WGS_Chip (ay)" = "#CC79A7",
  "WGS_Chip (bx)" = "#999999",
  "WGS_Chip (cw)" = "#000000"
)

# Create a named vector to recode Data_Set column
recode_vector <- c(
  "ab" = "Chip (ab)",
  "ac" = "Chip (ac)",
  "bc" = "Chip (bc)",
  "xy" = "WGS (xy)",
  "wy" = "WGS (wy)",
  "wx" = "WGS (wx)",
  "ay" = "WGS_Chip (ay)",
  "bx" = "WGS_Chip (bx)",
  "cw" = "WGS_Chip (cw)"
)

# Recode the Data_Set column
long_results$Data_Set <- recode_vector[long_results$Data_Set]

# Create the bar plot with new legend labels
ggplot(long_results,
       aes(x = n_sample_fail, y = Percentage, fill = Data_Set)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    x = "Samples (n)",
    y = "SNPs with mismatches (%)",
    fill = "Comparison",
    title = "Cummulative mismatches by number of samples for each SNP",
    caption =  "Number of samples per genotype call: \nChip:\n'ab' - 18 versus 95 samples\n'ac' -  18 versus 500 samples\n'bc' - 95 versus 500 samples\n\nWGS:\n'xy' - 18 versus 30 samples\n'wy' - 18 versus 800 samples\n'wx' - Genotyping calls with 30 versus 800 samples\n\nChip x WGS:\n'ay' - both 18 samples\n'bx' - WGS 30 samples and chip 95 samples\n'cw' - WGS 800 samples and chip 500 samples"
  ) +
  scale_fill_manual(
    values = color_blind_friendly,
    labels = c(
      "Chip (ab)" = "Chip (ab)",
      "Chip (ac)" = "Chip (ac)",
      "Chip (bc)" = "Chip (bc)",
      "WGS (xy)" = "WGS (xy)",
      "WGS (wy)" = "WGS (wy)",
      "WGS (wx)" = "WGS (wx)",
      "WGS_Chip (ay)" = "WGS_Chip (ay)",
      "WGS_Chip (bx)" = "WGS_Chip (bx)",
      "WGS_Chip (cw)" = "WGS_Chip (cw)"
    )
  ) +
  coord_flip() +
  my_theme() +
  scale_x_continuous(breaks = seq(0, 18, 1)) +
  theme(
    legend.position = "top",
    plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    )
  )  # This changes the caption's size, color, and makes it italic.

# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "percentage_all_samples.pdf"
  ),
  height = 10,
  width = 8,
  dpi = 300
)
```


Per population
```{r}
# Your data set identifiers
datasets_identifiers <-
  c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw")

# Initialize an empty list to hold results data frames for each data set
results_list <- list()

# Iterate over the data set identifiers
for (ds_id in datasets_identifiers) {
  # Generate the variable names for this data set
  kat_var_name <- paste0("summary_kat_", ds_id)
  sai_var_name <- paste0("summary_sai_", ds_id)
  
  # Retrieve the data frames
  summary_kat <- get(kat_var_name)
  summary_sai <- get(sai_var_name)
  
  # Datasets and corresponding number of samples
  datasets <-
    list(
      KAT = list(data = summary_kat, num_samples = 6),
      SAI = list(data = summary_sai, num_samples = 12)
    )
  
  # Create an empty data frame to store results
  results <- data.frame()
  
  for (name in names(datasets)) {
    data <- datasets[[name]]$data
    num_samples <- datasets[[name]]$num_samples
    
    for (i in 1:num_samples) {
      # How many SNPs we tested
      tested_snps <- length(unique(data$SNP_id))
      
      # How many SNPs failed
      failed_snps <- length(unique(data[data$REF_mismatch >= i |
                                          data$ALT_mismatch >= i |
                                          data$Zigo_mismatch >= i,]$SNP_id))
      
      # Calculate percentage
      percentage_failed <- round(failed_snps / tested_snps * 100, 2)
      
      # Create a data frame with results for this number of samples
      temp_results <- data.frame(
        'Data_Set' = name,
        'Num_Samples' = i,
        'Tested_SNPs' = tested_snps,
        'Failed_SNPs' = failed_snps,
        'Percentage' = percentage_failed
      )
      
      # Assign the appropriate column name
      colnames(temp_results)[which(colnames(temp_results) == "Percentage")] <-
        paste0('Perc_', ds_id)
      
      # Append the results to the main results data frame
      results <- rbind(results, temp_results)
    }
  }
  
  # Add the results data frame to the list
  results_list[[ds_id]] <- results
}

# Initialize the final merged results data frame with just 'Num_Samples', 'Data_Set' and percentage column.
merged_results <-
  results_list[[datasets_identifiers[1]]][, c("Data_Set",
                                              "Num_Samples",
                                              paste0('Perc_', datasets_identifiers[1]))]

# Merge all other results data frames into the final results data frame
for (ds_id in datasets_identifiers[-1]) {
  # Select only 'Num_Samples', 'Data_Set' and 'Perc_*' column for merging.
  merge_data <-
    results_list[[ds_id]][, c("Data_Set", "Num_Samples", paste0('Perc_', ds_id))]
  
  merged_results <-
    merge(
      merged_results,
      merge_data,
      by = c("Data_Set", "Num_Samples"),
      all = TRUE
    )
}


# Select only the 'Data_Set', 'Num_Samples' and 'Perc_*' columns
perc_columns <- grep("^Perc_", names(merged_results), value = TRUE)
selected_columns <- c("Data_Set", "Num_Samples", perc_columns)

# Subset the merged results
subset_results <- merged_results[, selected_columns]


# Group by 'Data_Set' and 'Num_Samples' and calculate the mean for each 'Num_Samples' across all 'Perc_*' columns, ignoring NAs
summary_results <- subset_results |>
  group_by(Data_Set, Num_Samples) |>
  summarise(across(starts_with("Perc_"), mean, na.rm = TRUE), .groups = "drop")

# Remove "Perc_" from column names
names(summary_results) <- sub("Perc_", "", names(summary_results))

# Create the flextable
ft <- flextable::flextable(summary_results)

# Apply zebra theme
ft <- flextable::theme_zebra(ft)

# Add a caption to the table
ft <-
  flextable::add_header_lines(
    ft,
    "Table 4: Summary of the SNP mismatch percentage for Zygosity across each population in all data sets. "
  )

# Save it to a Word document
officer::read_docx() |>
  body_add_flextable(ft) |>
  print(
    target = here::here(
      "output",
      "wgs_vs_chip",
      "figures",
      "summary_all_data_sets_per_pop.docx"
    )
  )

ft
```

Per population plot
```{r}
# Convert the data frame to long format
long_results <- summary_results %>%
  pivot_longer(
    cols = -c(Data_Set, Num_Samples),
    names_to = "Comparison",
    values_to = "Percentage"
  ) |>
  mutate(Data_Set = factor(Data_Set))

# Specify the order of the fill factor
long_results$Comparison <- factor(long_results$Comparison,
                                  levels = c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw"))

# Define color blind friendly palette
color_blind_friendly <- c(
  "Chip (ab)" = "#E69F00",
  "Chip (ac)" = "#56B4E9",
  "Chip (bc)" = "#009E73",
  "WGS (xy)" = "#F0E442",
  "WGS (wy)" = "#0072B2",
  "WGS (wx)" = "#D55E00",
  "WGS_Chip (ay)" = "#CC79A7",
  "WGS_Chip (bx)" = "#999999",
  "WGS_Chip (cw)" = "#000000"
)

# Create a named vector to recode Data_Set column
recode_vector <- c(
  "ab" = "Chip (ab)",
  "ac" = "Chip (ac)",
  "bc" = "Chip (bc)",
  "xy" = "WGS (xy)",
  "wy" = "WGS (wy)",
  "wx" = "WGS (wx)",
  "ay" = "WGS_Chip (ay)",
  "bx" = "WGS_Chip (bx)",
  "cw" = "WGS_Chip (cw)"
)

# Recode the Data_Set column
long_results$Comparison <- recode_vector[long_results$Comparison]


# Create the bar plot with facets
ggplot(long_results,
       aes(x = Num_Samples, y = Percentage)) +
  geom_bar(aes(fill = Comparison), stat = "identity", position = "dodge") +
  facet_wrap( ~ Data_Set, ncol = 1, scales = "free_y") +
  labs(
    x = "Samples (n)",
    y = "SNPs with mismatches (%)",
    fill = "Comparison",
    title = "Number of samples that SNPs have mismatches in the zygosity",
    caption = "Number of samples per genotype call: \nChip:\n'ab' - 18 versus 95 samples\n'ac' -  18 versus 500 samples\n'bc' - 95 versus 500 samples\n\nWGS:\n'xy' - 18 versus 30 samples\n'wy' - 18 versus 800 samples\n'wx' - Genotyping calls with 30 versus 800 samples\n\nChip x WGS:\n'ay' - both 18 samples\n'bx' - WGS 30 samples and chip 95 samples\n'cw' - WGS 800 samples and chip 500 samples"
  ) +
  scale_fill_manual(
    values = color_blind_friendly,
    labels = c(
      "Chip (ab)" = "Chip (ab)",
      "Chip (ac)" = "Chip (ac)",
      "Chip (bc)" = "Chip (bc)",
      "WGS (xy)" = "WGS (xy)",
      "WGS (wy)" = "WGS (wy)",
      "WGS (wx)" = "WGS (wx)",
      "WGS_Chip (ay)" = "WGS_Chip (ay)",
      "WGS_Chip (bx)" = "WGS_Chip (bx)",
      "WGS_Chip (cw)" = "WGS_Chip (cw)"
    )
  ) +
  coord_flip() +
  my_theme() +
  scale_x_continuous(breaks = seq(0, 12, 1)) +
  theme(
    legend.position = "top",
    plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    )
  )

# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "percentage_per_pop.pdf"
  ),
  height = 10,
  width = 8,
  dpi = 300
)
```

We can also create a plot with the pairwise sample mismatch rate across all 18 samples and comparisons

```{r}
# Your data set identifiers
datasets_identifiers <- c("ab", "ac", "bc", "xy", "wy", "wx", "ay", "bx", "cw")

# Initialize the final merged results data frame with the first dataset
merged_results <- get(paste0("Zygosity_", datasets_identifiers[1]))[, .(Population, Sample, Percent_Mismatch)]
setnames(merged_results, "Percent_Mismatch", datasets_identifiers[1])

# Merge all other results data frames into the final results data frame
for (ds_id in datasets_identifiers[-1]) {
  # Retrieve the dataset
  Zygosity_data <- get(paste0("Zygosity_", ds_id))[, .(Population, Sample, Percent_Mismatch)]
  
  # Rename the Percent_Mismatch column to the dataset identifier
  setnames(Zygosity_data, "Percent_Mismatch", ds_id)
  
  # Merge with the final results data frame
  merged_results <- merge(merged_results, Zygosity_data, by = c("Population", "Sample"), all = TRUE)
}

# Create the flextable
ft <- flextable::flextable(merged_results)

# Apply zebra theme
ft <- flextable::theme_zebra(ft)

# Add a caption to the table
ft <-
  flextable::add_header_lines(
    ft,
    "Table 4: Summary of the SNP mismatch percentage for Zygosity for pairwise comparisons "
  )

# Save it to a Word document
officer::read_docx() |>
  body_add_flextable(ft) |>
  print(
    target = here::here(
      "output",
      "wgs_vs_chip",
      "figures",
      "summary_all_data_sets_pairwise.docx"
    )
  )

ft
```

Create a plot

```{r}
# Convert the data frame to long format
long_results <- merged_results |>
  pivot_longer(
    cols = -c(Population, Sample),
    names_to = "Comparison",
    values_to = "Percentage"
  ) |>
  mutate(Population = factor(Population))

# Specify the order of the fill factor
long_results$Comparison <- factor(long_results$Comparison,
                                  levels = datasets_identifiers)

# Define color blind friendly palette
color_blind_friendly <- c(
  "ab" = "#E69F00",
  "ac" = "#56B4E9",
  "bc" = "#009E73",
  "xy" = "#F0E442",
  "wy" = "#0072B2",
  "wx" = "#D55E00",
  "ay" = "#CC79A7",
  "bx" = "#999999",
  "cw" = "#000000"
)

# Create the bar plot with facets
ggplot(long_results,
       aes(x = Sample, y = Percentage)) +
  geom_bar(aes(fill = Comparison), stat = "identity", position = "dodge") +
  facet_wrap(~Population, ncol = 1, scales = "free_y") +
  labs(
    x = "Sample",
    y = "SNPs with mismatches (%)",
    fill = "Comparison",
    title = "Percentage of SNPs with zygosity mismatches in pairwise comparisons",
    caption = "Number of samples per genotype call: \nChip:\n'ab' - 18 versus 95 samples\n'ac' -  18 versus 500 samples\n'bc' - 95 versus 500 samples\n\nWGS:\n'xy' - 18 versus 30 samples\n'wy' - 18 versus 800 samples\n'wx' - Genotyping calls with 30 versus 800 samples\n\nChip x WGS:\n'ay' - both 18 samples\n'bx' - WGS 30 samples and chip 95 samples\n'cw' - WGS 800 samples and chip 500 samples"
  ) +
  my_theme() +
  scale_fill_manual(
    values = color_blind_friendly,
    labels = datasets_identifiers
  ) +
  coord_flip() +
  theme(
    legend.position = "top",
    plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    )
  )


# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "percentage_per_pop_pairwise.pdf"
  ),
  height = 10,
  width = 8,
  dpi = 300
)
```



## 15. Get allele counts from cram files

We can count how many reads for each allele in each cram file for all 175k sites for every sample

### 15.1 Use Samtools to get allele read counts

Changed strategy: count how many ATCG for each SNP position

```{bash get_allele_counts, eval=FALSE}
#!/bin/sh
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=luciano.cosme@yale.edu
#SBATCH --array=1-30
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10gb
#SBATCH --time=100:00:00 
#SBATCH --job-name=base_count
#SBATCH -o base_count%A_%a.o.txt
#SBATCH -e base_count%A_%a.ERROR.txt

cd /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls

module load SAMtools/1.16-GCCcore-10.2.0

# File containing the paths to the CRAM files
file_list="/ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/crams_30.txt"

# Get the file path for this array task
file_path=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "$file_list")

# Reference genome
reference="/gpfs/ycga/project/caccone/lvc26/september_2020/genome/aedes_albopictus_LA2_20200826.fasta"

# Sites file
sites_file="/ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/wgs_sites.txt"

# Extract the file name from the file path
file_name=$(basename "$file_path" .cram)

# Call samtools mpileup on the entire sites file
samtools mpileup -q 20 -Q 20 -f "$reference" -l "$sites_file" "$file_path" > "pileup_${file_name}.txt"
```

Merge the output files
```{bash, eval=FALSE}
#!/bin/sh
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=luciano.cosme@yale.edu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=5gb
#SBATCH --time=02:00:00 
#SBATCH --job-name=merge_pileup
#SBATCH -o merge_pileup%A_%a.o.txt
#SBATCH -e merge_pileup%A_%a.ERROR.txt

cd /ycga-gpfs/project/caccone/lvc26/wgs_chip_calls

# File containing the paths to the CRAM files
file_list="/ycga-gpfs/project/caccone/lvc26/wgs_chip_calls/crams_30.txt"

# Total number of chunks
num_chunks=50

# For each CRAM file
for i in $(seq 1 30); do
    # Get the file path for this CRAM file
    file_path=$(sed -n "$i"p "$file_list")
  
    # Extract the file name from the file path
    file_name=$(basename "$file_path" .cram)

    # Concatenate the chunk outputs and delete them
    for j in $(seq -f "%02g" 0 $((num_chunks-1))); do
        cat "pileup_${file_name}_$j.txt" >> "pileup_${file_name}.txt"
        rm "pileup_${file_name}_$j.txt"
    done
done
```


**The pileup format example:**
chr2.196	14755	A	20	,...,..,....,,,,..,,	FkFFFkkFFFFFFFFFFFkF

Explanation:
**chr2.196:** This is the name of the chromosome or scaffold.

**14755:** This is the position on the chromosome or scaffold.

**A:** This is the reference base at this position.

**20:** This is the total depth of coverage for this position across all reads. In other words, this position has been sequenced 20 times.

**,...,..,....,,,,..,,:** This string represents the bases at this position in the reads that mapped to this location. The character "," or "." represents a match to the reference base (with "," indicating a match on the reverse strand and "." indicating a match on the forward strand). The pileup string here is showing that all 20 reads are matching the reference base, "A". The directionality of the reads (whether they are from the forward or reverse strand) is also encoded here, with forward strand reads shown as "." and reverse strand reads shown as ",".

**FkFFFkkFFFFFFFFFFFkF:** This represents the base quality scores for the bases at this position in the reads. These scores are in Phred format and ASCII encoded. The higher the score, the lower the probability that the base is called incorrectly.

In summary, for position 14755 on chromosome chr2.196, the reference base is "A". All 20 reads that cover this position have a base that matches the reference base "A". These 20 reads are derived from both the forward and reverse strands. The base quality scores show that the base calling accuracy is high for most of these bases.

Check the files
```{bash}
# Check file names
ls output/wgs_vs_chip/allele_counts
```

### 15.2 Parse the pileup files

We can create a Python script to parse the pileup files. 

```{python parse_pileups, eval=FALSE}
import pandas as pd
import numpy as np
import glob
import os

def process_pileup_file(filename):
    def phred33ToQ(qual):
        score = ord(qual) - 33
        return min(score, 40)  # Limit the score to a maximum of 40

    # Read the file into a DataFrame
    df = pd.read_csv(filename, sep='\t', header=None, names=['chr', 'pos', 'ref_base', 'site_counts', 'pileup', 'quality'],
                     usecols=range(6), dtype={'site_counts': str}, on_bad_lines='skip')

    # Remove rows with missing 'ref_base' or 'site_counts'
    df = df.dropna(subset=['ref_base', 'site_counts'])

    # Replace NaNs with empty strings in the 'ref_base' and 'pileup' columns
    df['ref_base'] = df['ref_base'].replace(np.nan, '', regex=True).str.upper()
    df['pileup'] = df['pileup'].replace(np.nan, '', regex=True)

    # Convert 'site_counts' to numeric, handle errors by converting them to NaN, then to int
    df['site_counts'] = pd.to_numeric(df['site_counts'], errors='coerce').fillna(0).astype(int)

    # Initialize nucleotide count columns
    df['A'] = 0
    df['T'] = 0
    df['C'] = 0
    df['G'] = 0
    df['ref_allele'] = df['ref_base']
    df['ref_count'] = 0
    df['alt_allele'] = ''
    df['alt_count'] = 0

    # Initialize InDel column
    df['InDel'] = False

    # Calculate counts and identify InDels
    for i, row in df.iterrows():
        # Replace '.' and ',' with reference base
        pileup = row['pileup'].replace('.', row['ref_base']).replace(',', row['ref_base'])

        # Count each nucleotide
        counts = {
            'A': pileup.count('A') + pileup.count('a'),
            'T': pileup.count('T') + pileup.count('t'),
            'C': pileup.count('C') + pileup.count('c'),
            'G': pileup.count('G') + pileup.count('g'),
        }

        # Assign nucleotide counts
        df.at[i, 'A'] = counts['A']
        df.at[i, 'T'] = counts['T']
        df.at[i, 'C'] = counts['C']
        df.at[i, 'G'] = counts['G']

        # Assign reference allele count
        ref_allele = row['ref_base'].upper()
        df.at[i, 'ref_count'] = counts.get(ref_allele, 0)

        # Identify InDels
        if '+' in pileup or '-' in pileup:
            df.at[i, 'InDel'] = True

        # Identify alternative alleles
        if ref_allele in counts:
            del counts[ref_allele]
        if counts:  # If there are any alternative alleles
            alt_allele, alt_count = max(counts.items(), key=lambda x: x[1])  # Pick the most common alternative allele
            df.at[i, 'alt_allele'] = alt_allele
            df.at[i, 'alt_count'] = alt_count

        # Handle bases on both strands
        quality_scores = [phred33ToQ(qual) for qual in str(row['quality'])]

        # Calculate average quality scores for reference and alternative alleles
        ref_qual_scores = [score for base, score in zip(pileup, quality_scores) if base.upper() == ref_allele]
        alt_qual_scores = [score for base, score in zip(pileup, quality_scores) if base.upper() == alt_allele]

        ref_mean_quality = np.mean(ref_qual_scores) if ref_qual_scores else np.nan
        alt_mean_quality = np.mean(alt_qual_scores) if alt_qual_scores else np.nan

        # Assign mean quality scores
        df.at[i, 'ref_mean_quality'] = round(ref_mean_quality, 2)
        df.at[i, 'alt_mean_quality'] = round(alt_mean_quality, 2)

        # Calculate zygosity
        ref_count = df.at[i, 'ref_count']
        alt_count = df.at[i, 'alt_count']
        if ref_count == 0 and alt_count > 0:
            zygosity = 'hom_alt'
        elif ref_count > 0 and alt_count == 0:
            zygosity = 'hom_ref'
        elif ref_count > 0 and alt_count > 0:
            zygosity = 'hete'
        else:
            zygosity = ''
        df.at[i, 'zygosity'] = zygosity

    # Create an 'id' column by concatenating 'chr' (without 'chr') and 'pos'
    df['id'] = df['chr'].astype(str).apply(lambda x: x.replace('chr', '')) + '_' + df['pos'].astype(str)

    # Keep only the desired columns
    df = df[['id', 'chr', 'pos', 'site_counts', 'ref_base', 'A', 'T', 'C', 'G', 'ref_allele', 'ref_count', 'alt_allele', 'alt_count', 'InDel', 'ref_mean_quality', 'alt_mean_quality', 'zygosity']]

    return df


# Get a list of all .txt files
files = glob.glob('output/wgs_vs_chip/allele_counts/*.txt')

# Create output directory if it does not exist
output_directory = 'output/wgs_vs_chip/allele_counts/processed'
os.makedirs(output_directory, exist_ok=True)

# Process all files
for file in files:
    df = process_pileup_file(file)

    # Create output filename
    output_filename = os.path.join(output_directory, f'{os.path.basename(file)[:-4]}.csv')

    # Write the processed data to a new .csv file
    df.to_csv(output_filename, index=False)
```

### 15.2 Parse the csv files

```{python parse_pileups_csv, eval=FALSE}
import pandas as pd
import glob

# Get a list of all processed CSV files
files = glob.glob('output/wgs_vs_chip/allele_counts/processed/*.csv')

# Initialize an empty list to store the individual DataFrames
dfs = []

# Iterate over the files and read them into DataFrames
for file in files:
    # Read the CSV file
    df = pd.read_csv(file)

    # Extract the file name without the extension
    file_name = file.split('/')[-1].split('.')[0]

    # Append the file name to the column names
    df = df.rename(columns={col: col + '_' + file_name for col in df.columns if col != 'id'})

    # Append the DataFrame to the list
    dfs.append(df)

# Merge the individual DataFrames into a single DataFrame using 'id' as the key
merged_df = dfs[0]  # Initialize merged_df with the first DataFrame
for df in dfs[1:]:
    merged_df = merged_df.merge(df, on='id', how='outer')

# Drop the 'chr_' and 'pos_' columns
merged_df = merged_df.drop(columns=[col for col in merged_df.columns if col.startswith('chr_') or col.startswith('pos_')])

# Save the merged data to a new CSV file
merged_df.to_csv('output/wgs_vs_chip/allele_counts/merged_data.csv', index=False)
```


Clean env
```{r clean_python_env2_csv}
# python
py_run_string("import gc; gc.collect()")
```

### 15.3 Update SNP ids

We created a file earlier to update the SNP ids. We can use it to add the information to our data.

Check the file
```{bash}
head output/wgs_vs_chip/new_calls/wgs_snps_ids.txt
```

Our data has a column named "id" that we can use to add the SNP id
```{r import_merge_data_csv}
merged_data <-
  fread(here("output", "wgs_vs_chip", "allele_counts", "merged_data.csv"))

head(merged_data)
```

Import the SNP id file
```{r}
# Import the .txt file
snp_ids <- read_delim(
  here("output", "wgs_vs_chip", "new_calls", "wgs_snps_ids.txt"),
  delim = "\t",
  show_col_types = FALSE,
  col_names = c("chr_ref", "id_ref", "bp_ref", "id", "snp_id"),
  col_types = cols(.default = col_character())
)

# Remove "chr"
snp_ids <- 
  snp_ids |>
  mutate(id = gsub("chr", "", id),
         id_ref = gsub("chr", "", id_ref)) |>
  dplyr::select(-"id_ref")

head(snp_ids)
```

We can merge them now

```{r}
# Convert the 'snp_ids' object to a data.table
snp_ids <- as.data.table(snp_ids)

# Merge the two objects based on the "id" column
merged_data2 <- merge(merged_data, snp_ids, by = "id")

# Remove rows with NA values (rows without a match)
# merged_data2 <- na.omit(merged_data2)

# Make sure merged_data2 is a data.table
setDT(merged_data2)

# Reorder columns with setcolorder() function
setcolorder(merged_data2, c("id", "snp_id", setdiff(names(merged_data2), c("id", "snp_id"))))

# Print the first few rows of the data table
head(merged_data2)
```


We can explore the data sets now and see if the SNPs with mismatching genotypes have lower read counts or low base quality.

### 15.4 Check if there are indels at SNP sites

First we can check the SNPs with indels
```{r}
# Make sure merged_data2 is a data.table
setDT(merged_data2)

# Select columns
column_names <- c("snp_id", grep("^InDel_", names(merged_data2), value = TRUE))

# Create a new data table with the selected columns
indels_dt <- merged_data2[, ..column_names]

# Filter rows with any TRUE values
indels_dt_true <- indels_dt[rowSums(indels_dt[, -1, with = FALSE] == TRUE, na.rm = TRUE) > 0, ]

# Count TRUE values in each column
indels_count_true <- sapply(indels_dt_true[, -1, with = FALSE], function(col) sum(col == TRUE, na.rm = TRUE))

# Print the counts
print(indels_count_true)
```

We have around 900 SNPs per sample with indels. So, the genotype of these samples may be wrong in the WGS calls.

We can count how many SNPs have indels across all samples. We can create a new column and see if there is any TRUE values

```{r}
# Create a new column "any_true"
indels_dt[, any_true := rowSums(.SD == TRUE, na.rm = TRUE) > 0, .SDcols = patterns("InDel")]

# Select rows where "any_true" is TRUE
true_rows <- indels_dt[any_true == TRUE]

# Count unique "snp_id" where "any_true" is TRUE
num_true_snp_id <- uniqueN(true_rows$snp_id)

# Print the number of unique "snp_id" with any TRUE
print(num_true_snp_id)
```

Across all samples, we see 4,814 sites with indel (deletion or insertion). Next, how many times we see indels per SNP?

Theme for plotting
```{r}
# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R" # choose my_theme.R (Roboto Condensed) or my_theme2.R (default font)
  )
)
```

Create histogram
```{r}
# Create a new column "num_true"
indels_dt[, num_true := rowSums(.SD == TRUE, na.rm = TRUE), .SDcols = patterns("InDel")]

# Count number of snp_id for each number of TRUE
true_counts <- indels_dt[, .(count = .N), by = num_true]

# Plot histogram with the indel counts
ggplot(true_counts, aes(x = num_true, y = count)) +
  geom_bar(
    stat = "identity",
    fill = "#ddfacc",
    color = "#f5c5d8",
    width = 0.8
  ) +
  geom_text(aes(label = scales::comma(count)), size = 2) +
  scale_y_log10(labels = scales::comma) +
  labs(x = "Number of times the SNP site has an indel",
       y = "Number of SNPs (log10)",
       title = "How many times a SNP site has indels in 30 cram files") +
  coord_flip() +
  my_theme()


# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "indels_per_30_cram.pdf"
  ),
  height = 8,
  width = 6,
  dpi = 300
)
```

We see that most of the sites have no indels (170,546) but 4,814 have indels. 1,621 indels appear only in 1 sample. While 685 appear in two samples, etc. We see that 35 sites have indels in all the samples.

We can repeat the sample calculations using only the 18 samples we have data for chip and wgs

```{r}
# Get names of the columns that do not end with "n"
cols_to_keep <- names(indels_dt)[!grepl("n$", names(indels_dt))]

# Subset the dataframe to keep only the desired columns
indels_dt <- indels_dt[, ..cols_to_keep]

# Create a new column "any_true"
indels_dt[, any_true := rowSums(.SD == TRUE, na.rm = TRUE) > 0, .SDcols = patterns("InDel")]

# Select rows where "any_true" is TRUE
true_rows <- indels_dt[any_true == TRUE]

# Count unique "snp_id" where "any_true" is TRUE
num_true_snp_id <- uniqueN(true_rows$snp_id)

# Print the number of unique "snp_id" with any TRUE
print(num_true_snp_id)
```

Across all samples, we see 4,020 down from 4,814 sites with indel (deletion or insertion) when we used the 30 samples. Next, how many times we see indels per SNP in the 18 samples?

Create histogram
```{r}
# Create a new column "num_true"
indels_dt[, num_true := rowSums(.SD == TRUE, na.rm = TRUE), .SDcols = patterns("InDel")]

# Count number of snp_id for each number of TRUE
true_counts <- indels_dt[, .(count = .N), by = num_true]

# Plot histogram with the indel counts
ggplot(true_counts, aes(x = num_true, y = count)) +
  geom_bar(
    stat = "identity",
    fill = "#ddfacc",
    color = "#f5c5d8",
    width = 0.8
  ) +
  geom_text(aes(label = scales::comma(count)), size = 2) +
  scale_y_log10(labels = scales::comma) +
  labs(x = "Number of times the SNP site has an indel",
       y = "Number of SNPs (log10)",
       title = "How many times a SNP site has indels in 30 cram files") +
  coord_flip() +
  my_theme()


# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "indels_per_18_cram.pdf"
  ),
  height = 8,
  width = 6,
  dpi = 300
)
```

We still see sites with indels in multiple samples. However, it is lower than when we use the 30 samples. It might explain the mismatches when we compare samples from genotype calls with different number of samples.

### 15.5 Check correlation between low allele read count and genotype mismatches

Next, we can chose one within WGS comparisons and one chip vs. WGS and see if there is any correlation and allele read depth and mismatches.

WGS
"xy" Genotyping calls with 18 versus 30 samples *
"wy" Genotyping calls with 18 versus 800 samples

Chip x WGS:
"ay" - WGS and chip calls with 18 samples *
"bx" - WGS call with 30 samples and chip call with 95 samples

Because of limited time, I will compare the WGS (18 vs. 30 samples in the genotype call), chip (18 vs 95), then WGS vs. chip (18 samples)

### 15.6 WGS "xy" - genotyping calls with 18 versus 30 samples

First we need to get the SNP ids with 2 or more mismatches

Find those with zero mismatches
```{r}
# Filter the dataframe for Zigo_mismatch = 2
no_mismatches_xy <- summary_xy[summary_xy$Zigo_mismatch == 0,]

# Create a vector with SNP_id
SNPs_0_mismatches_xy <- no_mismatches_xy$SNP_id

# Print the vector
length(SNPs_0_mismatches_xy)
```

Find those with 2 or more mismatches
```{r}
# Filter the dataframe for Zigo_mismatch > 2
filtered_xy <- summary_xy[summary_xy$Zigo_mismatch >= 2,]

# Create a vector with SNP_id
SNPs_2_mismatches_xy <- filtered_xy$SNP_id

# Print the vector
length(SNPs_2_mismatches_xy)
```

Now we can check in our data the read count of this two sets of SNPs. We first need to select only the 18 samples.

```{r}
# Identify columns that end with "n"
cols_to_remove <- grep("n$", names(merged_data2))

# Remove those columns
merged_data3 <- merged_data2[, -cols_to_remove, with = FALSE]

# Print the updated data table
head(merged_data3)
```

Now we can get the mean read count for each allele across all the samples, or we could compare only two samples. Lets subset the columns with counts and quality into a new data table

```{r}
# Define the patterns to look for
patterns <- c("^ref_count_", "^alt_count_", "^ref_mean_quality_", "^alt_mean_quality_", "^site_counts_")

# Create an empty vector to store the column indices
cols_to_keep <- integer(0)

# Loop over the patterns
for (pattern in patterns) {
  # Find columns that start with the pattern and append their indices to cols_to_keep
  cols_to_keep <- c(cols_to_keep, grep(pattern, names(merged_data3)))
}

# Append the index of the 'snp_id' column to cols_to_keep
cols_to_keep <- c(which(names(merged_data3) == "snp_id"), cols_to_keep)

# Subset the data table
merged_data4 <- merged_data3[, cols_to_keep, with = FALSE]

# Print the updated data table
head(merged_data4)

```

We can get the mean sample values across all the 18 samples. We will ignore the NAs

```{r}
# Define the prefixes
prefixes <- c("site_counts_", "ref_count_", "alt_count_", "ref_mean_quality_", "alt_mean_quality_")

# Create an empty data table for the results
snp_depth_qual <- data.table(snp_id = merged_data4$snp_id)

# Loop over the prefixes
for (prefix in prefixes) {
  # Get the column indices for the current prefix
  cols <- grep(prefix, names(merged_data4))
  
  # Compute the row-wise means while ignoring NA values and round them to two decimal places
  mean_values <- apply(merged_data4[, cols, with = FALSE], 1, function(x) round(mean(x, na.rm = TRUE), 2))
  
  # Add the mean values to the results data table
  snp_depth_qual[[paste0(prefix, "mean")]] <- mean_values
}

# Print the results
head(snp_depth_qual)
```

Now we can merge our data tables

```{r}
# Using data.table's efficient join
setkey(snp_depth_qual, snp_id)
setkey(summary_xy, SNP_id)
snp_depth_qual_xy <- snp_depth_qual[summary_xy]

head(snp_depth_qual_xy)
```

Let's start easy and see if there is any correlation between site_counts_mean and Zigo_mismatch

```{r}
# Compute the correlation
correlation <- cor(snp_depth_qual_xy$site_counts_mean, snp_depth_qual_xy$Zigo_mismatch, use = "complete.obs")

# Print the correlation
print(correlation)
```

A negative correlation coefficient, like the -0.2451664 we've obtained, indicates a negative or inverse relationship between the two variables, site_counts_mean and Zigo_mismatch in our case.

What this means is that as site_counts_mean increases, Zigo_mismatch tends to decrease, and vice versa. However, the value of -0.2451664 suggests a weak negative correlation.

Typically, we would interpret the strength of the correlation using the absolute value of the correlation coefficient (ignoring the negative sign), where:

Values near 0 indicate a very weak correlation.
Values near 0.2 to 0.3 are generally considered weak.
Values near 0.4 to 0.6 are moderate.
Values above 0.6 are strong.

So in our case, the weak negative correlation of -0.2451664 suggests that while there may be a general trend of Zigo_mismatch decreasing as site_counts_mean increases, this relationship is not particularly strong and there is a lot of variability not accounted for by this relationship.

```{r message=FALSE}
# Create a scatter plot with a regression line
ggplot(snp_depth_qual_xy, aes(x = site_counts_mean, y = Zigo_mismatch)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, color = "red") +
  my_theme() +
  labs(x = "Site Counts Mean", y = "Zigo Mismatch", title = "Correlation between Site Counts Mean and Zigo Mismatch")
```


We can see if there is any strong correlation between counts and quality with the mismatches using data table library

```{r}
# Define the suffixes of interest
mean_suffixes <- c("_counts_mean", "_count_mean", "_quality_mean")  # Add "_counts_mean" to match "site_counts_mean"
mismatch_suffixes <- c("_mismatch")

# Get the column names of interest
mean_cols <- grep(paste(mean_suffixes, collapse = "|"), names(snp_depth_qual_xy), value = TRUE)
mismatch_cols <- grep(paste(mismatch_suffixes, collapse = "|"), names(snp_depth_qual_xy), value = TRUE)

# Compute the correlations
correlations <- list()
for (mean_col in mean_cols) {
  for (mismatch_col in mismatch_cols) {
    correlations[[length(correlations) + 1]] <- list(
      Mean_Column = mean_col,
      Mismatch_Column = mismatch_col,
      Correlation = cor(snp_depth_qual_xy[[mean_col]], snp_depth_qual_xy[[mismatch_col]], use = "complete.obs")
    )
  }
}

# Convert correlations into a data table
correlations_dt <- rbindlist(correlations)

# Rename values in the 'Mean_Column' column
correlations_dt[, Mean_Column := gsub("_mean", "", Mean_Column)]

# Rename values in the 'Mismatch_Column' column
correlations_dt[, Mismatch_Column := gsub("_mismatch", "", Mismatch_Column)]

# Convert data table to long format
correlations_dt_long <- melt(correlations_dt, id.vars = c("Mean_Column", "Mismatch_Column"), 
                             measure.vars = "Correlation")

# Convert 'value' column to numeric
correlations_dt_long[, value := as.numeric(value)]

# Rename 'value' column to 'Correlation'
setnames(correlations_dt_long, old = "value", new = "Correlation")

# Format the correlation to 2 decimal places
correlations_dt_long[, Correlation_formatted := sprintf("%.2f", Correlation)]

# Create scatter plot
ggplot(correlations_dt_long,
       aes(x = Mean_Column, y = Mismatch_Column, fill = Correlation)) +
  geom_tile(color = "black", size = 0.5) +  # Here you can specify the border color and size
  geom_text(aes(label = Correlation_formatted), color = "black", size = 4) +  # Add correlation values
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1, 1),
    space = "Lab",
    name = "Pearson\nCorrelation"
  ) +
  my_theme() +
  theme(axis.text.x = element_text(
    angle = 45,
    vjust = 1,
    size = 12,
    hjust = 1
  )) +
  coord_fixed() +
  labs(x = "Counts or quality", y = "Mismatches", title = "Correlation between sites read counts and quality and mismatches", caption = "WGS samples, comparison of genotype calls using 18 or 30 samples.") +
  theme(plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ))

# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "xy_read_depth_by_zigo_mismatches.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```

The highest correlation is between the number of reads at the site and the Zygosity mismatches. As the read depth decreases the number of mismatches increase. Let's group the data by Zigo_mismatch and get the mean site_counts per group.

```{r}
# Group by 'Zigo_mismatch' and calculate the mean of 'site_counts_mean'
snp_summary_dt <- snp_depth_qual_xy[, .(mean_site_counts = round(mean(site_counts_mean, na.rm = TRUE), 2)), by = Zigo_mismatch]

# Create the bar plot with annotations and adjusted x-axis limits
ggplot(snp_summary_dt, aes(x = Zigo_mismatch, y = mean_site_counts)) +
  geom_bar(stat = "identity",
           fill = "#b0dfe8",
           color = "#f5c5d8") +
  geom_text(aes(label = sprintf("%.1f", mean_site_counts)), vjust = -0.5) +
  labs(x = "Number of samples with Zygosity mismatches", y = "Mean Site Counts", title = "Mean Site Counts by Zygosity Mismatch", caption = "WGS samples, comparison of genotype calls using 18 or 30 samples.") + 
  my_theme() + coord_cartesian(xlim = c(0, 18)) +
  scale_x_continuous(breaks = seq(0, 18, 1)) +
  theme(plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ))
 
# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "xy_read_depth_by_zigo_mismatches.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```


### 15.7 WGS vs chip "ay" - WGS and chip calls with 18 samples

First we need to get the SNP ids with 2 or more mismatches

Find those with zero mismatches

```{r}
# Filter the dataframe for Zigo_mismatch = 2
no_mismatches_ay <- summary_ay[summary_ay$Zigo_mismatch == 0,]

# Create a vector with SNP_id
SNPs_0_mismatches_ay <- no_mismatches_ay$SNP_id

# Print the vector
length(SNPs_0_mismatches_ay)
```

Find those with 2 or more mismatches

```{r}
# Filter the dataframe for Zigo_mismatch > 2
filtered_ay <- summary_ay[summary_ay$Zigo_mismatch >= 2,]

# Create a vector with SNP_id
SNPs_2_mismatches_ay <- filtered_ay$SNP_id

# Print the vector
length(SNPs_2_mismatches_ay)
```

Now we can check in our data the read count of this two sets of SNPs. We first need to select only the 18 samples.

```{r}
# Identify columns that end with "n"
cols_to_remove <- grep("n$", names(merged_data2))

# Remove those columns
merged_data3 <- merged_data2[, -cols_to_remove, with = FALSE]

# Print the updated data table
head(merged_data3)
```

Now we can get the mean read count for each allele across all the samples, or we could compare only two samples. Lets subset the columns with counts and quality into a new data table

```{r}
# Define the patterns to look for
patterns <- c("^ref_count_", "^alt_count_", "^ref_mean_quality_", "^alt_mean_quality_", "^site_counts_")

# Create an empty vector to store the column indices
cols_to_keep <- integer(0)

# Loop over the patterns
for (pattern in patterns) {
  # Find columns that start with the pattern and append their indices to cols_to_keep
  cols_to_keep <- c(cols_to_keep, grep(pattern, names(merged_data3)))
}

# Append the index of the 'snp_id' column to cols_to_keep
cols_to_keep <- c(which(names(merged_data3) == "snp_id"), cols_to_keep)

# Subset the data table
merged_data4 <- merged_data3[, cols_to_keep, with = FALSE]

# Print the updated data table
head(merged_data4)
```

We can get the mean sample values across all the 18 samples. We will ignore the NAs

```{r}
# Define the prefixes
prefixes <- c("site_counts_", "ref_count_", "alt_count_", "ref_mean_quality_", "alt_mean_quality_")

# Create an empty data table for the results
snp_depth_qual <- data.table(snp_id = merged_data4$snp_id)

# Loop over the prefixes
for (prefix in prefixes) {
  # Get the column indices for the current prefix
  cols <- grep(prefix, names(merged_data4))
  
  # Compute the row-wise means while ignoring NA values and round them to two decimal places
  mean_values <- apply(merged_data4[, cols, with = FALSE], 1, function(x) round(mean(x, na.rm = TRUE), 2))
  
  # Add the mean values to the results data table
  snp_depth_qual[[paste0(prefix, "mean")]] <- mean_values
}

# Print the results
head(snp_depth_qual)
```

Now we can merge our data tables

```{r}
# Using data.table's efficient join
setkey(snp_depth_qual, snp_id)
setkey(summary_ay, SNP_id)
snp_depth_qual_ay <- snp_depth_qual[summary_ay]

head(snp_depth_qual_ay)
```

Let's start easy and see if there is any correlation between site_counts_mean and Zigo_mismatch

```{r}
# Compute the correlation
correlation <- cor(snp_depth_qual_ay$site_counts_mean, snp_depth_qual_ay$Zigo_mismatch, use = "complete.obs")

# Print the correlation
print(correlation)
```

A negative correlation coefficient, like the -0.3468091 we've obtained, indicates a negative or inverse relationship between the two variables, site_counts_mean and Zigo_mismatch in our case.

What this means is that as site_counts_mean increases, Zigo_mismatch tends to decrease, and vice versa. However, the value of -0.3468091 suggests a weak negative correlation.

Typically, we would interpret the strength of the correlation using the absolute value of the correlation coefficient (ignoring the negative sign), where:

Values near 0 indicate a very weak correlation.
Values near 0.2 to 0.3 are generally considered weak.
Values near 0.4 to 0.6 are moderate.
Values above 0.6 are strong.

So in our case, the weak negative correlation of -0.3468091 suggests that while there may be a general trend of Zigo_mismatch decreasing as site_counts_mean increases, this relationship is not particularly strong and there is a lot of variability not accounted for by this relationship.

```{r message=FALSE}
# Create a scatter plot with a regression line
ggplot(snp_depth_qual_ay, aes(x = site_counts_mean, y = Zigo_mismatch)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, color = "red") +
  my_theme() +
  labs(x = "Site Counts Mean", y = "Zigo Mismatch", title = "Correlation between Site Counts Mean and Zigo Mismatch")

```


We can see if there is any strong correlation between counts and quality with the mismatches using data table library

```{r}
# Define the suffixes of interest
mean_suffixes <- c("_counts_mean", "_count_mean", "_quality_mean")  # Add "_counts_mean" to match "site_counts_mean"
mismatch_suffixes <- c("_mismatch")

# Get the column names of interest
mean_cols <- grep(paste(mean_suffixes, collapse = "|"), names(snp_depth_qual_ay), value = TRUE)
mismatch_cols <- grep(paste(mismatch_suffixes, collapse = "|"), names(snp_depth_qual_ay), value = TRUE)

# Compute the correlations
correlations <- list()
for (mean_col in mean_cols) {
  for (mismatch_col in mismatch_cols) {
    correlations[[length(correlations) + 1]] <- list(
      Mean_Column = mean_col,
      Mismatch_Column = mismatch_col,
      Correlation = cor(snp_depth_qual_ay[[mean_col]], snp_depth_qual_ay[[mismatch_col]], use = "complete.obs")
    )
  }
}

# Convert correlations into a data table
correlations_dt <- rbindlist(correlations)

# Rename values in the 'Mean_Column' column
correlations_dt[, Mean_Column := gsub("_mean", "", Mean_Column)]

# Rename values in the 'Mismatch_Column' column
correlations_dt[, Mismatch_Column := gsub("_mismatch", "", Mismatch_Column)]

# Convert data table to long format
correlations_dt_long <- melt(correlations_dt, id.vars = c("Mean_Column", "Mismatch_Column"), 
                             measure.vars = "Correlation")

# Convert 'value' column to numeric
correlations_dt_long[, value := as.numeric(value)]

# Rename 'value' column to 'Correlation'
setnames(correlations_dt_long, old = "value", new = "Correlation")

# Format the correlation to 2 decimal places
correlations_dt_long[, Correlation_formatted := sprintf("%.2f", Correlation)]

# Create scatter plot
ggplot(correlations_dt_long,
       aes(x = Mean_Column, y = Mismatch_Column, fill = Correlation)) +
  geom_tile(color = "black", size = 0.5) +  # Here you can specify the border color and size
  geom_text(aes(label = Correlation_formatted), color = "black", size = 4) +  # Add correlation values
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1, 1),
    space = "Lab",
    name = "Pearson\nCorrelation"
  ) +
  my_theme() +
  theme(axis.text.x = element_text(
    angle = 45,
    vjust = 1,
    size = 12,
    hjust = 1
  )) +
  coord_fixed() +
  labs(x = "Counts or quality", y = "Mismatches", title = "Correlation between sites read counts \nand quality and mismatches", caption = "WGS and Chip calls done with the 18 samples.") +
  theme(plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ))

 # Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_read_depth_by_zigo_mismatches_correlation.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```

The highest correlation is not between the number of reads at the site and the Zygosity mismatches as within the wgs data set we compared before. Now, the read depth of the alternative allele has a moderate correlation with the number of mismatches (0.39) As the read depth decreases the number of mismatches increase. Let's group the data by Zigo_mismatch and get the mean site_counts per group.

We can check the number of reads at the site and the Zygosity mismatches as we did before
```{r}
# Group by 'Zigo_mismatch' and calculate the mean of 'site_counts_mean'
snp_summary_dt <- snp_depth_qual_ay[, .(mean_site_counts = round(mean(site_counts_mean, na.rm = TRUE), 2)), by = Zigo_mismatch]

# Create the bar plot with annotations and adjusted x-axis limits
ggplot(snp_summary_dt, aes(x = Zigo_mismatch, y = mean_site_counts)) +
  geom_bar(stat = "identity",
           fill = "#b0dfe8",
           color = "#f5c5d8") +
  geom_text(aes(label = sprintf("%.1f", mean_site_counts)), vjust = -0.5, size = 3) +
  labs(x = "Number of samples with Zygosity mismatches", y = "Mean Site Counts", title = "Mean Site Counts by Zygosity Mismatch", caption = "WGS and chip samples genotyped with 18 samples") + 
  my_theme() + coord_cartesian(xlim = c(0, 18)) +
  scale_x_continuous(breaks = seq(0, 18, 1)) +
  theme(plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ))
 
# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_read_depth_by_zigo_mismatches.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```

We can also look at the alternative allele read counts
```{r}
# Group by 'ALT_mismatch' and calculate the mean of 'site_counts_mean'
snp_summary_dt <- snp_depth_qual_ay[, .(mean_site_counts = round(mean(site_counts_mean, na.rm = TRUE), 2)), by = ALT_mismatch]

# Create the bar plot with annotations and adjusted x-axis limits
ggplot(snp_summary_dt, aes(x = ALT_mismatch, y = mean_site_counts)) +
  geom_bar(stat = "identity",
           fill = "#b0dfe8",
           color = "#f5c5d8") +
  geom_text(aes(label = sprintf("%.1f", mean_site_counts)), vjust = -0.5, size = 3) +
  labs(x = "Number of samples with Zygosity mismatches", y = "Mean ALT allele Counts", title = "Mean ALT allele Counts by Zygosity Mismatch", caption = "WGS samples, comparison of genotype calls using 18 or 30 samples.") + 
  my_theme() + coord_cartesian(xlim = c(0, 18)) +
  scale_x_continuous(breaks = seq(0, 18, 1)) +
  theme(plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ))
 
# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_read_depth_by_ALT_read_zigo_mismatches.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```

We see that the read depth decreases with the number of samples for which we find Zygosity mismatches (homo_ref, homo_alt and heterozygous)

Now we can import the SNP metrics for the chip genotype call and see if we find correlations as well.

```{r}
# Read the file with fread() 
ay_chip_metrics <- fread(
  here(
    "data",
    "raw_data",
    "albo",
    "wgs_vs_chip",
    "wgs_18_samples_metrics.txt"
  )
)

# We can add two new columns, n_NoCall (missing call_ and n_OTV (off target variant) and subset our data table

# Define a list of columns to check
call_code_cols = grep("_call_code$", names(ay_chip_metrics), value = TRUE)

# Create the n_NoCall column
ay_chip_metrics[, n_NoCall := rowSums(do.call(cbind, lapply(.SD, function(x) x == "NoCall"))), .SDcols = call_code_cols]

# Create the n_OTV column
ay_chip_metrics[, n_OTV := rowSums(do.call(cbind, lapply(.SD, function(x) x == "OTV"))), .SDcols = call_code_cols]

# Select columns to subset - I remove MMD. HomFLD, HetSO and HomRO columns since it had NAs
selected_columns <- ay_chip_metrics[, .(probeset_id, CR, FLD, nMinorAllele, Nclus, n_AA, n_AB, n_BB, n_NC, MinorAlleleFrequency, n_NoCall, n_OTV)]

# Subset
ay_chip_metrics <- ay_chip_metrics[, .(probeset_id, CR, FLD, nMinorAllele, Nclus, n_AA, n_AB, n_BB, n_NC, MinorAlleleFrequency, n_NoCall, n_OTV)]

# Check output
head(ay_chip_metrics)
```

Now we can merge our data sets to run the correlation analysis (merge with snp_depth_qual_ay)

```{r}
# Set the key for each table
setkey(ay_chip_metrics, probeset_id)
setkey(snp_depth_qual_ay, snp_id)

# Join the tables
ay_wgs_chip_metrics <- snp_depth_qual_ay[ay_chip_metrics, nomatch = 0L]

# Remove rows with NAs
ay_wgs_chip_metrics <- ay_wgs_chip_metrics[complete.cases(ay_wgs_chip_metrics), ]

head(ay_wgs_chip_metrics)
```

Check for NAs

```{r}
# Check for NAs in ay_wgs_chip_metrics
any_na <- any(colSums(is.na(ay_wgs_chip_metrics)) > 0)

if (any_na) {
  print("There are NA values in the ay_wgs_chip_metrics table.")
} else {
  print("There are no NA values in the ay_wgs_chip_metrics table.")
}

```

Plot

```{r}
# Define the suffixes of interest
mean_suffixes <- c("_counts_mean", "_count_mean", "_quality_mean")
mismatch_suffixes <- c("_mismatch")

# Get the column names of interest
mean_cols <-
  grep(paste(mean_suffixes, collapse = "|"),
       names(ay_wgs_chip_metrics),
       value = TRUE)
mismatch_cols <-
  grep(paste(mismatch_suffixes, collapse = "|"),
       names(ay_wgs_chip_metrics),
       value = TRUE)

other_numeric_cols <-
  c(
    "CR",
    "FLD",
    "nMinorAllele",
    "Nclus",
    "n_AA",
    "n_AB",
    "n_BB",
    "n_NC",
    "MinorAlleleFrequency",
    "n_NoCall",
    "n_OTV"
  )

# Combine mean_cols and other_numeric_cols
mean_and_other_numeric_cols <- c(mean_cols, other_numeric_cols)

# Compute the correlations
ay_correlations <- list()
for (col1 in mean_and_other_numeric_cols) {
  for (col2 in mismatch_cols) {
    ay_correlations[[length(ay_correlations) + 1]] <- list(
      Column1 = col1,
      Column2 = col2,
      Correlation = cor(ay_wgs_chip_metrics[[col1]], ay_wgs_chip_metrics[[col2]], use = "complete.obs")
    )
  }
}

# Convert correlations into a data table
ay_correlations_dt <- rbindlist(ay_correlations)

# Format the correlation to 2 decimal places
ay_correlations_dt[, Correlation_formatted := sprintf("%.2f", Correlation)]

# Update the visualization
ggplot(ay_correlations_dt,
       aes(x = Column1, y = Column2, fill = Correlation)) +
  geom_tile(color = "black", size = 0.5) +
  geom_text(aes(label = Correlation_formatted),
            color = "black",
            size = 4) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1, 1),
    space = "Lab",
    name = "Pearson\nCorrelation"
  ) +
  my_theme() +
  theme(axis.text.x = element_text(
    angle = 45,
    vjust = 1,
    size = 12,
    hjust = 1
  )) +
  coord_fixed() +
  labs(
    x = "WGS/chip metrics",
    y = "Mismatches",
    title = "Correlation between WGS counts/quality,\n chip metrics against mismatches",
    caption = "WGS and Chip calls done with the 18 samples."
  ) +
  theme(plot.caption = element_text(
    size = 8,
    color = "gray30",
    face = "italic",
    hjust = 1
  ))


# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_wgs_chip_metrics_vs_mismatches.pdf"
  ),
  height = 6,
  width = 10,
  dpi = 300
)
```


We can check the number of reads at the site and the Zygosity mismatches as we did before

```{r}
snp_summary_dt <-
  ay_wgs_chip_metrics[, .(
    mean_FLD = round(mean(FLD, na.rm = TRUE), 2),
    mean_CR = round(mean(CR, na.rm = TRUE), 2),
    mean_site_counts_mean = round(mean(site_counts_mean, na.rm = TRUE), 2)
  ),
  by = Zigo_mismatch]


# Reshape data from wide to long format
snp_summary_dt_long <-
  melt(
    snp_summary_dt,
    id.vars = "Zigo_mismatch",
    variable.name = "Variable",
    value.name = "Mean"
  )

cbPalette <- c("#CC79A7", "#56B4E9", "#009E73")

# Change variable names for legend keys
snp_summary_dt_long$Variable <- factor(
  snp_summary_dt_long$Variable,
  levels = c("mean_FLD", "mean_CR", "mean_site_counts_mean"),
  labels = c("FLD", "Call Rate", "Read Count")
)

ggplot(snp_summary_dt_long,
       aes(x = Zigo_mismatch, y = Mean, fill = Variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(
    aes(label = sprintf("%.1f", Mean)),
    position = position_dodge(width = 0.9),
    vjust = -0.25,
    size = 2
  ) +
  scale_fill_manual(values = cbPalette) +
  labs(
    x = "Number of samples with Zygosity mismatches",
    y = "Mean Value (log 10)",
    fill = "Metrics",
    title = "Mean Values by Zygosity Mismatch",
    caption = "WGS and chip samples genotyped with 18 samples"
  ) +
  theme_minimal() + coord_cartesian(xlim = c(0, 18)) +
  scale_x_continuous(breaks = seq(0, 18, 1)) +
  scale_y_log10() + # apply log transformation to y-axis
  theme(
    plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ),
    legend.position = "top"
  )

# Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_wgs_chip_bars_after_correlation.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```

We do not see a clear pattern since the FLD, Call Rate and Read Count seem similar for sites with mismatches. Perhaps the mean values per number of samples with mismatches are not a good way to represent the correlation. We know that SNPs with lower FLD can have an higher rate of mismatches. 

Lets try a violin plot

```{r}
# Reshape data from wide to long format
snp_summary_dt_long <- melt(
  snp_summary_dt,
  id.vars = "Zigo_mismatch",
  variable.name = "Variable",
  value.name = "Value"
)

# Change variable names for legend keys
snp_summary_dt_long$Variable <- factor(
  snp_summary_dt_long$Variable,
  levels = c("mean_FLD", "mean_CR", "mean_site_counts_mean"),
  labels = c("FLD", "Call Rate", "Read Count")
)


# Create a new categorical variable based on Zigo_mismatch
snp_summary_dt_long$Zigo_group <-
  cut(
    snp_summary_dt_long$Zigo_mismatch,
    breaks = seq(0, 18, 2),
    labels = seq(0, 16, 2),
    include.lowest = TRUE
  )



# Create violin plot
ggplot(snp_summary_dt_long,
       aes(x = Zigo_group, y = Value, fill = Variable)) +
  geom_violin(scale = "width", trim = FALSE) +
  geom_boxplot(
    width = 0.2,
    fill = "white",
    color = "black",
    outlier.shape = NA
  ) +
  scale_fill_manual(values = cbPalette) +
  labs(
    x = "Number of samples with Zygosity mismatches",
    y = "Value",
    fill = "Metrics",
    title = "Distribution of Values by Zygosity Mismatch",
    caption = "WGS and chip samples genotyped with 18 samples"
  ) +
  my_theme() +
  scale_y_log10() +
  theme(
    plot.caption = element_text(
      size = 8,
      color = "gray30",
      face = "italic",
      hjust = 1
    ),
    legend.position = "top"
  )

# # Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_wgs_chip_violin_after_correlation.pdf"
  ),
  height = 5,
  width = 6,
  dpi = 300
)
```

### 15.8 Filtering based on variables correlated with mismatches

We can filter our data to using the FLD, call rate and read count to see if the errors decrease when comparing WGS and chip. The default FLD value from Axiom Suite recommended thresholds is 3.6. We can test FLD = 5. We can use the call rate threshold of 95% for example. Now, for the WGS data, we can filter the SNPs with highest read depth. For example, we can only look at sites with 15 or 20 reads. We will have to try a few filtering strategies. Perhaps filter based on the number of reads of each allele. Based on our previous plots, the reference allele read counts is negatively correlated with the number of mismatches. We can change the thresholds and re-do the plots to see if there is an improvement.

Check the data

```{r}
head(ay_wgs_chip_metrics)
```

Check how many SNPs we have in our data
```{r}
length(
  unique(
    ay_wgs_chip_metrics$snp_id
  )
)
```

How many SNPs will we remove for each filtering strategy?

FLD
```{r}
# How many SNPs we remove if we set FLD of 6
90686 - ay_wgs_chip_metrics |>
  dplyr::filter(FLD >= 6)  |>
  summarise(n = n_distinct(snp_id)) 
```

Lets filter by FLD

```{r}
ay_wgs_chip_FLD <-
  ay_wgs_chip_metrics |>
  dplyr::filter(FLD >= 6)

# How many SNPs left
length(
  unique(
    ay_wgs_chip_FLD$snp_id
  )
)
```

Lets check how many SNPs we will remove if we filter by call rate of 98.5

```{r}
# How many SNPs we remove if we set CR of 97.5
63596 - ay_wgs_chip_FLD |>
  dplyr::filter(CR >= 98.5)  |>
  summarise(n = n_distinct(snp_id))
```

Let filter out these SNPs

```{r}
ay_wgs_chip_FLD_CR <-
  ay_wgs_chip_FLD |>
  dplyr::filter(CR >= 98.5)

# How many SNPs left
length(
  unique(
    ay_wgs_chip_FLD_CR$snp_id
  )
)
```

Now we can look at the site mean read count and the read count of each allele

```{r}
# How many SNPs we remove if we set minimal read count of the site of 20
61972 - ay_wgs_chip_FLD_CR |>
  dplyr::filter(site_counts_mean >= 20) |>
  summarise(n = n_distinct(snp_id))
```

What about the reference allele

```{r}
# How many SNPs we remove if we set minimal read count of the reference allele of 10
61972 - ay_wgs_chip_FLD_CR |>
  dplyr::filter(ref_count_mean >= 10)  |>
  summarise(n = n_distinct(snp_id))
```

What about the alternative allele

```{r}
# How many SNPs we remove if we set minimal read count of the alternative allele of 10 reads
61972 - ay_wgs_chip_FLD_CR |>
  dplyr::filter(alt_count_mean >= 10)  |>
  summarise(n = n_distinct(snp_id))
```

Now we can combine them to see how many SNPs we will remove

```{r}
# How many SNPs we remove if we set minimal read count of the reference allele of 10 reads, or alternative allele of 10 reads
61972 - ay_wgs_chip_FLD_CR |>
  dplyr::filter(site_counts_mean >= 20 &
                  ref_count_mean >= 10 | alt_count_mean >= 10)  |>
  summarise(n = n_distinct(snp_id))
```

Let remove these SNPs

```{r}
# We can use | (or) or & (and)
ay_wgs_chip_FLD_CR_depth <-
  ay_wgs_chip_FLD |>
  dplyr::filter(site_counts_mean >= 20 |
                  ref_count_mean >= 20 | alt_count_mean >= 20)

# How many SNPs left
length(
  unique(
    ay_wgs_chip_FLD_CR_depth$snp_id
  )
)
```

We went from 90,686 to 55,833 SNPs. Now we can see if it improved the matches of WGS and chip calls.

Lets get the SNPs ids that we want to keep
```{r}
# Get the ids of the SNPs that passed filtering
unique(ay_wgs_chip_FLD_CR_depth$snp_id) -> ay_filtered_snps
```

Now we can select these SNPs from our data table with the summary
```{r}
# summary_ay is our data.table and ay_filtered_snps is our vector with SNP ids that passed our filtering
filtered_summary_ay <- summary_ay[SNP_id %in% ay_filtered_snps]

# How many SNPs left
length(
  unique(
    filtered_summary_ay$SNP_id
  )
)
```

Now we can create the same plot we did before but only with the filtered data

Make the data long format for plotting
```{r ay_data_long_filtered}
dt_long_filtered_ay <- process_summary_object("filtered_summary_ay")
head(dt_long_filtered_ay)
```

Create plot of SNP error per sample. We need to change our plotting function

Function for errors per SNP per sample

```{r plot_dt_long_function_filtered}
plot_dt_long_filtered <- function(object_suffix) {
  # Get the object name based on the suffix
  object_name <- paste0("dt_long_filtered_", object_suffix)
  
  # Get the corresponding data.table object
  dt_long <- get(object_name)
  
  # Create facet histogram
  p <- ggplot(dt_long, aes(x = count, y = n)) +
    geom_bar(
      stat = "identity",
      fill = "#ffcae4",
      color = ifelse(
        dt_long$count == 0,
        "#CCFF00",
        ifelse(dt_long$count == 1, "#4169E1", "#FF7F50")
      ),
      width = 0.6,
      linewidth = 1
    ) +
    geom_text(
      aes(label = paste0(
        scales::comma(n), " (", round(perc, 2), "%)"
      )),
      hjust = ifelse(dt_long$count == 0, .7, 0.01),
      size = 2.3,
      color = "gray10"
    ) +
    facet_wrap(~ type, scales = "free_y") +
    labs(
      title = paste("Histogram of SNP Mismatch Counts", object_suffix),
      x = "Sample Count",
      y = "SNP Count",
      caption = paste(object_suffix, "\n Bar border colors: Electric Lime = no errors; Royal Blue =  1 error; Coral = more than 1 error")
    ) +
    scale_y_continuous(
      breaks = c(0, 25000, 50000, 75000, 100000, 125000, 150000, 175000),
      labels = function(x) paste0(x / 1000, "k"),
      expand = expansion(mult = c(0, 0.2))
    ) +
    scale_x_continuous(breaks = 0:18, expand = expansion(add = c(0.5, 0))) +
    my_theme() +
    coord_flip() +
    theme(
      plot.caption = element_text(
        face = "italic",
        size = 10,
        color = "grey20"
      ),
      panel.spacing = unit(2, "lines"),
      plot.margin = unit(c(1, 3, 1, 1), "cm"),
      axis.text.x = element_text(size = 7, angle = 0) 
    )
  
  # Save the plot
  output_file <- here("output", "wgs_vs_chip", "figures", paste0(object_suffix, "_filtered_mismatches.pdf"))
  ggsave(output_file, p, width = 8, height = 6, units = "in")
  
  # Return the plot object
  return(p)
}
```

Create new plot
```{r ay_plot1_filtered}
plot_dt_long_filtered("ay")
```

Compare to our first plot
```{r}
plot_dt_long("ay")
```

Overall it did improve the agreement between the WGS and chip. What about the pairwise comparisons?

Before we had prepared this plot
Counts first plot
```{r run_calculate_counts_plot_2, eval=FALSE}
# Call the function with data_*_dt as input
counts_ay <- calculate_counts(data_ay_dt)
plot_counts(counts_ay, here("output", "wgs_vs_chip", "figures", "ay_SAI_KAT_per_sample_stats.pdf"))
```

Calculate counts for the filtered data
We first need to filter the SNP from the original data table
```{r}
data_ay_dt_filtered <- data_ay_dt[SNP_id %in% ay_filtered_snps]
```

Then calculate counts
```{r run_calculate_counts_plot_3}
# Call the function with data_*_dt as input
counts_filtered_ay <- calculate_counts(data_ay_dt_filtered)
plot_counts(
  counts_filtered_ay,
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_filtered_SAI_KAT_per_sample_stats.pdf"
  )
)
```

We started with 90,686 and after filtering we have 38,632.
The percentage of mismatches decreased. We can change the parameters to see if we find thresholds which result in low mismatches between WGS and chip genotypes. We see that for the KAT population, the reference and alternative alleles mismatches are below 2%; the Zygosity mismatches are a bit higher, but below 2%. For the other population we see a slightly higher mismatch rate probably due to the fact it is from an island in the invasive range. 

Lets compare the mean mismatch before and after filtering

Before
```{r}
round(mean(counts_ay$Percent_Mismatch), 2)
```

After
```{r}
round(mean(counts_filtered_ay$Percent_Mismatch), 2)
```

We can get some summary statistics to see what improved. For example, what is the mean read depth per site before and after?

Before
```{r}
round(mean(ay_wgs_chip_metrics$site_counts_mean), 2)
```

After
```{r}
round(mean(ay_wgs_chip_FLD_CR_depth$site_counts_mean), 2)
```

Now we can create a PCA with the WGS and chip data sets before and after filtering.

### 15.9 PCA with WGS and chip data sets before and after filtering

We can remove the SNPs with segregation errors before we run the PCA analysis. In previous comparisons we did not see significant overlap between the two types of SNPs, with mismatches and segregation errors.

#### 15.9.1 Venn diagram between SNPs with mismatches and segregation errors

We can compare the SNPs with 1 or more samples with discrepancies with the SNPs that did not pass our segregation test.


Get the SNPs that have errors in 1 or more samples
```{r subset_mismatches_03_ay}
# Discrepancies in 1 or more samples
# How many SNPs we tested
tested_snps <- length(unique(data_ay_dt_filtered$SNP_id))
cat("Number of SNPs tested:", tested_snps, "\n")

# How many SNPs failed
failed_snpsR <-
  length(
    unique(data_ay_dt_filtered[data_ay_dt_filtered$REF_mismatch_count >= 1,]$SNP_id
           )
         )
cat("REF mismatch at in 1 samples:", failed_snpsR, "\n")

# How many SNPs failed
failed_snpsA <-
  length(
    unique(data_ay_dt_filtered[data_ay_dt_filtered$ALT_mismatch_count >= 1,]$SNP_id
           )
         )
cat("ALT mismatch at least in 1 samples:", failed_snpsA, "\n")


# How many SNPs failed zygosity
failed_snps <-
  length(
    unique(data_ay_dt_filtered[data_ay_dt_filtered$Zigo_mismatch_count >= 1,]$SNP_id
           )
         )
cat("Zygosity mismatch in at least 1 samples:", failed_snps, "\n")

# Calculate percentage
percentage_failed <- round(failed_snps / tested_snps * 100, 2)
cat("Percentage of failed SNPs in 1 or more samples:", percentage_failed, "%\n")
```

Get the SNP ids

```{r get_mismatch_SNP_ids_2_samplesb}
failed_snps_ids_filtered <-
  unique(
    data_ay_dt_filtered[data_ay_dt_filtered$Zigo_mismatch_count >= 1, ]$SNP_id
    )

# Define the file path
file_path <- here("output",
                  "wgs_vs_chip",
                  "SNPs_failed_1_samples.txt")

# Write unique SNPs to the file
writeLines(failed_snps_ids_filtered, con = file_path)
```

#### 15.9.2 Venn diagram fail Mendel and mismatches

Create a Venn diagram between the SNPs with genotyping mismatches and those that failed our segregation test

```{r Venn_diagram_segregation_genotype_mismatchesb}
# Read in the two files as vectors
fail_mendel <-
  read_table(
    here(
      "output",
      "segregation",
      "albopictus",
      "albopictus_SNPs_fail_segregation.txt"
    ),
    col_names = FALSE,
    show_col_types = FALSE
  )[[1]]

fail_geno <-
  read_table(
    here("output",
         "wgs_vs_chip",
         "SNPs_failed_2_samples.txt"),
    col_names = FALSE,
    show_col_types = FALSE
  )[[1]]



# Calculate shared values
errors_SNPs <-
  intersect(fail_mendel,
            fail_geno)


# Create Venn diagram
venn_data <-
  list(
    "Fail Mendel" = fail_mendel,
    "Genotype Mismatches" = fail_geno
  )
venn_plot <-
  ggvenn(
    venn_data,
    fill_color = c("steelblue", "darkorange"),
    show_percentage = TRUE
  )

# Add a title
venn_plot <-
  venn_plot +
  ggtitle("Comparison of SNPs with errors") +
  theme(plot.title = element_text(hjust = .5))

# Display the Venn diagram
print(venn_plot)

# Save Venn diagram to PDF
output_path <-
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "Mendel_mismatches_overlap_filtered.pdf"
  )
ggsave(
  output_path,
  venn_plot,
  height = 6,
  width = 6,
  dpi = 300
)
```

We will remove all SNPs with mismatches in 3 or more samples and those with segregation errors, in our case 141 SNPs. We can create a new file to use later with Plink
```{r}
# Merge the vectors
SNPs_to_exclude <- unique(c(fail_mendel, fail_geno))

# How many to remove
cat("How many SNPs to remove:", length(SNPs_to_exclude), "\n")

# Write to file
write.table(
  SNPs_to_exclude,
  file = here(
    "output",
    "wgs_vs_chip",
    "SNPs_to_exclude.txt"
  ),
  row.names = FALSE,
  col.names = FALSE,
  quote = FALSE
)

# We can also create a list of SNPs that we can use for the PCA
ay_SNPs_to_extract <-
  unique(
    data_ay_dt_filtered$SNP_id
    )

# We can remove the SNPs with errors
ay_SNPs_to_extract_filtered <- ay_SNPs_to_extract[!ay_SNPs_to_extract %in% SNPs_to_exclude]

# How many SNPs left
cat("How many SNPs to keep after filtering:", length(ay_SNPs_to_extract_filtered), "\n")

# Write it to file
write.table(
  ay_SNPs_to_extract_filtered,
  file = here(
    "output",
    "wgs_vs_chip",
    "ay_SNPs_to_extract_filtered.txt"
  ),
  row.names = FALSE,
  col.names = FALSE,
  quote = FALSE
)
```


#### 15.9.3 PCA before and after removing SNPs
WGS vs chip "ay" - WGS and chip calls with 18 samples

Now use Plink to create a PCA excluding only the SNPs that failed our segregation test

Lets import our .fam file to filter the IDs we want to compare.

```{r select_samples_pca_ay}
# Read the data
fam_data <-
  here("output", "wgs_vs_chip", "wgs_chip_merged.fam") |>
  read_delim(
    delim = "\t",
    col_names = FALSE,
    show_col_types = FALSE
   ) |>
  setNames(
    c(
      "FID", "IID", "PID", "MID", "Sex", "Phenotype"
      )
    )

# Filter the data
filtered_data <-
  fam_data |>
  dplyr::filter(stringr::str_detect(IID, "a$|y$")) |>
  dplyr::select("FID", "IID")

# Save to file
write.table(
  filtered_data,
  file = here("output", "wgs_vs_chip", "ay_wgs_chip_samples.txt"),
  quote = FALSE,
  sep = " ",
  row.names = FALSE,
  col.names = FALSE
)
```

Use Plink with only the samples we are comparing (priors) and remove SNPs that failed the Mendel test
     "output", 
     "segregation",
     "albopictus",
     "alobopictus_SNPs_fail_segregation.txt"
```{bash pca_1_ay}
# Before
# Here we set genotyping missingness to 10%, MAF 5%, and remove the SNPs with segreagtion errors
plink \
--allow-extra-chr \
--keep-allele-order \
--bfile output/wgs_vs_chip/wgs_chip_merged \
--exclude output/segregation/albopictus/alobopictus_SNPs_fail_segregation.txt \
--keep output/wgs_vs_chip/ay_wgs_chip_samples.txt \
--pca \
--geno 0.1 \
--maf 0.05 \
--out output/wgs_vs_chip/ay_pca_before \
--silent;

grep "samples\|variants" output/wgs_vs_chip/ay_pca_before.log
```

Now do it again but remove both SNPs that failed the Mendel test and that have genotype mismatches in at least 2 samples (plus those with segregation errors).

```{bash pca_2_ay}
# After
# Here we set genotyping missingness to 10%, MAF 5%, and remove the SNPs with segreagation errors and those with mismatches between wgs and chip. We now can extract the SNPs that passed
plink \
--allow-extra-chr \
--keep-allele-order \
--bfile output/wgs_vs_chip/wgs_chip_merged \
--extract output/wgs_vs_chip/ay_SNPs_to_extract_filtered.txt \
--keep output/wgs_vs_chip/ay_wgs_chip_samples.txt \
--pca \
--geno 0.1 \
--maf 0.05 \
--out output/wgs_vs_chip/ay_pca_after \
--silent;

grep "samples\|variants" output/wgs_vs_chip/ay_pca_after.log
```

Create PCA plot
```{r pca_3_ay}
# Load the PCA results
pca_1 <-
  read.table(here("output", "wgs_vs_chip", "ay_pca_before.eigenvec"),
             header = FALSE)
colnames(pca_1) <- c("FID", "IID", paste0("PC", 1:(ncol(pca_1) - 2)))
pca_1$analysis <- "Before"
pca_1$group <- ifelse(
  stringr::str_detect(pca_1$IID, "a$"),
  "a",
  ifelse(stringr::str_detect(pca_1$IID, "y$"), "y", "Other")
)

pca_2 <-
  read.table(here("output", "wgs_vs_chip", "ay_pca_after.eigenvec"),
             header = FALSE)
colnames(pca_2) <- c("FID", "IID", paste0("PC", 1:(ncol(pca_2) - 2)))
pca_2$analysis <- "After"
pca_2$group <- ifelse(
  stringr::str_detect(pca_2$IID, "a$"),
  "a",
  ifelse(stringr::str_detect(pca_2$IID, "y$"), "y", "Other")
)

# Combine the data
combined_pca <- rbind(pca_1, pca_2)

# import plotting theme
source(
  here(
    "scripts",
    "analysis",
    "my_theme2.R"
  )
)

# Convert the 'analysis' column to a factor and specify the level order
combined_pca$analysis <- 
  factor(combined_pca$analysis, levels = c("Before", "After"))

# Create a facet plot
ggplot(combined_pca, aes(x = PC1, y = PC2, color = group, shape = group)) +
  geom_point(size = 2) +
  facet_grid(FID ~ analysis, scales = "fixed") +
  labs(
    x = "PC1",
    y = "PC2",
    # title = "The effect of SNPs with genotyping mismatches in\n 1 or more samples and low read depth",
    colour = "Method",
    shape = "Method"#,
    # caption = "Removing SNPs with genotypes mismatches in >= 1 sample.\n WGS data filtered based on read depth (20x for site or alleles). \n Chip data filtered by call rate (CR=98.5%) and Fisher Linear Discriminant (FLD>=6). \n'Before' with 88,443 SNPs 'After' with 20,293 SNPs (--maf 0.05 and --geno 0.1)."
  ) +
  my_theme() +
  scale_color_manual(
    values = c(
      "a" = "red",
      "y" = "black",
      "Other" = "black"
    ),
    labels = c("a" = "Chip", "y" = "WGS", "Other" = "Other")
  ) +
  theme(plot.caption = element_text(
    face = "italic",
    size = 10,
    color = "grey20"
  ),
  legend.position = "top",
  panel.spacing = grid::unit(1, "cm")) +
  scale_shape_manual(
    values = c(
      "a" = 19,  # Filled circle
      "y" = 1,  # Open circle
      "Other" = 3  # Plus
    ),
    labels = c("a" = "Chip", "y" = "WGS", "Other" = "Other")
  )

# # Save plot to PDF
ggsave(
  here(
    "output",
    "wgs_vs_chip",
    "figures",
    "ay_PCA_before_after_remove_SNPs_errors_EDITED.pdf"
  ),
  height = 6,
  width = 6,
  dpi = 300
)
```

## 16. Conclusion

We can use these thresholds: 
WGS -> read depth = 20 for site or alleles
Chip -> FLD >= 6 and call rate >= 98.5% 
if we wish to combine samples genotyped with WGS and chip.
Besides, the number of samples in the genotype calls do affect the mismatch rate between the technologies. The sample size seems to affect more the WGS data than the chip. Therefore, it is crucial to check the read depth and allele specific read depth to filter sites or alleles with low read depth. It is probably not possible to combine low depth sequencing data with the chip data. Due to incredibly repetitive nature of the genome (~70%) the sequencing cost will be higher if we want to obtain 20x per site or allele. It is probably not a good idea to merge the data sets without any of these considerations.








